---
title: CNN
created: 2025-12-27
tags:
  - deep_learning
links:
---
*Сверточная нейронная сеть* (CNN) - тип искусственной нейронной сети, специально разработанный для эффективной обработки данных с сеточной структурой.

Стандартный прямой слой является полносвязным; каждый вход соединен с каждым выходом. В отличие от него, сверточная сеть использует идею *ядра*, своего рода меньшей сети, которую мы пропускаем через вход. Например, в задачах классификации изображений мы пропускаем ядро горизонтально и вертикально по изображению, чтобы распознать визуальные признаки, и поэтому мы описываем визуальное изображение как 2D сверточную сеть. Для речи мы будем перемещать наше ядро по сигналу во временном измерении, чтобы извлечь речевые признаки, поэтому CNN для речи - это 1D сверточные сети.

Давайте подробнее рассмотрим эту идею. Начнем с очень схематичной версии сверточного слоя, который принимает на вход последовательность векторов $x_{1},\dots,x_{t}$ и выдает на выходе одну последовательность $z_{1},\dots,z_{t}$ той же длины $t$.

Затем мы рассмотрим, как работать с более сложными входными и выходными данными.

Сверточная нейронная сеть использует *ядро* - небольшой вектор весов $w_{1},\dots w_{k}$ - для извлечения признаков. Она делает это путем *свертки* этого ядра с входными данными. Свертка делает это в 3 шага:
1. Переворот ядра слева направо.
2. Покадровая передача ядра на входные данные
	- В каждом кадре вычисляется скалярное произведение ядра с локальными входными данными.
3. Выходные данные представляют собой результирующую последовательность скалярных произведений. 

Процесс свертки можно рассматривать как поиск областей в сигнале, которые похожи на ядро, поскольку скалярное произведение велико, когда два вектора похожи. Операция свертки представлена оператором $*$. Давайте посмотрим, как вычислить $x*w$, свертку одного вектора $x$ с вектором ядра $w$. Давайте сначала рассмотрим простой случай ширины ядра, равной 1. Мы вычисляем каждый выходной элемент $z_{j}$ как произведение ядра на $x_{j}$:
$$
\text{convolution with width-1 kernel:}\quad \mathrm{z}_{j}=x_{j}w_{1}\quad\forall j:1\leq j\leq t
$$
На рисунке ниже показана иллюстрация этого вычисления.

![[Pasted image 20251227234759.png]]

Теперь перейдем к более длинным ядрам. Хотя был описан первый шаг свертки как переворачивание ядра, на самом деле в системах распознавания речи этот шаг пропускается. Технически это означает, что используемый нами алгоритм на самом деле не является сверткой, а представляет собой *кросс-корреляцию*, то есть алгоритм, который перемещает ядро по сигналу, вычисляя его скалярное произведение кадр за кадром, без предварительного переворачивания ядра. Разница не имеет значения, поскольку параметры ядра будут обучены во время обучения, и поэтому модель может легко обучить ядро с параметрами в любом порядке. Тем не менее, по историческим причинам мы по-прежнему называем этот процесс одномерной сверткой, а не кросс-корреляцией. 

Давайте рассмотрим более общее уравнение для этих более длинных ядер. Чтобы избежать неопределенности свертки на левом и правом краях сигнала, мы можем *дополнить* вход, добавив небольшое количество $p$ нулей в начале и конце сигнала, так что мы можем начать центр ядра с первого элемента $x_{1}$, и слева от $x_{1}$ будет определенное значение. Это также упрощает установление длины выходного вектора равной длине входного. Для этого удобно определить вектор ядра как имеющий нечетное число элементов длиной $k=2p+1$, таким образом, центральный элемент будет иметь $p$ элементов с каждой стороны. Каждый элемент $z_{j}$ выходного вектора $z$ затем вычисляется как следующее скалярное произведение:
$$
\mathrm{z}_{j}=\sum_{i=-p}^{p}x_{j+i}w_{i+p}
$$
На рисунке ниже показаны вычисления свертки $x*w$ с ядром ширины 3 и с добавлением 1 в начале и конце $x$ значением, равным нулю.

![[Pasted image 20251228000046.png]]

Обратите внимание, что размер $k$ (*рецептивное поле*) ядра разработан таким образом, чтобы быть малым по сравнению с сигналом. Например, для сверточных слоев Whisper ширина ядра составляет 3 кадра, что означает, что ядро представляет собой вектор размером 3(мы говорим, что ядро имеет рецептивное поле 3). Это означает, что ядро сравнивается с 3 кадрами речи. в Whisper кадр появляется каждые 10 мс, и каждый кадр представляет собой окно в 25 мс информации из 40 мс речи. Этого достаточно, чтобы извлечь различные фонетические признаки, такие как формантные переходы или смыкания взрывных согласных, или аспирацию.

Было описано упрощенное представление свертки, в котором вход представляет собой один вектор $x$, а выход один вектор $z$, оба соотвествующие сигналу во времени. На практике входом сверточного слоя обычно является выход логарифмического мел-спектра, что означает, что он имеет много каналов, по одному для каждого выхода логарифмического мел-фильтра. Ядро будет иметь отдельные векторы для каждого из этих входных каналов. Мы говорим, что ядро имеет *глубину* 128, что означает, что ядро имеет форму $[128,3]$. Чтобы получить выход ядра, мы суммируем по всем входным каналам. То есть, мы получаем один выход $z^{c}$ для каждого из входных каналов $x^{c}$, свернув ядро $w$ с ним, а затем суммируем все результирующие выходы:
$$
z= \sum_{c=1}^{C_{i}}x^{c}*w
$$
Выходной сигнал в кадре $j$, $z_{j}$, таким образом, интегрирует информацию из всех входных каналов.

Наконец, выходной сигнал сверточного слоя также сложнее, чем просто вектор, состоящий из одного скалярного произведения для представления каждого кадра. Вместо этого, выходной сигнал сверточного слоя для данного входного кадра должен представлять собой эмбеддинг, латентное представление этого кадра. Как и во всех нейронных моделях, латентные представления должны иметь размерность модели, какой бы она не была. Например, размерность модели Whisper составляет 1280, поэтому сверточный слой должен иметь один выходной канал для каждого из этих 1280 измерений модели. Для этого мы фактически обучим одно отдельное ядро для каждого из измерений модели. То есть мы будем обучать 1280 отдельных ядер, каждое из которых будет иметь глубину, равную количеству входных каналов, и ширину фильтра. Таким образом, представление каждого кадра будет содержать 1280 независимо вычисленных признаков входного сигнала. 
Схема представлена ниже.

![[Pasted image 20251228002642.png]]

Одномерный сверточный слой также может иметь *шаг*. *Шаг* - это величина, на которую мы перемещаем ядро по входному сигналу между каждым шагом. На рисунках выше показан шаг, равный 1, что означает, что мы сначала позиционируем ядро над $x_{1}$, затем над $x_{2}$, затем над $x_{3}$ и тд. Для шага, равного 2, мы сначала позиционируем ядро над $x_{1}$, затем над $x_{3}$, затем над $x_{5}$ и тд. Больший шаг означает более короткую выходную последовательность; шаг равный двум, означает, что выходная последовательность $z$ будет вдвое короче, чем входная последовательность $x$. Сверточные слои с шагом больше 1 обычно используются для укорачивания входной последовательности. Это полезно, частично потому, что более короткий сигнал требует меньше памяти и вычислительной мощности, но также, как мы увидим, потому что это помогает устранить несоответствие между длиной акустических встраиваний кадров и буквами или словами, которые покрывают гораздо большую часть сигнала.

Наконец, на практике за сверточным слоем может следовать нелинейный выходной слой, например, слой ReLU.

