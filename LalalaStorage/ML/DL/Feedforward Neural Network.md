---
title: Feedforward Neural Network
created: 2025-11-16
tags:
  - deep_learning
  - supervised
links:
  - "[[Нейронные сети]]"
---
**Определение 1:** *Feedforward Neural Network* - простейшая многослойная нейронная сеть, в котором юниты соединены без циклов.  

Иногда такие нейронные сети называют *многослойными персептронами*. 

Простая сеть прямого распространения имеет 3 типа нод: входные юниты, скрытые юниты и выходные юниты. Слои между входным и выходным называются *скрытыми*, которые состоят из *скрытых юнитов*. Они являются ядром feedforward network. В стандартной архитектуре слои *полностью соединены* между собой. Для вычислительного ускорения иногда объединяют вектора весов в матрицу весов, а сдвиги в вектор сдвигов.

![[Pasted image 20251116093046.png]]

Выходом скрытого слоя является вектор $h$:
$$
h=\sigma(Wx+b)
$$
Для задач классификации мы не можем просто использовать выход скрытого слоя, нам нужны вероятности. Нам нужна функция нормализации вектора действительных чисел в вектор вероятностного распределения: функция $\text{softmax}$:
$$
\text{softmax}(z_{i}) = \frac{\exp(z_{i})}{\sum_{j=1}^{d}\exp(z_{j})}\quad 1\leq i\leq d
$$
Слои нейронной сети на рисунке выше:
$$
\begin{array} \\
h&=&\sigma(Wx+b) \\
z & = & Uh \\
y &=&\text{softmax}(z)
\end{array}
$$
Алгоритм вычисления прямых шагов сети прямого распространения из $n$ слоёв:
```pseudo
\begin{algorithm}
\begin{algorithmic}
\For{ $i = 1, \dots, n$}
  \State $z^{[i]}= W^{[i]} a^{[i-1]} + b^{[i]}$
  \State $a^{[i]}=g^{[i]}(z^{[i]})$
\EndFor
\State $\hat{y}= a^{[n]}$
\end{algorithmic}
\end{algorithm}
```
, где $z^{[i]}$ это комбинация предыдущих слоев и весов со сдвигом, $a^{[i]}$ - выход текущего слоя, $g$ - функция активации, где в скрытых слоя используют $\text{ReLU}$ или $\tanh$, а для выходного слоя $\text{softmax}$. Чтобы была заметна разница между слоями, то нужно использовать нелинейные функции активации. Также иногда для упрощения вместо сдвига, добавим $x_{0}=1$, а сдвиг в матрицу весов $W$.

![[Pasted image 20251116095858.png]]


Если данные представляют собой матрицу, то можно конвертировать в вектор с помощью 2 вещей: конкатенации и pooling. Через конкатенацию мы просто соединяем все строки в единый вектор. А через pooling мы превращаем матрицу в одну строку. Pooling может быть  **mean-pooling** или **max-pooling**. Рассмотрим **mean-pooling**:
$$
x_{mean} = \frac{1}{N} \sum_{i=1}^{N}\mathrm{e}(w_{i})
$$
Архитектура модели с pooling:
![[Pasted image 20251116105533.png]]

Для языковых моделей, построенных на этой архитектуре также считают вероятность слов приближением, поэтому для более простых задач лучше использовать классические [[N-gram model|n-gram]] модели, но при этом нейронные сети лучше обобщают контекст близких слов.
![[Pasted image 20251116110511.png]]

## Обучение

В качестве лосс функции используем кросс-энтропийный лосс, в качестве оптимизатора градиентные методы. Алгоритм для обучения feedforward neural network называется [[Error Backpropagation|алгоритм обратного распространения ошибки]].

Также для предотвращения переобучения используется регуляция и одна из них это исключение(dropout): случайное исключение юнитов и их связей с сетью во время обучения. На каждой итерации выбирается вероятность $p$ и для каждого юнита с вероятностью $p$  его значение заменяется на нули(и нормализирует остаток выхода из этого слоя).

Также используется тюнинг гиперпараметров модели.