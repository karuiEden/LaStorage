---
title: LSTM
created: 2025-12-23
tags:
  - deep_learning
links:
  - "[[RNN]]"
---
На практике довольно сложно обучать RNN для задач, требующих от сети использования информации, удаленной от текущей точки обработки. Несмотря на доступ ко всей предшествующей последовательности, информация, закодированная в скрытых состояниях, как правило, достаточно локальна, более релевантна самым последним частям входной последовательности и недавним решениям. Тем не менее, удаленная информация имеет решающее значение для многих приложений обработки языка.

Одна из причин неспособности RNN передавать критически важную информацию заключается в том, что скрытые слои, и, соответственно, веса, определяющие значения в скрытом слое, должны одновременно выполнять 2 задачи: предоставлять информацию, полезную для текущего решения, и обновлять и передавать информацию, необходимых для будущих решений.

Вторая трудность при обучении RNN возникает из необходимости обратного распространения сигнала ошибки во времени. На рисунке выше показано, что скрытый слой в момент времени $t$ вносит вклад в потери на следующем временном шаге, поскольку он участвует в этом вычислении. В результате, во время обратного прохода обучения скрытые слои подвергаются многократным умножениям, определяемым длиной последовательности. Частым результатом этого процесса является то, что градиенты в конечном итоге стремятся к нулю, ситуация, называемая проблемой *исчезающих градиентов*.

![[Pasted image 20251223185434.png]]

Для решения этих проблем были разработаны более сложные сетевые архитектуры, явно управляющие задачей поддержания актуального контекста во времени, позволяя сети научиться забывать информацию, необходимую для принятия решения в будущем.

Наиболее часто используемым расширением для RNN является сеть *долговременной кратковременной памяти (LSTM)*. LSTM делят проблему управления контекстом на две подзадачи: удаление из контекста информации, которая больше не нужна, и добавление информации, которая, вероятно, понадобится для принятия решений в будущем. Ключ к решению обеих проблем заключается в том, чтобы научиться управлять этим контекстом, а не жестко закладывать стратегию в архитектуру. LSTM достигают этого, сначала добавляя в архитектуру явный контекстный слой, и используя специализированные нейронные блоки, которые используют вентили для управления потоком информации в блоки, составляющие слои сети, и из них. Эти вентили реализованы с помощью дополнительных весов, которые последовательно воздействуют на входные данные, предыдущий скрытый слой и предыдущие контекстные слои.

Вентили в LSTM имеют общую схему проектирования: каждый состоит из прямого слоя, за которым следует сигмоидная функция активация, а затем поточечное умножение на слой, подвергаемый вентилированию. Выбор сигмоидной функции активации обусловлен её склонностью смещать выходные значения либо в 0, либо в 1. Сочетание этого с поточечным умножением дает эффект, аналогичный эффекту бинарной маски. Значения в подвергаемом вентилированию слое, совпадающие со значениями, близкими к 1 в маске, проходят практически без изменений.  Значения, соответствующие меньшим значениям, по сути стираются.

Первый вентиль - это *вентиль забывания*. Цель этого вентиля - удалить из контекста информацию, которая больше не нужна. Вентиль забывания вычисляет взвешенную сумму скрытого слоя предыдущего состояния и текущего входного сигнала и пропускает её через сигмоидную функцию. Затем эта маска поэлементно умножается на вектор контекста, чтобы удалить из контекста информацию, которая больше не нужна. Поэлементное умножение двух векторов(называемое произведением Адамара) - это вектор той же размерности, что и два входных вектора, где каждый элемент $i$ является произведением элемента $i$ двух входных векторов:
$$
\begin{array} \\
\mathrm{f}_{t}=\sigma(\mathrm{U}_{f}\mathrm{h}_{t-1}+\mathrm{W}_{f}\mathrm{x}_{t})\\
\mathrm{k}_{t}=\mathrm{c}_{t-1}\odot \mathrm{f}_{t}
\end{array}
$$

Следующая задача  - вычислит фактическую информацию, которую нам необходимо извлечь из предыдущего скрытого состояния и текущих входных данных - тот же базовый вычислительный процесс, который мы использовали для всех наших рекуррентных сетей.
$$
\mathrm{g}_{t}=\tanh(\mathrm{U}_{g}\mathrm{h}_{t-1}+\mathrm{W}_{g}\mathrm{x}_{t})
$$
Далее мы генерируем маску для вентиля добавления, чтобы выбрать информацию, которую нужно добавить к текущему контексту.
$$
\begin{array} \\
i_{t} = \sigma(\mathrm{U}_{i}\mathrm{h}_{t-1}+\mathrm{W}_{i}\mathrm{x}_{t}) \\
\mathrm{j}_{t}=\mathrm{g}_{t}\odot\mathrm{i}_{t}
\end{array}
$$
Далее мы добавляем это к измененному контекстному вектору, чтобы получить наш новый контекстный вектор.
$$
\mathrm{c}_{t} = \mathrm{j}_{t} + \mathrm{k}_{t}
$$
Последний вентиль, который используют, - это выходной вентиль, который используется для определения того, какая информация необходима для текущего скрытого состояния.
$$
\mathrm{o}_{t} = \sigma(\mathrm{U}_{o}\mathrm{h}_{t-1}+\mathrm{W}_{o}\mathrm{x}_{t})
\mathrm{h}_{t} = \mathrm{o}_{t} \odot \tanh(\mathrm{c}_{t})
$$
На рисунке ниже показан полный процесс вычислений для одного блока LSTM. При наличии соответствующих весов для различных вентилей, LSTM принимает на вход контекстный слой и скрытый слой с предыдущего временного шага, а также текущий входной вектор. Затем он генерирует обновленные контекстный и скрытый векторы в качестве выходных данных. 

Именно скрытое состояние $\mathrm{h}_{t}$ обеспечивает выходные данные для LSTM на каждом временном шаге. Эти выходные данные могут использоваться в качестве входных данных для последующих слоев в многослойной RNN, или на последнем слое сети $\mathrm{h}_{t}$ может использоваться для обеспечения окончательных выходных данных LSTM.

![[Pasted image 20251223193551.png]]

### Закрытые блоки, уровни и сети

Нейронные блоки, используемые в LSTM, очевидно, гораздо сложнее, чем те, что используются в базовых сетях прямого распространения. К счастью, эта сложность инкапсулирована в базовых блоках обработки, что позволяет нам сохранять модульность и легко экспериментировать с различными архитектурами. Чтобы убедиться в этом, есть рисунок ниже, на котором показаны входы и выходы, связанные с каждым типом блока. В крайнем левом углу (а) показан базовый блок прямого распространения, где один набор весов и одна функция активации определяют его выход, и при размещении в слое между блоками в слое нет связей. Далее блок (б) представляет блок в простой RNN. Теперь у него два входа и один дополнительный набор весов. Однако по-прежнему есть одна функция активации и один выход. 
Повышенная сложность блоков LSTM инкапсулирована в самом блоке. Единственная дополнительная внешняя сложность для LSTM по сравнению с базовым блоком RNN (б) заключается в наличии дополнительного контекстного вектора в качестве входа и выхода. Эта модульность является ключом к мощности и широкой применимости блоков LSTM. Блоки LSTM могут быть использованы в любой из архитектур RNN. И, как в случае с простыми RNN, многослойные сети, использующие вентильные блоки, могут быть развернуты в глубокие сети прямого распространения и обучены обычным способом с помощью обратного распространения ошибки. Поэтому на практике LSTM, а не RNN стали стандартным блоком для любой современной системы, использующие RNN.

![[Pasted image 20251223195217.png]]

