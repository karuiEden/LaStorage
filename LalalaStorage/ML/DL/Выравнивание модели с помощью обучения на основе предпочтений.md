---
title: Выравнивание модели с помощью обучения на основе предпочтений
created: 2025-12-09
tags:
  - deep_learning
links:
  - "[[Обучение на основе предпочтений]]"
---
Текущие подходы для выравнивания LLM, используя данные предпочтений, основаны на структуре обучения с подкреплением. В условиях RL модели выбирают последовательности действий на основе политик, которые используют характеристики текущего состояния. Среда предоставляет награду за каждое выполненное действие, где награда за целую последовательность является функцией вознаграждения за действия, составляющие всю последовательность. Цель обучения в RL заключается в максимизировании общего вознаграждения за некоторый период обучения. Применяя RL для оптимизации LLM, мы будем использовать следующую структуру:
- *Действия* соответствуют выбору токенов, сделанному во время авторегрессивной генерации;
- *Состояния* соответствуют контексту текущего шага декодирования. То есть, история токены, сгенерированных к этому моменту;
- *Политики* соответствуют вероятностным языковым моделям, реализованным в предварительно обученных LLM;
- *Вознаграждения* для LLM выходов основаны на вознаграждающих моделях, полученных на основе данных о предпочтениях.

В соответствии с этой схемой RL, мы будем называть предварительно обученные LLM политиками, $\pi$, а оценки предпочтений, связанные с промптами и выходами, вознаграждениями, $r(x,o)$.

Таким образом, нашей целью является обучить политику, $\pi_{\theta}$, которая максимизирует вознаграждения за результаты, полученные с помощью политики, учитывая модель вознаграждения, полученную на основе данных о предпочтениях. То есть, мы хотим обученную на предпочтениях LLM для генерации выходов с высокими вознаграждениями. Мы можем выразить это как задачу оптимизации следующим образом:

$$
\pi^{*}= \text{argmax}_{\pi_{0}} \mathbb{E}_{x\thicksim\mathcal{D},o\thicksim\pi_{\omega}(o\ | \ x)}[r(x,o)]
$$

^6b041f

Используя эту формулу, мы выберем промпты $x$ из набора соответствующих обучающих промптов, выборку выходных данных $o$ из заданной политики, и оцениваем вознаграждение для каждого элемента. Средняя награда по обучающим выборкам дает нам ожидаемое вознаграждение для $\pi_{\theta}$, с целью найти политику, которая максимизирует вознаграждение.

Здесь две ключевых разницы между традиционным RL и способе, который обычно применяется для выравнивания LLM. Первое отличие  в том, что в традиционном RL,  сигнал вознаграждения приходит из окружения и отражает наблюдаемый факт о результате действия. С обучением по предпочтениям, обученная модель вознаграждения служит лишь шумовым заменителем истинной модели вознаграждения.
Второе отличие заключается в стартовой точке для обучения. Обычные RL приложения стремятся обучить оптимальную политику с нуля, то есть на основе случайно заданной политики. Здесь, мы начинаем с моделями, которые уже демонстрируют высокую производительность - модели, которые были предварительно обучены на основе большого количества данных, затем тонко настроены, используя настройку по инструкций, и только затем улучшены с помощью данных о предпочтениях. Акцент здесь делается не на радикальном изменении поведения существующей модели, а на подталкивании её к предпочтительному поведению.

![[Pasted image 20251209184556.png]]

Имея это, если мы оптимизируем [[#^6b041f|выражение]] для наград, предварительно обученная модель, как правило, забудет все, чему она научилась во время предварительного обучения, поскольку она переключается на поиск высоких вознаграждений из относительно небольшого количества доступных данных о предпочтениях. Чтобы избежать это, в функцию вознаграждения добавляется член, который штрафует модели, слишком сильно отклоняющиеся от начальной точки.
$$
\pi^{*}= \text{argmax}_{\pi_{\theta}} \mathbb{E}_{x\thicksim\mathcal{D},o\thicksim\pi_{\theta}(o\ | \  x)}[r(x,o)-\beta \mathbb{D}_{\mathrm{KL}}[\pi_{\theta}(o\ | \ x)||\pi_{\mathrm{ref}})o\ | \ x]]
$$
Второй член в этой формуле, $\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}(o\ | \ x)||\pi_{\mathrm{ref}}(o\ | \ x))$, это Кульбака-Лейблера (KL) дивергенция. Вкратце, KL дивергенция измеряет расстояние между двумя вероятностными распределениям. Член $\beta$ - это гиперпараметр, который модулирует влияние этого штрафного члена. Для политик, основанных на LLM, KL дивергенция - это логарифм отношения обученной политики к исходной эталонной политике $\pi_{\mathrm{ref}}$.

$$
\pi^{*}= \text{argmax}_{\pi_{\theta}} \mathbb{E}_{x\thicksim\mathcal{D},o\thicksim\pi_{\theta}(o\ | \ x)} \left[ r_{\phi}(x,o)- \beta\frac{ \pi_{\theta}(o\ | \ x)}{\pi_{\mathrm{ref}}(o\ | \ x)} \right] 
$$

^c7a576

### Прямая оптимизация предпочтений

Прямая оптимизация предпочтений (DPO) использует градиентное обучение для оптимизации кандидатов LLM, используя данные о предпочтений, без обучения явной модели вознаграждения или выборки из обновляемой модели.

DPO начинается с [[#^c7a576|максимизации с ограничениями KL]]. Ключевая идея DPO заключается в том, чтобы переписать решение в замкнутой форме для этой максимизации, чтобы выразить вознаграждающую функцию $r(x,o)$ через оптимальную политику $\pi^{*}$ и исходную политику $\pi_{\mathrm{ref}}$.

$$
r(x,o)= \beta \log \frac{\pi_{r}(o\ | \ x)}{\pi_{\mathrm{ref}}(o\ | \ x)} +\beta \log Z(x),
$$

^4953cb

где $Z(x)$ - это статистическая сумма - сумма по всем возможным выходам $o$ при заданном промпте $x$.

$$
Z(x)=\sum_{y}\pi_{\mathrm{ref}}(o\ | \ x)\exp\left(  \frac{1}{\beta}r(x,o)  \right)
$$

Суммирование в этой функции распределения делает любое её прямое использование нецелесообразным. Однако, поскольку модель Брэдли-Терри основана на разнице вознаграждений за предметы, подставляя [[#^4953cb|это]] в модель Брэдли-Терри:

$$
P(o_{i}\succ o_{j})=\sigma(r(x,o_{i})-r(x,o_{j}))=\sigma \left( \beta \log \frac{\pi_{\theta}(o_{i}\ | \ x)}{\pi_{\mathrm{ref}}(o_{i}\ | \ x)} - \beta  \frac{\pi_{\theta}(o_{j}\ | \ x)}{\pi_{\mathrm{ref}}(o_{j}\ | \ x)}  \right)
$$
С этим изменением, DPO выражает вероятность пары предпочтений в терминах двух политик, а не в терминах явной модели вознаграждения. Учитывая это, потеря кросс-энтропии для одного экземпляра:

$$
L_{\mathrm{DPO}}(x,o_{w},o_{l})=-\log \sigma \left( \beta \log \frac{\pi_{\theta}(o_{w}\ | \ x)}{\pi_{\mathrm{ref}}(o_{w}\ | \ x)} - \beta \log \frac{\pi_{\theta}(o_{l}\ | \ x)}{\pi_{\mathrm{ref}}(o_{l}\ | \ x)} \right) 
$$
И потеря по всему обучающему набору $\mathcal{D}$:

$$
L_{\mathrm{DPO}}(\pi_{\theta})= - \mathbb{E}_{(x,o_{w},o_{l})\thicksim\mathcal{D}} \left[ \log\sigma \left( \beta \log \frac{\pi_{\theta}(o_{w}\ | \ x)}{\pi_{\mathrm{ref}}(o_{w}\ | \ x)} - \beta \log \frac{\pi_{\theta}(o_{l}\ | \ x)}{\pi_{\mathrm{ref}}(o_{l}\ | \ x)} \right)  \right] 
$$

Эта потеря следует из производной сигмоиды. С точки зрения эксплуатации, конструкция этой функции потерь и соответствующее обновление на основе градиента увеличивают вероятность предпочтительных вариантов и уменьшают вероятность нежелательных. Это уравновешивает эту цель с целью не отклоняться слишком далеко от $\pi_{\mathrm{ref}}$ из-за штрафов KL. Коэффициент $\beta$ - это гиперпараметр, который контролирует штрафующий член; значения $\beta$ обычно находятся в диапазоне от $0.1$ до $0.01$.

![[Pasted image 20251209193712.png]]

