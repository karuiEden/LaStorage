---
title: Двунаправленные энкодер-трансформеры
created: 2025-11-21
tags:
  - deep_learning
links:
  - "[[Трансформеры]]"
---
Природа казуального трансформера является ограничением, так как существуют задачи, для которых  при обработке токена было бы полезно иметь возможность заглядывать в будущие токены. Это особенно нужно для задач маркировки последовательностей, в которых мы хотим пометить каждый токен меткой, например, для задачи маркировки именованной сущности.
*Двунаправленные энкодеры* - это немного другое, чем казуальные модели. Двунаправленные энкодеры фокусируются на вычислении контекстуализированных представлений входных токенов. Двунаправленные энкодеры используют внутреннее внимание для сопоставления входных векторных представлений $(x_{1},\dots,x_{n})$ c последовательностями выходных эмбеддингов той же длины $(h_{1},\dots,h_{n})$, где выходные векторы контекстуализированы с использованием информации из всей входной последовательности. Эти выходные эмбеддинги являются контекстуализированными представлениями каждого входного токена, которые полезны в разных задачах, где нужно выполнить классификацию или принять решение на основе токена в контексте.
В отличии от казульных моделей, которые называют *decoder-only*, двунаправленные энкодеры называют *encoder-only* моделями

## Архитектура двунаправленных маскированных моделей

Двунаправленные энкодеры отличаются в плане архитектуры от [[Трансформеры|казуальных моделей]] в 2 случаях: в механизме внимания и процессе обучения модели.

### Вычисление внимания

Механизм внутреннего внимания в двунаправленных энкодерах не казуальный, а является двусторонним, то есть для вычисления внимания токена $i$ мы можем использовать токен $i+1$ и так далее. 

![[Pasted image 20251121101446.png]]

Реализация очень проста, если в казуальном механизме внимания мы маскировали матрицу $\mathbf{QK^{T}}$, чтобы скрыть комбинации с будущими токенами, то теперь мы используем полную матрицу $\mathbf{QK^{T}}$. 

![[Pasted image 20251121101714.png]]

Тогда вычисление внимания будет выглядеть так:
$$
\mathbf{\text{head}} = \text{softmax}\left(  \frac{\mathbf{QK^{T}}}{\sqrt{ d_{k} }}  \right)\mathbf{V}
$$
### Обучение 

Так как мы сняли маску с матрицы $\mathbf{QK^{T}}$, то просто заставлять модель предсказывать следующее слово, мы не можем, так как оно доступно прямо из контекста. Поэтому вместо предсказания следующего слова модель учится выполнять задание по заполнению пропусков, которое технически называется *clozure task*("закрыть"). То есть вместо предсказаний в конце ("The water of Walden Pond is so beatifully ___ ") мы просим модель предсказать пропущенное слово, имея остальное предложение ("The ___ of Walden Pond is so beatifully ... ")
То есть, имея входную последовательность с одним или несколькими пропущенными элементами, задача обучения заключается в предсказании пропущенных слов. Точнее, во время обучения модель лишается одного или нескольких токенов входной последовательности и должна сгенерировать вероятностное распределение по словарю для каждого пропущенного метода. Затем используется потеря кросс-энтропии для каждого предсказания модели для управления процессом обучения.
Этот подход может быть обобщен для любых вариаций методов, которые повреждают обучающий вход и затем просят модель восстановить его. Имя такого типа обучения называют *denoising*: мы повреждаем ввод разными способами и цель системы удалить шум.

#### Маскировка слов

Опишем подход *Маскированного моделирования языка(Masked Language Modeling)* для обучения двустороннего энкодера. MLM использует неаннотированный текст из большого корпуса. В обучении MLM, модели предоставляется ряд предложений из обучающего корпуса, в котором определённый процент токенов был выбран случайным образом для процедуры маскирования. Имея входное предложение "lunch was delicious",  выберем 3 токен "delicious" для обработки:
- $80\%$  - Токен будет заменен специальным словарным токеном **\[MASK]**  
- $10\%$ - Токен будет заменен другим токеном, выбранным случайно из словаря по юниграмной вероятности токена.
- $10\%$ - Токен будет оставаться неизменённым.
Затем мы обучаем модель угадывать правильные токены в обработанных токенах. Три возможные манипуляции нужны, чтобы модель всегда предсказывала входной токен. Добавление **\[MASK]** создает несоответствие между предобучением и последующей тонкой настройкой или выводом, поскольку при использовании модели MLM для выполнения последующей задачи мы не используем токен **\[MASK]**. Если бы токены просто заменили на **\[MASK]**, модель могла бы предсказывать токены только при обнаружении **\[MASK]**.

Чтобы обучить модель делать предсказания, изначальную входную последовательность токенизируют разбивая на под слова и затем токены сэмплируют, чтобы подменить их. Эмбеддинги слов извлекаются из матрицы эмбеддингов $E$ и комбинируются с позиционными эмбеддингами, затем подаются на вход блоку трансформера, проходят через стак блоков, и затем на головку моделирования языка. Цель **MLM** обучения - предсказывать изначальный вход для каждого замаскированного слова и потеря кросс-энтропии по этим предсказаниям управляет процессом обучения всех параметров модели. То есть, все входные токены играют роль в вычислении собственного внимания, но только выбранные токены используются для обучения.

![[Pasted image 20251203125722.png]]

С помощью предсказанного распределения вероятности из головки моделирования языка мы можем использовать кросс-энтропию для каждого замаскированного элемента - отрицательную логарифмическую вероятность:
$$
L_{MLM}(x_{i}) = -\log P(x_{i}\ | \  h_{i}^{L})
$$
Градиент в этой форме для обновления весов основан на средней потере по всем выбранным обучающим элементам из одной обучающей последовательности:
$$
L_{MLM} = - \frac{1}{|M|} \sum_{i\in M}\log P(x_{i}\ | \ h_{i}^{L})
$$
Нетрудно заметить, что только токены в $M$ играют роль в обучении модели.

#### Предсказание следующего предложения

В основном обучение, основанное на маскировании слов, используется для предсказаний слов из окружающего контекста с целью создания эффективных представлений на уровне слов. Однако важный класс задач включает в себя определения связи между двумя предложениями. К ним относятся задачи парафразы(определения насколько предложения схожи по смыслу), импликации(определения влекут ли за собой значения предложений друг за другом, или противоречат друг другу) или связность дискуссии(определение, образуют ли два предложения связный текст).

Чтобы охватить этот тип знаний, требуемые для этих задач, некоторые модели из семейства BERT включают в себя вторую задачу обучения, которая называется *предсказанием следующего предложения*. В данной задаче, модели давалась пара предложений и затем просят её предсказать, является ли данная пара предложений настоящей связной парой из обучающего корпуса или несвязной парой. В BERT 50% обучающих пар является настоящими, а остальные 50% пар являются случайно выбранными предложениями в остальном корпусе. Потеря NSP основывается на том, насколько модель хорошо различает настоящие пары от случайных пар.
Для облегчения обучения NSP BERT добавляет два специальных токена к входному представлению. После токенизации с помощью модели подслов токен **\[CLS\]**  добавляется к паре предложений, а токен **\[SEP\]** помещается между предложениями и после последнего токена второго предложения. Также есть два специальных токена: "First Segment" токен и "Second Segment" токен. Они добавляются на этапе встраивания эмбеддингов слов и позиционными эмбеддингами. То есть, каждый входной токен из матрицы $X$ формируется сложением трех эмбеддингов: слово, позиция и первый/второй сегмент. 

Во время обучения выходной вектор $h^{L}_{CLS}$ из последнего слоя, связанный с токеном **\[CLS\]**, представляет собой прогноз следующего предложения. Как и в случае с **MLM**, мы добавляем специальную голову, в этом случае **NSP** голову, которая содержит обучаемое множество классификационных весов $W_{NSP}\in \mathbb{R}^{d\times{2}}$, которое создает двухклассовое предсказание из сырого **\[CLS\]** вектора $h^{L}_{CLS}$:
$$
y=\text{softmax}(h^{L}_{CLS}W_{NSP})
$$
Кросс энтропия используется для вычисления потери **NSP** для каждой пары предложения

![[Pasted image 20251203141545.png]]

#### Режимы обучения

Для обучения исходных моделей BERT пары частей текста были выбраны со схемой предсказания следующего предсказания 50/50. Пары были выбраны таким образом, чтобы их входная длина была не больше, чем 512 токенов. Токены внутри этих пар были замаскированы с помощью подхода MLM, при этом для окончательной потери использовались потери от целей MLM и NSP. Поскольку окончательная потеря распространяется обратно по всему трансформеру, эмбеддинги на каждом слое трансформера будут изучать представления, полезные для прогнозирования слов по соседним словам. Так как токены **\[CLS\]** являются прямыми входными данными классификатора NSP, их изученные представления будут содержать информацию о всей последовательности в целом. Для сходимости моделей потребовалось около 40 проходов по обучающим данным.

Некоторые модели, например RoBERTa, отбрасывают цель предсказания следующего предложения, и, следовательно, немного меняют режим обучения. Вместо выборки пар предложений, входные данные представляют собой просто серию смежных предложений, по прежнему начинающихся с токена **\[CLS\]**. Если документ заканчивается до достижения 512 токенов, добавляется дополнительный токен-разделитель, и предложения из следующего документа упаковываются, пока не будет достигнуто 512 токенов. Обычно используются большие размеры пакетов, от 8 до 32 тысяч токенов.

Многоязычным моделям приходится принимать дополнительное решение: какие данные использовать для построения словарного запаса? Напомним, что все языковые модели используют токенизацию подслов. Какой текст следует использовать для обучения этой многоязычной токенизации, учитывая, что некоторые языки имеют гораздо больше текста, чем другие? Одним из вариантов было создание этого набора даных для изучения словарного запаса путем случайной выборки предложений из наших обучающих данных. В этом случае мы выберем множество из языков с большим количеством веб-представлений, таких как английский, и токены будут смещены в сторону редких английских токенов вместо создания частных токенов из языков с меньшим количеством данных. Вместо этого делят обучающие данные на подкорпусы $N$ различных языков, вычисляют количество предложений $n_{i}$ каждого языка $i$ и корректируют эти вероятности, чтобы повысить вероятность менее представленных языков. Новая вероятность выбора предложения с каждого из $N$ языков это $\{ q_{i} \}_{q=1}^{N}$, где
$$
q_{i}= \frac{p_{i}^{\alpha}}{\sum_{j=1}^{N}p_{j}^{\alpha}}\text{ with }p_{i}= \frac{n_{i}}{\sum_{k=1}^{N}n_{k}},
$$
где $\alpha \in[0,1]$, но оптимальное значение равно $\alpha=0.3$, что хорошо подходит для большей интеграции редких языков в токенизацию, что приводит к улучшению многоязычной производительности в целом.

Результат этого процесса предварительной подготовки состоит как из изученных эмбеддингов слов, так и из всех параметров двунаправленного энкодера, которые используются для создания контекстных эмбеддингов для новых входных данных.

Для многих целей предобученная многоязычная модель более практична, чем моноязычная модель, поскольку она избегает необходимости создания множества отдельных моноязычных моделей. Кроме того, многоязычные модели могут повысить производительность на языках с малым количеством информации, используя лингвистическую информацию из похожего языка, которые имеют больше информации. Тем не менее, когда количество языков становится очень большим, многоязычные модели демонстрируют то, что называется *проклятием  многоязычности*: производительность на каждом языке ухудшается по сравнению с моделью, обученной на меньшем количестве языков. Другая проблема многоязычных моделей заключается в том, что они "имеют акцент": грамматические структуры языков с большим количеством информации проникают в языки с меньшим количеством информации, таким образом, делая их похожими на язык с большим количеством информации.