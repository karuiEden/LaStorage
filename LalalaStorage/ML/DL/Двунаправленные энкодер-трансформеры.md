---
title: Двунаправленные энкодер-трансформеры
created: 2025-11-21
tags:
  - ds
links:
  - "[[Трансформеры]]"
---
Природа казуального трансформера является ограничением, так как существуют задачи, для которых  при обработке токена было бы полезно иметь возможность заглядывать в будущие токены. Это особенно нужно для задач маркировки последовательностей, в которых мы хотим пометить каждый токен меткой, например, для задачи маркировки именованной сущности.
*Двунаправленные энкодеры* - это немного другое, чем казуальные модели. Двунаправленные энкодеры фокусируются на вычислении контекстуализированных представлений входных токенов. Двунаправленные энкодеры используют внутреннее внимание для сопоставления входных векторных представлений $(x_{1},\dots,x_{n})$ c последовательностями выходных эмбеддингов той же длины $(h_{1},\dots,h_{n})$, где выходные векторы контекстуализированы с использованием информации из всей входной последовательности. Эти выходные эмбеддинги являются контекстуализированными представлениями каждого входного токена, которые полезны в разных задачах, где нужно выполнить классификацию или принять решение на основе токена в контексте.
В отличии от казульных моделей, которые называют *decoder-only*, двунаправленные энкодеры называют *encoder-only* моделями

## Архитектура двунаправленных маскированных моделей

Двунаправленные энкодеры отличаются в плане архитектуры от [[Трансформеры|казуальных моделей]] в 2 случаях: в механизме внимания и процессе обучения модели.

### Вычисление внимания

Механизм внутреннего внимания в двунаправленных энкодерах не казуальный, а является двусторонним, то есть для вычисления внимания токена $i$ мы можем использовать токен $i+1$ и так далее. 

![[Pasted image 20251121101446.png]]

Реализация очень проста, если в казуальном механизме внимания мы маскировали матрицу $\mathbf{QK^{T}}$, чтобы скрыть комбинации с будущими токенами, то теперь мы используем полную матрицу $\mathbf{QK^{T}}$. 

![[Pasted image 20251121101714.png]]

Тогда вычисление внимания будет выглядеть так:
$$
\mathbf{\text{head}} = \text{softmax}\left(  \frac{\mathbf{QK^{T}}}{\sqrt{ d_{k} }}  \right)\mathbf{V}
$$
### Обучение 

Так как мы сняли маску с матрицы $\mathbf{QK^{T}}$, то просто заставлять модель предсказывать следующее слово, мы не можем, так как оно доступно прямо из контекста. Поэтому вместо предсказания следующего слова модель учится выполнять задание по заполнению пропусков, которое технически называется *clozure task*("закрыть"). То есть вместо предсказаний в конце ("The water of Walden Pond is so beatifully ___ ") мы просим модель предсказать пропущенное слово, имея остальное предложение ("The ___ of Walden Pond is so beatifully ... ")
То есть, имея входную последовательность с одним или несколькими пропущенными элементами, задача обучения заключается в предсказании пропущенных слов. Точнее, во время обучения модель лишается одного или нескольких токенов входной последовательности и должна сгенерировать вероятностное распределение по словарю для каждого пропущенного метода. Затем используется потеря кросс-энтропии для каждого предсказания модели для управления процессом обучения.
Этот подход может быть обобщен для любых вариаций методов, которые повреждают обучающий вход и затем просят модель восстановить его. Имя такого типа обучения называют *denoising*: мы повреждаем ввод разными способами и цель системы удалить шум.

#### Маскировка слов

Опишем подход *Маскированного моделирования языка(Masked Language Modeling)* для обучения двустороннего энкодера. MLM использует неаннотированный текст из большого корпуса. В обучении MLM, модели предоставляется ряд предложений из обучающего корпуса, в котором определённый процент токенов был выбран случайным образом для процедуры маскирования. Имея входное предложение "lunch was delicious",  выберем 3 токен "delicious" для обработки:
- $80\%$  - Токен будет заменен специальным словарным токеном **\[MASK]**  
- $10\%$ - Токен будет заменен другим токеном, выбранным случайно из словаря по юниграмной вероятности токена.
- $10\%$ - Токен будет оставаться неизменённым.
