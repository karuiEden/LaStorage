---
title: Кодер-декодер архитектура
created: 2025-12-09
tags:
  - deep_learning
links:
---
Системы обучаются максимизировать вероятность последовательности целевых токенов $y_{1},\dots,y_{m}$, имея последовательность исходных токенов $x_{1},\dots,x_{n}$:
$$
P(y_{1},\dots,y_{m}\ | \  x_{1},\dots,x_{n})
$$
Вместо того, чтобы использовать входные токены напрямую, архитектура *кодер-декодер* состоит из 2 компонентов, *кодера* и *декодера*, который дополнен новым специальным слоем *cross-attention*. Энкодер принимает на вход токены $\mathrm{X}=x_{1},\dots,x_{n}$ и отражает их в выходное представление $\mathrm{H}^{enc}=h_{1},\dots,h_{n}$ с помощью стака блоков кодеров. Во время декодинга, система принимает $\mathrm{H}^{enc}$, и, токен за токеном генерирует выход $y$:
$$
\begin{array} \\
h=\mathrm{encoder}(x) \\
y_{t+1}=\mathrm{decoder}(h,y_{1},\dots,y_{t})\quad\forall t\in[1,\dots,m]

\end{array}
$$
Но компоненты этой архитектуры несколько отличаются от классического блока трансформера. Во-первых, для работы с исходным языком блоки трансформера имеют дополнительный слой *перекрестного внимания(cross-attention)*. Блок декодер трансформера включает в себя дополнительный слой со специальным типом внимания, перекрестное внимание(cross-attention)(иногда называют *encoder-decoder attention* или *source attention*). Cross-attention имеет ту же форму, что и [[Трансформеры#Multi-head attension|multi-head attention]] в обычном блоке трансформера, за исключением того, что запросы, как обычно поступают из предыдущего слоя декодера, а ключи и значения - с выхода кодера.

![[Pasted image 20251209214254.png]]

То есть, если в стандартном multi-head attention вводом для каждого слоя внимания является $\mathrm{X}$, то в cross-attention ввод - это последний вывод кодера $\mathrm{H}^{enc}=h_{1},\dots,h_{n}$. $H^{enc}$ имеет форму $[n\times d]$, каждая строка представляет один вводный токен. Для связки ключей и значений из кодера с запросом из предыдущего слой декодера, мы умножаем матрицу выхода кодера $\mathrm{H}^{enc}$ на весы ключей слоя cross-attention $\mathrm{W}^{K}$ и весы значений $\mathrm{W}^{V}$. Запросы формируются на основе выходных данных предыдущего слоя декодера $\mathrm{H}^{dec[\ell-1]}$, которые умножаются на весовые коэффициенты слоя cross-attention $\mathrm{W}^{Q}$:
$$
\mathrm{Q}=\mathrm{H}^{dec[\ell-1]}\mathrm{W^{Q}};\quad \mathrm{K}=\mathrm{H}^{enc}\mathrm{W^{K}};\quad \mathrm{V}=\mathrm{H}^{enc}\mathrm{W^{V}}
$$
$$
\mathrm{CrossAttention}(\mathrm{Q,K,V}) = \mathrm{softmax}\left( \frac{\mathrm{QK^{T}}}{\sqrt{ d_{k} }} \right) \mathrm{V}
$$

Таким образом, cross attention позволяет декодеру учитывать каждый токен исходной последовательности, проецируемый в итоговые представления кодера.

Обучается так же, как и обычные трансформеры, т.е. используя принуждение учителя(использует золотой токен вместо предсказанного для следующих токенов).