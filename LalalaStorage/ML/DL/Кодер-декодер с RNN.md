---
title: Кодер-декодер с RNN
created: 2025-12-26
tags:
  - deep_learning
links:
  - "[[RNN]]"
  - "[[Кодер-декодер архитектура]]"
---
Кодер-декодер сети, иногда называемые sequence-to-sequence, - это модели, способные генерировать контекстно подходящие выходные последовательности произвольной длины на основе входной последовательности. Сети кодер-декодер применяются в разных сферах, включая суммаризацию, ответы на вопросы и диалоги, но особенно популярны они для машинного перевода.

Ключевая идея этих сетей заключается в использовании сетью кодировщика, который принимает входную последовательность и создает её контекстуализированное представление, часто называемое контекстом. Затем это представление передается декодеру, который генерирует выходную последовательность, специфичную для данной задачи.

![[Pasted image 20251226143800.png]]

Кодер-декодер сети состоят из трех концептуальных компонентов:
1. *Кодировщик*, который принимает на вход последовательность, $x_{1:n}$, и генерирует соответствующую последовательность контекстуализированных представлений, $h_{1:n}$. В качестве кодировщиков могут использоваться LSTM-сети, свёрточные сети и трансформеры.
2. *Контекстный вектор*, $c$, являющийся функцией от $h_{1:n}$, передает суть входных данных декодеру.
3. *Декодер*, который принимает $c$ как вход и генерирует последовательность скрытых состояний произвольной длины $h_{1:m}$, из которой можно получить соответствующую последовательность выходных состояний $y_{1:m}$. Как и в случае с кодировщиками, декодеры могут быть реализованы с помощью любой последовательной архитектуры.

В моделировании языка RNN в определенный момент времени $t$ пропускается префикс $t-1$ токенов через языковую модель, используя прямой вывод(инференс) для получения последовательности скрытых состояний, заканчивающейся скрытым состоянием, соответствующим последнему слову префикса. Затем используется конечное скрытое состояние префикса в качестве отправной точки для генерации следующего токена.

Более формально, если $g$ - функция активации, такая как $\tanh$ или $\mathrm{ReLU}$, функция входных данных в момент времени $t$ и скрытого состояния в момент времени $t-1$, а softmax применяется к множеству возможных элементов словаря, то в момент времени $t$ выходные данные $y_{t}$ и скрытое состояние $h_{t}$ вычисляются следующим образом:

$$
\begin{array} \\
\mathrm{h}_{t} = g(\mathrm{h}_{t-1}, \mathrm{x}_{t}) \\
\hat{\mathrm{y}} = \mathrm{softmax}(\mathrm{h}_{t})
\end{array}
$$
Нужно внести лишь одно небольшое изменение, чтобы превратить эту языковую модель с авторегрессивной генерацией в модель кодер-декодера, которая является моделью перевода, способной переводить исходный текст на одном языке в целевой текст за секунду: добавить разделитель предложений в конце исходного текста, а затем просто объединить предложения целевого текста. 

Будем использовать \<s\> в качестве разделителя предложений и представим себе перевод английского исходного текста на испанское предложение. Мы также можем проиллюстрировать модели кодировщика-декодера на примере пары "вопросы-ответ" или пары "текст-суммирование". 

Давайте используем $x$ для обозначения исходного текста плюс разделитель \<s\>, а $y$ для обозначения целевого текста $y$. Тогда модель кодировщика-декодера вычисляет вероятность $p(y \ | \ x)$:

$$
p(y\ | \ x)= p(y_{1}\ | \ x)p(y_{2}\ | \ y_{1},x)p(y_{3}\ | \ y_{1},y_{2},x)\dots p(y_{m}\ | \ y_{1}, \dots,y_{m-1},x)
$$
На рисунке ниже показана упрощенная версия модели кодировщика-декодера.

![[Pasted image 20251226151930.png]]

Давайте немного формализуем и обобщим эту модель на рисунке ниже. (Для удобства будем использовать верхние индексы $e$ и $d$ там, где необходимо, для различения скрытых состояний кодировщика и декодера.) Элементы сети слева обрабатывают входную последовательность $x$ и составляют кодировщик. Хотя на урощенном рисунке выше показан только один слой сети для кодировщика, обычно используются многослойные архитектуры, где выходные состояния верхнего слоя стека принимаются за окончательное представление, а кодировщик состоит из многослойных biLSTM, где скрытые состояния верхних слоев из прямого и обратного проходов объединяются для обеспечения контекстуализированных представлений для каждого временного шага.

![[Pasted image 20251226152742.png]]

Основная задача кодировщика - создание контекстуализированного представления входных данных. Это представление воплощено в конечном скрытом состоянии кодировщика, $\mathrm{h}^{e}_{n}$. Это представление, также называемое $c$ (контекстом), затем передается декодеру. 

Простейшая версия сети декодера использует это состояние только для инициализации первого скрытого состояния декодера; первая ячейка RNN декодера использует $c$ в качестве своего предыдущего скрытого состояния $h^{d}_{0}$. Затем декодер авторегрессивно генерирует последовательность выходных данных, по одному элементу за раз, до тех пор, пока не будет сгенерирован маркер конца последовательности. Каждое скрытое состояние обусловлено предыдущим скрытым состоянием и выходными данными, сгенерированными в предыдущем состоянии. 

Как показано на рисунке выше, мы делаем нечто более сложное: мы делаем контекстный вектор с доступным не только для первого скрытого состояния декодера, чтобы гарантировать, что влияние контекстного вектора $c$ не ослабевает по мере генерации выходной последовательности.

Мы делаем это, добавляя $c$ в качестве параметра к вычислению текущего скрытого состояния. Используя следующее уравнение:
$$
\mathrm{h}_{t}^{d}=g(\hat{y}_{t-1}, \mathrm{h}_{t-1}^{d}, \mathrm{c})
$$

Теперь мы готовы увидеть полные уравнения для этой версии декодера в базовой модели кодировщик-декодер с контекстом, доступным на каждом шаге декодирования. Напомним, что $g$ - замена для RNN, а $\hat{y}_{t-1}$ - векторное представление выходных данных, полученных из функции softmax на предыдущем шаге:
$$
\begin{array} \\
\mathrm{c}&=&\mathrm{h}_{n}^{e} \\
\mathrm{h}^{d}_{0}&=&\mathrm{c} \\
\mathrm{h}^{d}_{t}&=&g(\hat{y}_{t-1},\mathrm{h}^{d}_{t-1},\mathrm{c}) \\
\mathrm{\hat{y}}_{t}&=&\mathrm{softmax}(\mathrm{h}_{t}^{d})
\end{array}
$$
Таким образом, $\hat{y}_{t}$ - вектор вероятности по словарю, представляющий вероятность появления каждого слова в момент времени $t$. Для генерации текста мы выбираем значения из этого распределения $\hat{y}_{t}$. 

### Обучение модели кодер-декодер модели

Архитектуры кодер-декодер обучаются сквозным(end-to-end) методом. Каждый обучающий пример представляет собой кортеж из пар строк, источника и цели. Объединенные разделительным токеном, эти пары "источник-цель" теперь могут служить обучающим примером.

Заметим разницу между обучением и инференсом в отношении выходных данных на каждом временном шаге. Декодер во время инференса использует свой собственный оценочный вывод $\hat{y}_{t}$ в качестве входных данных для следующего временного шага $x_{t+1}$. Таким образом, декодер будет все больше отклоняться от эталонного целевого предложения по мере генерации новых токенов. Поэтому при обучении чаще используется метод "принуждения учителя" в декодере.

![[Pasted image 20251226160231.png]]

