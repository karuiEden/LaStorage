---
title: Механизм внимания в RNN Encoder-Decoder
created: 2025-12-26
tags:
  - deep_learning
links:
  - "[[RNN]]"
  - "[[Кодер-декодер с RNN]]"
---
Простота модели кодер-декодер заключается в четком разделении кодировщика, который создает представление исходного текста, и декодера,  который использует этот контекст для генерации целевого текста. В описанной модели этот контекстный вектор - $h_{n}$, скрытое состояние последнего ($n$-ого) шага исходного текста. Это конечное скрытое состояние, таким образом, выступает в роли узкого места: оно должно представлять абсолютно всё о значении исходного текста, поскольку декодер знает об исходном тексте только то, что содержится в этом контекстном векторе.
Информация в начале представления, особенно для длинных предложений, может быть представлена в контекстном векторе не в равной степени.

![[Pasted image 20251226161453.png]]

*Механизм внимания* является решением проблемы узкого места, способ разрешить декодеру получать информацию из всех скрытых состояний кодировщика, не только последнего скрытого состояния.

В механизме внимания, как и в стандартной модели кодировщик-декодер, контекстный вектор $c$ представляет собой единый вектор, являющийся функцией скрытых состояний кодировщика. Но вместо того, чтобы брать его из последнего скрытого состояния, он представляет собой взвешенное среднее всех скрытых состояний кодировщика. И это взвешенное среднее также определяется частью состояния декодера, состоянием декодера непосредственно перед текущим токеном $i$. То есть, $\mathrm{c_{i}}=f(\mathrm{h}_{1}^{e}, \dots,\mathrm{h}^{e}_{n}, \mathrm{h}^{d}_{i-1})$. Веса формируются на определенной части текста, которая имеет отношение к токену $i$, который декодер в данный момент генерирует. Таким образом, механизм внимания заменяет статистический контекстный вектор вектором, динамически выведенным из скрытых состояний кодировщика, но также определяемым и, следовательно, различным для каждого токена при декодировании.

Этот контекстный вектор $c_{i}$ генерируется заново на каждом шаге декодирования $i$ и учитывает все скрытые состояния кодировщика при его вычислении. Затем мы делаем этот контекст доступным во время декодирования, обусловливая вычисление текущего скрытого состояния декодера им, как показано ниже:
$$
\mathrm{h}_{i}^{d}=g(\hat{y}_{i-1}, \mathrm{h}_{i-1}^{d}, \mathrm{c}_{i})
$$
![[Pasted image 20251226163540.png]]

Первый шаг в вычислении $c_{i}$ - это определение того, насколько следует сосредоточиться на каждом состоянии кодировщика релевантно состоянию декодера, зафиксированному в $h^{d}_{i-1}$. Мы оцениваем релевантность, вычисляя - для каждого состояния $i$ во время декодирования  - $\mathrm{score}(\mathrm{h}_{i-1}^{d},\mathrm{h}_{j}^{e})$ для каждого состояния кодировщика $j$. Простейшим способом вычислить такую оценку это посчитать скалярное произведение.

Чтобы использовать эти оценки, необходимо нормализовать их с помощью softmax, чтобы создать вектор весов, $\alpha_{ij}$, который говорит нам пропорциональную релевантность для каждого скрытого состояния кодировщика $j$ c предыдущим скрытым состоянием декодера, $h_{i-1}^{d}$.

Наконец, используя распределение $\alpha$, мы можем вычислить фиксированной длины контекстный вектор для текущего состояния, считая взвешенное среднее по всем скрытым состояниям кодировщика:
$$
\mathrm{c}_{i} = \sum_{j} \alpha_{ij}\mathrm{h}_{j}^{e}
$$
Таким образом, мы наконец получаем контекстный вектор фиксированной длины, который учитывает информацию из всего состояния кодировщика и динамически обновляется в соответствии с потребностями декодера на каждом этапе декодирования.

![[Pasted image 20251226164818.png]]

Также можно создавать более сложные функции оценки для моделей внимания. Вместо простого оценивания на основе скалярного произведения мы можем получить более мощную функцию, которая вычисляет релевантность каждого скрытого состояния кодировщика скрытому состоянию декодера, параметризуя оценку собственным набором весов, $\mathrm{W}_{s}$.
$$
\mathrm{score}(\mathrm{h}_{i-1}^{d}, \mathrm{h}_{j}^{e})= \mathrm{h}_{i-1}^{d}\mathrm{W}_{s}\mathrm{h}_{j}^{e}
$$
Веса $\mathrm{W}_{s}$, которые затем обучаются в ходе обычного сквозного обучения, позволяет сети определять, какие аспекты сходства между состоянием декодера и кодировщика важны для текущего приложения. Эта билинейная модель также позволяет кодировщику и декодеру использовать векторы разной размерности, тогда как простая функция внимания на основе скалярного произведения требует, чтобы скрытые состояния кодировщика и декодера имели одинаковую размерность
