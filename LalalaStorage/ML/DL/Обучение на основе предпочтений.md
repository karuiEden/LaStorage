---
title: Обучение на основе предпочтений
created: 2025-12-08
tags:
  - deep_learning
links:
---
После настройки по инструкциям производительность модели может быть улучшена ещё. Это особенно актуально в отношении аспектов поведения LLM, которые могут быть довольно проблематичными, таких как галлюцинации, небезопасность, болезненный, или токсичный вывод, и ответы даже технически верные, но не такие полезные, как могли быть. Цель *обучения на основе предпочтений* заключается в использовании суждений о предпочтениях для дальнейшего повышения производительности тонко настроенных моделей, как с точки зрения общей  производительности, так и в отношении таких качеств, как честность, готовность помочь и безвредность.
В отличие от инструкций, суждения о предпочтениях не требуют знаний о том, как делать что-то, нам просто нужно иметь мнение о конечном результате. Люди способны выражать предпочтения в отношении широкого круга вещей, имея мало или совсем не имея опыта о том, как были произведены рассматриваемые предметы. Суждения о предпочтениях возникают естественным образом: дается одна пара вариантов, мы выбираем тот, который нравится нам больше, или имея большой набор альтернатив, мы можем выбрать один, или мы можем ранжировать набор вариантов, и, наконец, мы можем просто принять или отклонить вариант в изоляции от прямых альтернатив.

### Данные предпочтений LLM

В контексте выравнивания на основе предпочтений обучающие данные обычно имеют форму промпта $x$ в паре с множеством альтернативных выводов $o$, которые были выбраны из LLM, используя $x$ как промпт. Когда данный вывод, $o_{i}$ предпочтительнее другого, $o_{j}$, мы записываем это как $(o_{i}\succ o_{j} \ | \ x)$. Рассмотрим следующие промпты и пары предпочтений из датасет HH-RLHF:

![[Pasted image 20251208125308.png]]


Подобные аннотируемые пары предпочтений могут быть сгенерированы несколькими способами:
- Прямое аннотирование пар выбранных выводов с помощью обученных аннотаторов;
- Аннотатор ранжирует $N$ выводов, преобразованных в $A_{n}^{2}$ пар предпочтений;
- Выбор аннотатора одного предпочтительного варианта из $N$ выборок, дающих $N-1$ пар.

Источник данных предпочтений для выравнивания LLM идет из 3 источников: суждения аннотаторов людей, неявные суждения предпочтений, полученные из онлайн источников, и полностью синтетические коллекции предпочтений, используя LLMs как аннотаторы.

### Моделирование предпочтений 

Наш первый шаг в эффективном использовании дискретных суждений о предпочтениях - в их вероятностном моделировании. То есть, мы хотим перейти от простого утверждения $(o_{i}\succ o_{j} \ | \ x)$ к знанию значения $P(o_{i}\succ o_{j} \ | \ x)$. Это позволит нам лучше анализировать мелкие различия в степени предпочтения и облегчит обучения моделей на основе данных о предпочтениях.

Начнем с предположения, что, выражая предпочтения между двумя элементами, мы неявно присваиваем оценку, или награду, каждому из них отдельно. Далее, предположим, что эти оценки являются скалярными значениями, $z\in \mathbb{R}$. Предпочтение между двумя вещами зависит от того, какая вещь имеет большую оценку.

Чтобы моделировать предпочтения, как вероятности, мы следуем тому же подходу, который мы использовали для бинарной логистической регрессии. Имея два выходных значения $o_{i}$ и $o_{j}$ с соответствующими оценками $z_{i}$ и $z_{j}$, $P(o_{i}\succ o_{j}\ | \ x)$ - логистическая сигмоида разности оценок.

$$
P(o_{i}\succ o_{j}\ | \  x) = \frac{1}{1+e^{-(z_{i}-z_{j})}}= \sigma(z_{i}-z_{j})
$$

Этот подход, известный как модель Bradley-Terry имеет ряд преимуществ: очень небольшие различия дают вероятность около $0.5$, отражая слабое или отсутствующее предпочтение между элементами, более крупные различия быстро приближаются к $1$, или к $0$, а производная логистической сигмоиды облегчает обучение с помощью потери бинарной кросс-энтропии.

Мотивация этой конкретной формулировки та же, что и при выводе логистической регрессии. Разность оценок $\delta=z_{i}-z_{j}$, принимается за логарифм вероятности возможных исходов(логит).
$$
\delta=\log \left(  \frac{P(o_{i}\succ o_{j}\ | \  x)}{P(o_{j}\succ o_{i}\ | \  x)} \right)  =\log \left( \frac{P(o_{i}\succ o_{j}\ | \ x)}{1- P(o_{i}\succ o_{j}\ | \ x)} \right) 
$$

Если проэкспонировать и сделать некоторые преобразования, то получим знакомую логистическую сигмоиду.
$$
P(o_{i}\succ o_{j} \ | \  x)= \sigma(z_{i}-z_{j})
$$
### Обучение оценивания предпочтения

Этот подход требует доступа к оценкам $z_{i}$, лежащим в основе данных предпочтений, которых у нас нет. У нас есть лишь наборы оценок предпочтений по парам промптов/образцов. Мы будем использовать данные предпочтения и формулировка Бредли-Терри для обучения функции $r(x,o)$, которая присваивает скалярную награду для пар промптов/выводов. То есть, $r(x, o)$ вычисляет оценку $z$ выше:
$$
P(o_{i}\succ o_{j}\ | \  x)=\sigma(z_{i}-z_{j})=\sigma(r(o_{i},x)-r(o_{j},x))
$$

Чтобы обучить $r(x,o)$ по данным предпочтений, мы будем использовать градиентный спуск, чтобы минимизировать потерю бинарной кросс-энтропии, чтобы тренировать модель. Предположим, что если наши данные предпочтения говорят нам, что $(o_{i}\succ o_{j})$ и затем, то $P(o_{i}\succ o_{j}\ | \ x)=1$, и соответственно, $P(o_{j}\succ o_{i}\ | \ x)=0$. Обозначим предпочтительный выход паре(победитель) как $o_{w}$ и проигравшую как $o_{l}$. При этом потеря кросс-энтропии для одной пары выбранных выходов для промпта $x$, используя модель Брэдли-Терри:
$$
L_{CE}(x,o_{w},o_{l})=-\log P(o_{w}\succ o_{l}\ | \ x)=-\log\sigma(r(x,o_{w})-r(x,o_{l}))
$$
То есть, потеря - это отрицательный логарифм правдоподобия оценки модели $P(o_{w}\succ o_{l}\ | \ x)$. И потеря на всем наборе обучающих предпочтений $\mathcal{D}$, определяется следующим ожиданием:

$$
L_{CE}=-\mathbb{E}_{x,o_{w},o_{l} \thicksim \mathcal{D}}[\log\sigma(r(x,o_{w})-r(x,o_{l}))]
$$

^07567f

Чтобы обучить награждающую модель, используя потери, мы можем использовать любую регрессионную модель, способную принимать текст в качестве входных данных и генерировать взамен скалярный выход.

![[Pasted image 20251209121452.png]]

Как показано выше, текущий предпочтительный подход заключается в инициализации модели из существующей предварительно обученной LLM. Для генерации скалярного вывода, нужно удалить language modeling head из последнего слоя и заменить на один плотный линейный слой. Затем используем градиентный спуск с [[#^07567f|потерей]] для обучения оценки выходов модели, используя обучающие данные предпочтений.

Награждающие модели, обученные на данных предпочтений, непосредственно полезны для ряда задач, не требующих выравнивания модели. Например, награждающие модели используются для выбора одного предпочтительного выхода из набора выбранных ответов LLM. Они также используются для выбора данных, используемых во время [[Донастройка по инструкциям|настройки по инструкциям]].