---
title: Трансформеры
created: 2025-11-17
tags:
  - deep_learning
links:
---
**Определение 1:** *Трансформер* - нейронная сеть со специфической структурой, включающие механизмы self-attension и multi-head attension.

Attension(Внимание) можно рассматривать как способ создания контекстных представлений значения токена путем отслеживания и интеграции информации из окружающих токенов, помогая модели изучать, как токены соотносятся друг с другом на больших промежутках времени.

Трансформеры состоят из 3 компонентов:
1. [[#Блоки трансформера|Блок трансформера]]. Включает в себя [[#Multi-head attension|multi-head attention]], [[Feedforward Neural Network|feedforward network]] и шагов нормализации слоя. Сопоставляет входной вектор $x_{i}$ столбца $i$ в выходной вектор $h_{i}$. Набор $n$ блоков отображает целое контекстное окно входных векторов $(x_{1},\dots,x_{n})$ в окно выходных векторов $(h_{1},\dots,h_{n})$ той же длины. Столбец может содержать от 12 до 96 и более блоков, сложенных друг на друга. 
2. Входной кодер. Он преобразует входной токен в контекстуальное векторное представление, используя матрицу эмбеддингов $E$ и механизм кодирования позиции токена.
3. Language modeling head, которая принимает выходные эмбеддинги последнего блока трансформера, пропускает их через матрицу unembedding $U$ и $\text{softmax}$ по словарю, чтобы сгенерировать один токен для этого столбца.

![[Pasted image 20251117221357.png]]

### Механизм внимания

Трансформеры могут строить контекстуальные представления значений слов, *контекстуальные эмбеддинги*, интегрируя значения помогающих контекстных слов. В трансформере, слой за слоем мы строим более богатые представления значений входных токенов. На каждом слое мы вычисляем представление токена $i$, комбинируя информацию о $i$ из предыдущего слоя с информацией о соседних токенах, чтобы создать контекстуализированное представление для каждого слова в каждой позиции.

**Определение 2:** *Внимание* - механизм в трансформере, который взвешивает и объединяет представления из соответствующих других токенов в контексте из слоя $k$ для построения представления для токенов в слое $k+1$.

![[Pasted image 20251117224110.png]]

Внимание берет представление ввода $x_{i}$, соответствующее входному токену позиции $i$, и контекстное окно предыдущих вводов $x_{1},\dots,x_{i-1}$, и вычисляет вывод $a_{i}$.
В ауторегрессивных моделях, контекст - это любое из предыдущих слов. То есть, когда обрабатывается $x_{i}$, модель имеет доступ к $x_{i}$, а также к представлениям всех предыдущих токенов в контекстном окне, но токенов не после $i$.

![[Pasted image 20251117225713.png]]

Рисунок выше показывает поток информации в целостном казуальном слое внутреннего внимания(self-attension), в котором вычисление каждого внимания происходит параллельно в каждой позиции токена $i$. Таким образом, слой внутреннего внимания отображает входные последовательности $x_{1},\dots x_{n}$ в выходные последовательности той же длины $a_{1},\dots a_{n}$.
#### Упрощенная версия внимания

По сути, внимание - это просто взвешенная сумма векторов контекста, со множеством сложностей с тем, как вычисляются веса и что суммируется.
Вывод внимания $a_{i}$ токена позиции $i$ это просто взвешенная сумма всех представлений $x_{j};\forall j\leq i$, $\alpha_{ij}$ означает как много $x_{j}$ вносит в $a_{i}$:
$$
a_{i}=\sum_{j\leq i}\alpha_{ij}x_{j}
$$
Каждый $\alpha_{ij}$ - это скаляр, используемый для взвешивания значения ввода $x_{j}$, когда суммируется ввод для вычисления $a_{i}$. Во внимании мы взвешиваем каждый предыдущий эмбеддинг пропорционально тому, насколько они похожи с текущим токеном $i$. Итак, выход внимания - это сумма эмбеддингов предыдущих токенов, взвешенных по их сходству с текущим эмбеддингом токена. Мы считаем очки схожести с помощью скалярного произведения, тем больше оно, чем больше схожи 2 токена. Затем мы нормализируем очки при помощи softmax, чтобы создать вектор весов $\alpha_{ij},j\leq i$.
$$
\begin{array} \\
\text{score}(x_{i},x_{j})=x_{i}\cdot x_{j} \\
\alpha_{ij} = \text{softmax}(\text{score}(x_{i},x_{j}))\,\forall j\leq i
\end{array}
$$
#### A single attention head, используя матрицы запросов, ключей и значений

Теперь откажемся от упрощений и введем *механизм внимания с одной головой*(single attention head) - механизм внимания, который используется в трансформерах. Этот механизм позволяет нам точно представить три различные роли, которые каждый входной эмбеддинг участвует в ходе процесса внимания:
- Текущий элемент сравнивается с предыдущими входными данными. Мы будем называть эту роль *запросом*;
- В роли предыдущего ввода, который сравнивается с текущим элементом для определения веса сходства. Будем называть эту роль *ключом*.
- И наконец, *значение* предыдущего элемента, которое будет взвешенно и суммировано для вычисления вывода текущего элемента.

Чтобы охватить эти 3 различные роли, трансформеры вводят матрицы весов $W^{Q},W^{K}$  и $W^{V}$. Эти веса будут проецировать каждый вектор ввода $x_{i}$ в представления этих ролей как запрос, ключ и значение:
$$
q_{i}=x_{i}W^{Q};\quad k_{i}=x_{i}W^{K};\quad v_{i}=x_{i}W^{V}
$$
Имея эти проекции, когда мы вычисляем схожесть текущего элемента $x_{i}$ с предыдущим $x_{j}$, мы будем использовать скалярное произведение с вектором запроса $q_{i}$ и предшествующим ключевым вектором $k_{j}$. При этом результат скалярного произведения может получится слишком большим, и экспоненциально большие числа могут привести к числовым ошибкам и потерям градиента во время обучения. Чтобы избежать этого мы масштабируем его на степень размерности эмбеддингов, то есть делим произведение на квадратный корень размерности вектора запроса и ключевого вектора $d_{k}$. Вычисление веса останется как в упрощенной версии, а вот вычисление $\text{head}_{i}$ теперь базируется на весах $\alpha_{ij}$ и векторах $v$.
Сам же вектор вывода механизма внутреннего внимания теперь считается как произведение вектора $\text{head}$ и специальной матрицы $W^{O}$. Которая умножается справа, это делается, чтобы реформировать вектор $\text{head}_{i}$. Вектор ввода $x_{i}$ и вектор вывода $a_{i}$ имеют одинаковую размерность $[1\times d]$. Будем называть $d$ *размерностью модели*.
$$
q_{i}=x_{i}W^{Q};\quad k_{j}=x_{j}W^{K};\quad v_{j}=x_{j}W^{V}
$$
$$
\text{score}(x_{i},x_{j}) = \frac{q_{i}\cdot k_{j}}{\sqrt{ d_{k} }}
$$
$$
\alpha_{ij}= \text{softmax}(\text{score}(x_{i},x_{j}))\quad\forall j\leq i
$$
$$
\text{head}_{i}=\sum_{j\leq i}\alpha_{ij}v_{j}
$$
$$
a_{i}=\text{head}_{i}W^{O}
$$

![[Pasted image 20251118115826.png]]

#### Multi-head attension

Итак, *multi-head attension* состоит из $A$ отдельных механизмов внимания с одной головой, которые находятся в параллельных слоях на одинаковой глубине в модели, каждая голова имеет свои параметры, так как они могут специализироваться на различных задачах, например: представлять различные лингвистические отношения между контекстными словами и текущим словом, или рассматривать частные типы паттернов в контексте.
Таким образом, каждый $\text{head}_{i}$ имеет собственный набор матриц запросов, ключей и значений: $W^{Qi},W^{Ki},W^{Vi}$. Они используются для проецирования в отдельными запросы, ключи и значения для каждой "головы".
Когда мы используем несколько голов размерность модели $d$ все ещё используется для вводного вектора и вектора вывода, а вектора запросов и ключей - размерность $d_{k}$, вектора значений - $d_{v}$. Таким образом, для каждой головы, мы имеем слои веса $W^{Qi}\in \mathbb{R}^{d\times d_{k}},W^{Ki}\in \mathbb{R}^{d\times d_{k}}$ и $W^{Vi}\in \mathbb{R}^{d\times d_{v}}$:
$$
q_{i}^{c}=x_{i}W^{Qc};\quad k_{j}^{c}=x_{j}W^{Kc};\quad v_{j}^{c}=x_{j}W^{Vc};\quad\forall c\in[1,A]
$$
$$
\text{score}^{c}(x_{i},x_{j})= \frac{q_{i}^{c}\cdot k_{j}^{c}}{\sqrt{ d_{k} }}
$$
$$
\alpha_{ij}^{c}=\text{softmax}(\text{score}^{c}(x_{i},x_{j}))\quad\forall j\leq i
$$
$$
\text{head}_{i}^{c}=\sum_{j\leq i}\alpha_{ij}^{c}v_{j}^{c}
$$
$$
a_{i}^{c}=(\text{head}^{1}\oplus \text{head}^{2}\oplus \dots \oplus \text{head}^{A})W^{O}
$$
Следовательно
$$
\text{MultiHeadAttention}(x_{i},[x_{1},\dots,x_{i-1}])=a_{i}
$$
Формула выше корректна только для казуальных моделей.

![[Pasted image 20251118122537.png]]

### Блоки трансформера

Вычисление внутреннего внимания лежит в основе того, что мы называем *блоком трансформера*, который помимо механизма внутреннего внимания содержит слой прямой связи, остаточные связи и нормализирующие слои. С точки зрения остаточного потока мы рассматриваем обработку отдельного токена $i$ блоком-трансформером как единый поток $d$-мерных представлений для позиции токена $i$. Этот остаточный поток начинается с исходного вводного вектора, и различные компоненты считывают свои входные данные из остаточного потока и добавляют выходные данные обратно в поток.

![[Pasted image 20251119125809.png]]

Ввод внизу потока - это эмбеддинг для токена размерностью $d$. Изначальный эмбеддинг поднимается вверх и постепенно к нему добавляются другие компоненты: слой механизма внимания, и слоя прямой связи. Но перед этими слоями находятся слои нормализации 

Таким образом, исходный вектор нормализуется и попадает в слой внимания, затем результат складывается с изначальным вектором и затем вновь нормализируется и попадает в слой прямой связи, и затем вновь складывается с исходным вектором. Результатом блока трансформера будем обозначать $h_{i}$. 

#### Feedforward layer

*Feedforward layer* - это полностью связная 2-слойная сеть. Веса одинаковые для каждого $i$-ого токена, но разные от слоя к слою. Обычно делают размерность скрытых слоев $d_{ff}$ сети прямой связи бывают больше, чем размерность модели $d$.
$$
\text{FFN}(x_{i})=\text{ReLU}(x_{i}W_{1}+b_{1})W_{2}+b_{2}
$$
#### Layer Norm

В двух этапах блоке трансформера мы нормализуем вектор. Этот процесс, называемый *слой нормализации*(layer norm), одна из многих форм нормализации, которые используют, чтобы улучшить эффективность обучения в глубоких нейронных сетях, сохраняя значения в скрытом слое в диапазоне, который облегчает обучение на основе градиента.

Слой нормализации - это вариация *z-score* из статистики, применяемая к одному вектору в скрытом слое. Слой нормализации применяется не ко всему слою трансформеру, а только к вектору эмбеддингу одного токена. Таким образом, входными данными для слоя нормализации является один вектор размерности $d$, а выходными данными - этот вектор, нормализованный, также размерности $d$. Первым шагом нормализации является вычисление среднего значения $\mu$ и стандартного отклонения $\sigma$ для элементов нормируемого вектора. Для заданного вектора эмбеддинга $x$ размерности $d$ эти значения вычиляются следующим образом.
$$
\mu= \frac{1}{d}\sum_{i=1}^{d}x_{i}
$$
$$
\sigma=\sqrt{ \frac{1}{d}\sum_{i=1}^{d}(x_{i}-\mu)^{2} }
$$
Имея эти значения, компоненты вектора нормализуется вычитанием среднего и делением на стандартное отклонение. Результат вычислений - новый вектор с нулевым средним и стандартным отклонением, равным 1.
$$
\hat{x}= \frac{(x-\mu)}{\sigma}
$$
В конечном итоге, в стандартной реализации слоя нормализации есть 2 обучающих параметра $\gamma$ и $\beta$, представляющие прирост и сдвиг.
$$
\text{LayerNorm(x)}= \gamma  \frac{(x-\mu)}{\sigma}+\beta
$$


Таким образом, в каждом блоке вычисления идут так:
$$
\begin{array} \\
t^{1}_{i}=\text{LayerNorm}(x_{i}) \\
t_{i}^{2}=\text{MultiHeadAttention}(t_{i}^{1},[t_{1}^{1},\dots,t^{1}_{N}])  \\
t_{i}^{3}=t^{2}_{i}+x_{i} \\
t^{4}_{i}=\text{LayerNorm}(t^{3}_{i}) \\
t_{i}^{5}=\text{FFN}(t^{4}_{i}) \\
h_{i}=t^{5}_{i}+t^{3}_{i}
\end{array}
$$
Заметьте, что только слой механизма внимания принимает токены из других потоков. Блоки можно объединять в стэки, если стэк содержит много блоков требует дополнительны слой нормализации в конце последнего блока трансформера.

### Распараллеливание вычислений, используя одну матрицу X

Входные данные можно представить в виде матрицы $X$, каждая строка которой является вводным эмбеддингом, следовательно, $X\in \mathbb{R}^{N\times d}$.

#### Распараллеливание механизма внимания

Для одной головы мы умножаем матрицу $X$ на матрицы запросов, ключей и значений $W^{Q}\in \mathbb{R}^{d\times d_{k}}$, $W^{K}\in \mathbb{R}^{d\times d_{k}}$ и $W^{V}\in \mathbb{R}^{d\times d_{v}}$. Тогда мы получаем матрицы $Q\in \mathbb{R}^{N\times d_{k}},K\in \mathbb{R}^{N\times d_{k}}$ и $V\in \mathbb{R}^{N\times d_{v}}$, содержащие все запросы, ключи и значения:
$$
Q=XW^{Q}\quad K=XW^{K}\quad V=XW^{V}
$$
Имея эти матрицы, мы можем вычислить все необходимые запрос-ключ сравнения, вычисляемые умножением $Q$ и $K^{T}$ в одну матрицу. Произведение формы $N\times N$

![[Pasted image 20251119143041.png]]

Раз мы имеем эту $QK^{T}$ матрицу, мы можем очень эффективно масштабировать эти очки, взяв softmax, и затем умножить его на матрицу $V$ в результирующую матрицу $N\times d$: вектор эмбеддинга, представляющего каждый токен ввода. В итоге, сокращено вычисление весь шаг внутреннего внимания следующими вычислениями:
$$
\begin{array} \\
\text{head}= \text{softmax}\left( \text{mask}\left(  \frac{QK^{T}}{\sqrt{ d_{k} }}  \right) \right)V \\
A=\text{head}\,W^{O}
\end{array}
$$
Как можно заметить выше, была представлена функция маскировки. Потому что матрица $QK^{T}$ имеет проблему, вычисляются те пары запрос-ключ, в которых ключ следует после запроса. Это неуместно в условиях моделирования языка, так как угадать следующее слово просто, если его знать. Чтобы исправить это, элементы в верхнем треугольнике заменяются на $-\infty$, которая даст $0$ на softmax. Это нормальная практика прибавлять матрицу маскировки, в которой в верхнем треугольники элементы равны $-\infty$, в другом случае $0$. 

![[Pasted image 20251119144600.png]]

Можно заметить, что вычисление механизма внимания квадратично зависит от входных данных

#### Распараллеливание механизма внимания с несколькими головами

![[Pasted image 20251120100314.png]]

$$
\begin{array} \\
\mathbf{Q^{i}}=\mathbf{XW^{Qi}};\quad \mathbf{K^{i}}=\mathbf{XW^{Ki}};\quad \mathbf{V^{i}}=\mathbf{XW^{Vi}} \\
\text{head}_{i} = \text{SelfAttention}(\mathbf{Q}^{i},\mathbf{K}^{i},\mathbf{V}^{i})= \text{softmax}\left( \text{mask}\left(  \frac{\mathbf{Q}^{i}\mathbf{K}^{iT}}{\sqrt{ d_{k} }}  \right) \right)\mathbf{V}^{i } \\
\text{MultiHeadAttention}(\mathbf{X})= (\text{head}_{1}\oplus \text{head}_{2}\oplus \dots \oplus \text{head}_{A})\mathbf{W}^{\mathbf{O}}

\end{array}
$$

#### Распараллеливание вычисления блока трансформера

$$
\begin{array} \\
\mathbf{O}= \mathbf{X}+ \text{MultiHeadAttention}(\text{LayerNorm}(\mathbf{X})) \\
\mathbf{H}= \mathbf{O}+ \text{FFN}(\text{LayerNorm}(\mathbf{O}))

\end{array}
$$

Таким образом, весь блок будет вычисляться так:
$$
\begin{array} \\
\mathbf{T^{1}}= \text{LayerNorm}(\mathbf{X}) \\
\mathbf{T^{2}}= \text{MultiHeadAttention}(\mathbf{T^{1}}) \\
\mathbf{T^{3}}= \mathbf{T^{2}}+ \mathbf{X} \\
\mathbf{T^{4}}= \text{LayerNorm}(\mathbf{T^{3}}) \\
\mathbf{T^{5}}=\text{FFN}(\mathbf{T^{4}}) \\
\mathbf{H} = \mathbf{T^{5}}+\mathbf{T^{3}}
\end{array}
$$
### Входной кодер

Теперь выясним, откуда берется вход $\mathbf{X}$. Если нам дана последовательность из $N$ токенов, матрица $\mathbf{X}\in \mathbb{R}^{N\times d}$ имеет эмбеддинги для каждого слова в контексте. Трансформер делает это отдельными вычислениями 2 эмбеддингов: входной эмбеддинг токена и входной эмбеддинг позиции.

Эмбеддинг токена - вектор размерности $d$, который будет изначальным представлением входного токена. Набор изначальных эмбеддингов хранятся в матрице эмбеддингов $\mathbf{E}$, в котором имеется строка для каждого токена в словаре размером $|V|$. Таким образом, каждое слово представляет собой вектор-строку размерности $d$, а $\mathbf{E}$ имеет размерность $[|V|\times d]$.

Имея входную строку-токен, например, "Thanks for all the we", мы сначала конвертируем токены в индексы словаря. Тогда представление "Thanks for all the we" может быть $\mathbf{w}=[5,4000,10532,2224]$. Затем мы используем индексирование, чтобы выбрать подходящие строки из $\mathbf{E}$.

Есть другой способ выбор эмбеддинга у токена из матрицы $\mathbf{E}$, это представлять токены в виде one-hot векторов размерности $[1\times |V|]$ 

![[Pasted image 20251120110100.png]]

Мы можем расширить идею и представлять целую последовательность в виде матрицы one-hot векторов, по одному для каждой из $N$ позиций в контекстном окне трансформера.

Эти токены позиционно-независимы. Чтобы представить позицию каждого токена в последовательности, мы объединяем эти эмбеддинги токенов с позиционными эмбеддингами, специальными для каждой позиции.

Как найти позиционный эмбеддинг? Простейший метод, называемый *абсолютной позицией*, заключается в том, чтобы начать со случайно инициализированных эмбеддингов, соответствующих абсолютной позиции каждой возможной входной позиции до некоторой максимальной длины. Как и в случае с эмбеддингами токенов, позиционные эмбеддинги также считаются в процессе обучения модели. Мы можем хранить их в матрице $E_{pos}\in \mathbb{R}^{N\times d}$.

Чтобы получить входной эмбеддинг, который захватывает информацию о позиции, мы просто складываем эмбеддинг слова с соответствующим ему позиционным эмбеддингом, так как они оба имеют размерность $[1\times d]$.

![[Pasted image 20251120111227.png]]

Финальное представление ввода, матрица $\mathbf{X}\in \mathbb{R}^{N\times d}$, в которой каждая строка $i$ является представлением $i$-ого токена ввода, вычисленный сложением $\mathbf{E}[id(i)]$ с $\mathbf{P}[i]$ - позиционным эмбеддингом позиции $i$.

Возможная проблема с простым подходом к вычислению позиционных эмбеддингов заключается в том, что для начальных позиций во входных данных будет слишком много обучающих примеров, а для вне внешней длины меньше. Поэтому последние эмбеддинги могут быть плохо обучены и плохо обобщаться во время тестирования. Одним из подходом, решающим эту проблему, заключается в выборе статической функции, которая отражает целочисленный ввод в действительные векторы, чтобы лучше обрабатывать последовательности произвольной длины. Комбинация косинуса и синуса использовались в изначальном трансформере. Синусоидальные позиционные эмбеддинги могут также помочь в выявлении внутренних взаимосвязей между позициями.
Более сложные методы позиционных эмбеддингов расширяют идею выявления взаимосвязей, позволяя напрямую предоставить относительную позицию вместо относительно абсолютной, что часто реализуется в механизме внимания на каждом слое, а не добавляется один раз на начальном слое.

### Голова моделирования языка

Последний компонент трансформера это *голова моделирования языка(language modeling head)*. Мы называем этот компонент головой, чтобы обозначить, что оно находится на вершине архитектуры трансформера, когда мы применяем претренированную модель-трансформер для различных задач. Голова моделирования языка - необходимая схема для моделирования языка.
Как и все остальные языковые модели - трансформеры предсказывают следующие слова.
Работа головы моделирования языка состоит в том, чтобы принимать результат вычисления последнего блока трансформера последнего токена $N$ и использует его для предсказания следующего слова позиции $N+1$.

![[Pasted image 20251120132051.png]]

Сначала идёт линейный слой, который проецирует вывод $h^{L}_{N}$, который представляет эмбеддинг выводного токена из финального блока трансформера $L$ в *logit* вектор, который содержит "очки" для каждого из $|V|$ возможного слова в словаре $V$. Таким образом, логит-вектор имеет размерность $[1\times |V|]$. 
Этот линейный слой может быть обучен, но чаще связывают эту матрицу с матрицей эмбеддингов $\mathbf{E}$. Это называется *связкой весов*, когда используем одинаковые веса в разных матрицах. Таким образом, на стадии ввода трансформера матрица эмбеддингов $\mathbf{E}\in \mathbb{R}^{|V|\times d}$ используется для отображения one-hot векторов в вводные эмбеддинги размерностью $[1\times d]$ и после преобразований мы применяем транспонированную матрицу $\mathbf{E^{T}}$, чтобы отобразить обратно в вектор размерностью $[1\times |V|]$. Обычно, матрицу $\mathbf{E}$  оптимизируют, чтобы эффективно проводить вычисления с ней. Иногда мы называем матрицу $\mathbf{E^{T}}$ *unembeddings matrix*.
Затем с помощью softmax превращаем логиты в вероятностное распределение словаря.
$$
\begin{array} \\
\mathbf{u}= \mathbf{h_{N}^{L}\,E^{T}} \\
\mathbf{y} = \text{softmax}(\mathbf{u})
\end{array}
$$
Мы можем использовать эти вероятности для различных задач, но самая важная это генерация текста, которую мы делаем с помощью [[Сэмплирование|сэмплирования]] или других способов выбора для генерации следующего токена.

![[Pasted image 20251120134834.png]]

Сверху можно увидеть декодер трансформер, существуют также и другие виды трансформеров, такие как кодер-трансформеры и кодер-декодер трансформер.

## Обучение

На каждом шаге, учитывая все предыдущие слова, последний слой трансформера формирует выходное распределение по всему словарю. В процессе обучения вероятность, присвоенная моделью правильному слову, используется для расчета потери кросс-энтропии для каждого элемента последовательности. Потеря для обучающей последовательности - это средняя кросс-энтропия по всей последовательности. Веса в сети корректируются так, чтобы минимизировать среднюю потерю кросс-энтропии в обучающей последовательности с помощью градиентного спуска.

![[Pasted image 20251120225032.png]]

При использовании трансформеров каждый обучающий элемент может обрабатываться параллельно, поскольку выходные данные для каждого элемента в последовательности вычисляются отдельно.

## Масштабирование

### Законы масштабирования

Эффективность LLM главным образом определена 3 факторами: размер модели(количество параметров, не считая эмбеддинги), размер обучающей выборки и количество вычислений, используемых во время обучения. То есть мы можем улучшить модель, добавляя параметры, обучая на новых данных, обучая на больших итерациях.
Отношения между этими факторами и эффективностью известны как *законы масштабирования*.

Например, найдены следующие три соотношения для потерь $L$ как функции количества параметров неэмбеддингов $N$, размера обучающей выборки $D$ и бюджета вычислений $C$ для обучения моделей с ограниченными параметрами, набором данных или бюджетом вычислений, если в каждом случае два других свойства остаются постоянными:
$$
L(N)= ( \frac{N_{c}}{N} )^{\alpha_{N}}
$$
$$
L(D)= \left( \frac{D_{c}}{D} \right)^{\alpha_{D}}
$$
$$
L(C)= \left( \frac{C_{N}}{C} \right)^{\alpha_{C}}
$$
Число параметров $N$ можно приблизительно посчитать следующим образом (игнорируя смещения и используя $d$ как размерность входных и выходных данных модели, $d_{attn}$ как размер слоя внутреннего внимания, $d_{ff}$ как размер слоя прямой связи):
$$
N\approx 2\ d\ n_{\text{layer}}(2d_{attn}+d_{ff})\approx 12n_{\text{layer}}\ d^{2}
$$
Таким образом, GPT-3 с 96 слоями и размерностью $d=12228$ имеет по формуле примерно 175 миллиардов параметров.
Значения $N_{c},D_{c},C_{c},\alpha_{N},\alpha_{D}$ и $\alpha_{C}$ зависят от архитектуры трансформера, токенизации и размера словаря, поэтому законы масштабирования фокусируются не на всех точных значениях, а на соотношении с потерями. Законы масштабирования могут быть полезны при выборе способа обучения модели до определенной производительности, например, анализируя начальные этапы обучения или производительность на меньших объемах данных, чтобы предсказать, какими будут потери при добавлении большого количества данных или увеличении размера модели. Другие аспекты законов масштабирования также могут подсказать, сколько данных необходимо добавить при масштабировании модели.

### KV Кэш

Ранее мы видели как эффективно вычислить внимание, умножая 2 матрицы:
$$
\mathbf{A}=\text{softmax}\left(  \frac{\mathbf{QK^{T}}}{\sqrt{ d_{k} }}  \right)\mathbf{V}
$$
К сожалению, мы не можем выполнять вычисления во время инференса так же эффективно, как во время обучения. Так как во время инференса мы итеративно генерируем токены по одному за раз. Для только что сгенерированного токена $x_{i}$, нам нужно вычислить его запрос, ключ и значение, умножив их на $\mathbf{W^{Q},W^{K},W^{V}}$ соответственно. Но перевычисление векторов ключей и значений для всех предыдущих токенов $x_{<i}$ было бы пустой тратой времени, так как до этого мы уже их вычисляли. Поэтому вместо повторного вычисления, мы вычисляем векторы ключей и значений, сохраняем их в *KV кэше*, а затем просто извлекаем их из кэша, когда они нам понадобятся.

![[Pasted image 20251120233138.png]]

Черным вычислены элементы, которые хранятся в кэше.

### Параметр эффективности fine tuning

Довольно часто берут языковую модель и дают больше данных о новой сфере при помощи *finetuning* на дополнительных данных.

Fine-tuning может быть очень сложным с очень большими языковыми моделями, потому что огромное количество параметров обучаются; каждый проход батча через градиентный спуск имеет обратное распространение через очень много слоев. Это делает finetuning огромных языковых моделей слишком дорогой в вычислении, памяти и времени. Поэтому существуют методы, которые позволяют модели делать finetuning, не обновляя все параметры. Такие методы называются *параметр эффективности finetuuning*(*parameter-efficient finetuning*) или иногда *PEFT*, потому что мы эффективно выбираем подмножество параметров для обновления во время дообучения. Для примера мы фиксируем остальные параметры, и только обновляем некоторое подмножество параметров.

## Интерпретация трансформеров

Подраздел интерпретируемости, иногда называемый механистической интерпретируемостью, фокусируется на способах механистического понимания, что происходит внутри трансформера.
### Контекстное обучение и голова индукции

Чтобы заставить модель выполнять нужные нам действия, можно рассматривать промпты как принципиальное отличие от предобучения. Обучение с помощью предобучения означает обновление параметров модели, используя градиентный спуск в соответствии с некоторой функцией. Обучение с помощью промптов и подсказок могу научить модель выполнять новые задачи. Модель узнает что-то о задаче из этих демонстраций, обрабатывая подсказку. Даже без демонстраций процесс подсказок можно рассматривать как своего рода обучение. Например, чем глубже модель проходит промпт, тем лучше она, как правило, предсказывает последующие токены. Информация в контексте повышает предсказательную силу модели.
Термин *контекстное обучение* был представлен во время введения к GPT-3, посвященной контекстному обучению. Контекстное обучение означает, что языковые модели обучаются выполнять новые задачи, лучше предсказывать токены или в целом сокращать свои потери во время прямого процесса в инференсе без обновлений параметров модели через градиенты.
Одной из гипотезы того, как работает контекстное обучение является *головная индукция(induction head)*. Головная индукция - это имя схемы, которая является одним из типов абстрактных компонентов в индукции. Схема головной индукции является частью вычисления внимания в трансформерах, обнаруженной при рассмотрении мини-языковых моделей с всего 1-2 головками внимания. Функция индукционной головки заключается в предсказании повторяющихся последовательностей. Это достигается за счёт наличия компонента сопоставления префиксов в вычислении внимания, который при поиске текущего токена $A$ ищет по контексту предыдущее вхождение $A$. Если он находится, индукционная головка имеет механизм копирования, который "копирует" токен $B$, следующий за токеном $A$, увеличивая вероятность того, что $B$ станет следующим токеном.

![[Pasted image 20251121085823.png]]

Было предложено, что нечеткое обобщенное правило правила совпадающих паттернов, имплементируя на семантически эквивалентные слова, может быть ответственно за контекстное обучение. Предполагаемое доказательство этой гипотезы, которое показывает, что абляция индукционной головки приводит к ухудшению эффективности индукционной головки. Абляция - это медицинский термин, означающий удаление чего-либо. 

#### Линза логита

Другой инструмент интерпретации, линзы логита, предлагает визуализацию, что внутренние слои трансформера могут быть представлены.
Идея заключается в том, что мы берем любой вектор с любого слоя трансформера и предполагая, что это префинальный эмбеддинг, просто умножаем на unembedding слой, чтобы получить логиты и посчитать softmax, чтобы увидеть распределение слов, которые вектор может представить. Это может быть полезным окном во внутренние представления модели. 
Поскольку сеть не была обучена работать со внутренними представлениями таким образом, то логит-линза может работать неидеально, но все еще может быть полезным приемом, помогающим нам визуализировать внутренние слои трансформера.