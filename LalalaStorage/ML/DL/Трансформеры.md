---
title: Трансформеры
created: 2025-11-17
tags:
  - deep_learning
links:
---
**Определение 1:** *Трансформер* - нейронная сеть со специфической структурой, включающие механизмы self-attension и multi-head attension.

Attension(Внимание) можно рассматривать как способ создания контекстных представлений значения токена путем отслеживания и интеграции информации из окружающих токенов, помогая модели изучать, как токены соотносятся друг с другом на больших промежутках времени.

Трансформеры состоят из 3 компонентов:
1. Блок трансформера. Включает в себя multi-head attension, [[Feedforward Neural Network|feedforward network]] и шагов нормализации слоя. Сопоставляет входной вектор $x_{i}$ столбца $i$ в выходной вектор $h_{i}$. Набор $n$ блоков отображает целое контекстное окно входных векторов $(x_{1},\dots,x_{n})$ в окно выходных векторов $(h_{1},\dots,h_{n})$ той же длины. Столбец может содержать от 12 до 96 и более блоков, сложенных друг на друга. 
2. Входной кодер. Он преобразует входной токен в контекстуальное векторное представление, используя матрицу эмбеддингов $E$ и механизм кодирования позиции токена.
3. Language modeling head, которая принимает выходные эмбеддинги последнего блока трансформера, пропускает их через матрицу unembedding $U$ и $\text{softmax}$ по словарю, чтобы сгенерировать один токен для этого столбца.

![[Pasted image 20251117221357.png]]

### Внимание

Трансформеры могут строить контекстуальные представления значений слов, *контекстуальные эмбеддинги*, интегрируя значения помогающих контекстных слов. В трансформере, слой за слоем мы строим более богатые представления значений входных токенов. На каждом слое мы вычисляем представление токена $i$, комбинируя информацию о $i$ из предыдущего слоя с информацией о соседних токенах, чтобы создать контекстуализированное представление для каждого слова в каждой позиции.

**Определение 2:** *Внимание* - механизм в трансформере, который взвешивает и объединяет представления из соответствующих других токенов в контексте из слоя $k$ для построения представления для токенов в слое $k+1$.

![[Pasted image 20251117224110.png]]

Внимание берет представление ввода $x_{i}$, соответствующее входному токену позиции $i$, и контекстное окно предыдущих вводов $x_{1},\dots,x_{i-1}$, и вычисляет вывод $a_{i}$.
В ауторегрессивных моделях, контекст - это любое из предыдущих слов. То есть, когда обрабатывается $x_{i}$, модель имеет доступ к $x_{i}$, а также к представлениям всех предыдущих токенов в контекстном окне, но токенов не после $i$.

![[Pasted image 20251117225713.png]]

Рисунок выше показывает поток информации в целостном казуальном слое внутреннего внимания(self-attension), в котором вычисление каждого внимания происходит параллельно в каждой позиции токена $i$. Таким образом, слой внутреннего внимания отображает входные последовательности $x_{1},\dots x_{n}$ в выходные последовательности той же длины $a_{1},\dots a_{n}$.
#### Упрощенная версия внимания

По сути, внимание - это просто взвешенная сумма векторов контекста, со множеством сложностей с тем, как вычисляются веса и что суммируется.
Вывод внимания $a_{i}$ токена позиции $i$ это просто взвешенная сумма всех представлений $x_{j};\forall j\leq i$, $\alpha_{ij}$ означает как много $x_{j}$ вносит в $a_{i}$:
$$
a_{i}=\sum_{j\leq i}\alpha_{ij}x_{j}
$$
Каждый $\alpha_{ij}$ - это скаляр, используемый для взвешивания значения ввода $x_{j}$, когда суммируется ввод для вычисления $a_{i}$. Во внимании мы взвешиваем каждый предыдущий эмбеддинг пропорционально тому, насколько они похожи с текущим токеном $i$. Итак, выход внимания - это сумма эмбеддингов предыдущих токенов, взвешенных по их сходству с текущим эмбеддингом токена. Мы считаем очки схожести с помощью скалярного произведения, тем больше оно, чем больше схожи 2 токена. Затем мы нормализируем очки при помощи softmax, чтобы создать вектор весов $\alpha_{ij},j\leq i$.
$$
\begin{array} \\
\text{score}(x_{i},x_{j})=x_{i}\cdot x_{j} \\
\alpha_{ij} = \text{softmax}(\text{score}(x_{i},x_{j}))\,\forall j\leq i
\end{array}
$$
#### A single attention head, используя матрицы запросов, ключей и значений

Теперь откажемся от упрощений и введем *механизм внимания с одной головой*(single attention head) - механизм внимания, который используется в трансформерах. Этот механизм позволяет нам точно представить три различные роли, которые каждый входной эмбеддинг участвует в ходе процесса внимания:
- Текущий элемент сравнивается с предыдущими входными данными. Мы будем называть эту роль *запросом*;
- В роли предыдущего ввода, который сравнивается с текущим элементом для определения веса сходства. Будем называть эту роль *ключом*.
- И наконец, *значение* предыдущего элемента, которое будет взвешенно и суммировано для вычисления вывода текущего элемента.

Чтобы охватить эти 3 различные роли, трансформеры вводят матрицы весов $W^{Q},W^{K}$  и $W^{V}$. Эти веса будут проецировать каждый вектор ввода $x_{i}$ в представления этих ролей как запрос, ключ и значение:
$$
q_{i}=x_{i}W^{Q};\quad k_{i}=x_{i}W^{K};\quad v_{i}=x_{i}W^{V}
$$
Имея эти проекции, когда мы вычисляем схожесть текущего элемента $x_{i}$ с предыдущим $x_{j}$, мы будем использовать скалярное произведение с вектором запроса $q_{i}$ и предшествующим ключевым вектором $k_{j}$. При этом результат скалярного произведения может получится слишком большим, и экспоненциально большие числа могут привести к числовым ошибкам и потерям градиента во время обучения. Чтобы избежать этого мы масштабируем его на степень размерности эмбеддингов, то есть делим произведение на квадратный корень размерности вектора запроса и ключевого вектора $d_{k}$. Вычисление веса останется как в упрощенной версии, а вот вычисление $\text{head}_{i}$ теперь базируется на весах $\alpha_{ij}$ и векторах $v$.
Сам же вектор вывода механизма внутреннего внимания теперь считается как произведение вектора $\text{head}$ и специальной матрицы $W^{O}$. Которая умножается справа, это делается, чтобы реформировать вектор $\text{head}_{i}$. Вектор ввода $x_{i}$ и вектор вывода $a_{i}$ имеют одинаковую размерность $[1\times d]$. Будем называть $d$ *размерностью модели*.
$$
q_{i}=x_{i}W^{Q};\quad k_{j}=x_{j}W^{K};\quad v_{j}=x_{j}W^{V}
$$
$$
\text{score}(x_{i},x_{j}) = \frac{q_{i}\cdot k_{j}}{\sqrt{ d_{k} }}
$$
$$
\alpha_{ij}= \text{softmax}(\text{score}(x_{i},x_{j}))\quad\forall j\leq i
$$
$$
\text{head}_{i}=\sum_{j\leq i}\alpha_{ij}v_{j}
$$
$$
a_{i}=\text{head}_{i}W^{O}
$$

![[Pasted image 20251118115826.png]]

#### Multi-head attension

Итак, *multi-head attension* состоит из $A$ отдельных механизмов внимания с одной головой, которые находятся в параллельных слоях на одинаковой глубине в модели, каждая голова имеет свои параметры, так как они могут специализироваться на различных задачах, например: представлять различные лингвистические отношения между контекстными словами и текущим словом, или рассматривать частные типы паттернов в контексте.
Таким образом, каждый $\text{head}_{i}$ имеет собственный набор матриц запросов, ключей и значений: $W^{Qi},W^{Ki},W^{Vi}$. Они используются для проецирования в отдельными запросы, ключи и значения для каждой "головы".
Когда мы используем несколько голов размерность модели $d$ все ещё используется для вводного вектора и вектора вывода, а вектора запросов и ключей - размерность $d_{k}$, вектора значений - $d_{v}$. Таким образом, для каждой головы, мы имеем слои веса $W^{Qi}\in \mathbb{R}^{d\times d_{k}},W^{Ki}\in \mathbb{R}^{d\times d_{k}}$ и $W^{Vi}\in \mathbb{R}^{d\times d_{v}}$:
$$
q_{i}^{c}=x_{i}W^{Qc};\quad k_{j}^{c}=x_{j}W^{Kc};\quad v_{j}^{c}=x_{j}W^{Vc};\quad\forall c\in[1,A]
$$
$$
\text{score}^{c}(x_{i},x_{j})= \frac{q_{i}^{c}\cdot k_{j}^{c}}{\sqrt{ d_{k} }}
$$
$$
\alpha_{ij}^{c}=\text{softmax}(\text{score}^{c}(x_{i},x_{j}))\quad\forall j\leq i
$$
$$
\text{head}_{i}^{c}=\sum_{j\leq i}\alpha_{ij}^{c}v_{j}^{c}
$$
$$
a_{i}^{c}=(\text{head}^{1}\oplus \text{head}^{2}\oplus \dots \oplus \text{head}^{A})W^{O}
$$
Следовательно
$$
\text{MultiHeadAttention}(x_{i},[x_{1},\dots,x_{i-1}])=a_{i}
$$
Формула выше корректна только для казуальных моделей.

![[Pasted image 20251118122537.png]]

