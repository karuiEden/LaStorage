*Тэги:* ML
### Условия:
**Постановка задачи:**
Датасет:

$X \in R^{l \times n}, y \in \{0, 1\}^l$  
$$X = \begin{pmatrix}x_{11}&x_{12}&...&x_{1n}\cr&...\cr&&...\cr x_{l1}&x_{l2}&...&x_{ln}\end{pmatrix}  y = \begin{pmatrix}y_{1}\cr...\cr...\cr y_{l}\end{pmatrix} $$


### Вероятностая модель

$X$ - вектор признаков случайных величин
$Y$ - целевая переменная, также случайная величина
Пример модели:
$X$ = (количество кликов раньше, часы активности, уровень доходов)
$Y$ = 1 если клик будет, 0 если клика нет
Тогда можно задать распределение вероятностей:
$$P_{X|Y}(Y = 1|X = (10clicks, 2hours, 10dollars) = вероятность \spaceтого, что\newline\spaceчеловек\spaceс\spaceзаданными\spaceхарактеристиками\spaceкликнет\spaceна\spaceрекламу$$

### Функция правдоподобия

Найдём способ для обучения любой модели, предсказывающей вероятность принадлежности к классу.

**X** - вектор признаков, $\hat{f}(X)$ - наша модель:
$$P(Y=1|X) = \hat{f}(X)$$
Так как мы решаем задачу **бинарной классификации**, то мы можем обучить лишь одну модель, в иных случаях для предсказания вероятности классов надо обучать несколько моделей.

Назовём *правдоподобием* $\prod_{i=1}^l P(Y=y_i|x_i)$ 

Это вероятность получения нашей выборки согласно предсказаниям модели.

#### Обучение через максимальное правдоподобие 

Теорема из статистика гарантирует, что если мы найдём параметры, которые максимизируют правдоподобие, то они будут хорошие для модели.

Но так как максимизировать произведение тяжело, то мы можем максимизировать логарифм произведения.

### Связь с минимизации функции потерь

Преобразуем задачу максимизации в задачу минимизации.
$$L(x) = - \sum_{i=1}^l{ln(P(Y= y_i|x_i))} \rightarrow \underset{W}{min}$$
Мы видим, что минимизация полученного выражения похоже на минимизацию эмпирического риска, где функция потерь - логарифм вероятности правильного класса.

### Предсказание вероятностей

Будем считать, что наша модель предсказывает вероятности, именно поэтому она называется регрессией.

Вероятность для двух классов можно расписать так:
$y_i \in \{0,1\}$

$$ln P(Y=y_i|x_i) = y_iln(\hat{f}(x_i)) + (1-y_i)ln(1 - \hat{f}(x_i))$$
Подставим в раннее полученную формулу функции потерь предсказание вероятностей:

$$L(w) = - \sum^l_{i=1}y_iln(\hat{f}(x_i)) + (1-y_i)ln(1 - \hat{f}(x_i)) \rightarrow \underset{w}{min} \space - LogLoss\space для\space 2\space классов$$