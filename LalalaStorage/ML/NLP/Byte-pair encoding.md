---
title: Byte-pair encoding
created: 2025-11-06
tags:
  - nlp
links:
  - "[[Токенизация]]"
---
Состоит из двух частей: тренер и энкодер.

### Обучение BPE

Обучение BRE алгоритма итеративно объединяет часто соседствующие токены, чтобы создать все более длинные токены. Алгоритм начинает со словарного запаса со словаря, который просто является множеством все символов. Затем анализирует тренировочный корпус, и ищет два символа, которые чаще всего расположены вместе.

Алгоритм продолжает считать и объединять, создавая новые более длинные строки символов, пока не будет выполнено $k$ объединений, создающих $k$ новых токенов, таким образом, $k$ является параметром алгоритма. Результирующий словарь состоит из оригинального множества и $k$ новых токенов.

Обычно алгоритм запускается не на основе символьной последовательности, а только внутри слов. То есть алгоритм не выполняет слияние за пределами слов. Для этого входной корпус разбивается по пробелам и знаками препинания. Это даёт начальный набор строк, каждая из которых соответствует символам слова, вместе с количеством слов. Тогда, хотя количество слов определяется по всему корпусу, слияния допускаются только внутри строк.

```pseudo
\begin{algorithm}
\caption{Byte Pair Encoding (BPE)}
\begin{algorithmic}
\Require corpus $C$, number of merges $k$
\Ensure vocabulary $V$

\State $V \gets$ all unique characters in $C$
\For{$i = 1, \dots, k$}
    \State $(t_L, t_R) \gets$ most frequent pair of adjacent tokens in $C$
    \State $t_{\text{new}} \gets t_L + t_R$
    \State $V \gets V \cup \{t_{\text{new}}\}$
    \State replace all occurrences of $(t_L, t_R)$ in $C$ with $t_{\text{new}}$
\EndFor
\State \Return $V$
\end{algorithmic}
\end{algorithm}
```

### BPE энкодер

Энкодер используется для токенизации тестовую последовательность. Энкодер просто обрабатывает на тестовых данных, используя слияния, которые мы получили на обучающих данных. Обработка происходит в том порядке, в котором мы обучались.

### BRE в практике

Иногда происходит претокенизация, т.е. разбиение на более крупные куски текста.