---
title: LLM
created: 2025-11-16
tags:
  - deep_learning
  - nlp
links:
---
**Определение 1:** *LLM(Большая языковая модель)* - большая языковая модель, которая по контексту может предсказывать огромное количество слов(десятки тысяч).

Существует 3 архитектуры LLM:
1. Декодер. Принимает на вход ряд токенов и итеративно генерирует выходной токен за раз. Информационный поток идет слева направо, что означает, что модель предсказывает следующее слово, только исходя из предыдущих. Декодеры являются генеративными моделями. Примеры: ChatGPT, Gemini, Claude, Mistral.
2. Энкодер. Принимает на вход последовательность токенов и возвращает репрезентацию каждого токена. Информационный поток двусторонний, так как обучается, маскируя слова в тексте, затем пробуют предсказать его. Энкодеры обычно используются для маскированных языковых моделей. Используется для создания классификаторов текста. Примеры: BERT, RoBERTA.
3. Энкодер-декодер. Принимает последовательность токенов и возвращает ряд токенов. В отличии от декодера кодер-декор модели имеют более слабую связь между входными и выходными токенами, и они используются для сопоставления между разными типами токенов. Поэтому выходной ряд токенов может очень сильно отличаться от входного по длине и по типу токенов. Используются для машинного перевода, где на входе токены одного языка, а выходные - другого языка. Также используются для распознавания речи, где на входе токены, отвечающие за речь, а на выходе текстовые токены.
Все эти архитектуры могут быть построены на разном виде нейронных сетей, но чаще всего используются трансформеры. Некоторые построены на LTSM, а также на более современных state space models.

Почти всё, что можно сделать с языком, может быть моделировано как *условная генерация* текста. Мы даём модели на ввод кусок текста, *промт*, и затем заставляем модель продолжает генерировать текст токен за токеном, в зависимости от промта и впоследствии сгенерированных токенов. Мы генерируем из модели сначала вероятность следующего токена $w_{i}$ исходя предыдущего контекста $P(w_{i}\ | \ w_{<i})$ и затем сэмплируем из распределения для генерации токена.

Чтобы модель могла отвечать на вопросы и как-то общаться, помимо простой генерации, то необходимо её до обучить с помощью *instruction-tuning*.
**Определение 2:** *Instruction-tuning* - процесс дообучения LLM, на котором её учат следовать инструкциям пользователя.
Пример инструкций:
- Ответы на вопросы;
- Объяснение;
- Выполнение команд;
- Исправление текста;
- Написание кода;
- Рассуждение;
- Соблюдение формата.

**Определение 3:** *Prompt* - текстовая строка, которую пользователь отправляет языковой модели, чтобы та выполнила какое-либо полезное действие.

Промпты с помеченными примерами называют промпты с несколькими повторениями(few-shot). Наоборот, промпты без помеченных примеров называют zero-shot промптами.

Во время instruction-tuning примеров должно быть не очень много, иначе она начнет переобучаться на точных примерах, так как демонстрация должна показывать формат вывода ответа, а не правильные ответы. Неправильные ответы, также могут улучшить модель.

Промпты помимо способа заставить модель генерировать текст, могут быть рассмотрены как обучающие сигналы, особенно когда они включают в себя демонстрации. Такой метод отличается от методов претренинга, так как веса не меняются, меняются лишь контекст и активации в сети. Такое обучение называется *контекстным(incontext learning)*.

**Определение 4:** *Контекстное обучение* - обучение, которое улучшает производительность модели или сокращает лосс, но не вовлекает обновления градиентов внутренних параметров модели.

Каждая LLM обладает *системным промптом*, текстовый промпт, который является первой инструкцией языковой модели, определяет её задачи и роль, устанавливает контекст и тон.

Как было сказано выше, токены генерируются на основе распределения вероятности. Но перед этим модели считают для каждого слова в словаре баллы, называемые *логитами*, затем нормализируем через $\text{softmax}$ и получаем настоящее вероятностное распределение.

![[Pasted image 20251116172020.png]]

 И зная вероятности, выбирается уже токен.

**Определение 5:** *Декодинг* - выбор токена для генерации на основе вероятностей модели.

**Определение 6:** *Авторегрессивная генерация* - декодинг языковой модели слева направо и, таким образом, повторяющийся выбор следующего токена на основе предыдущих выборов.

Существует два вида генерации токенов:
1. [[Жадный декодинг|Жадный декодинг]];
2. [[Случайное сэмплирование|Случайное сэмплирование]];
3. [[Температурная выборка]].
### Обучение LLM

Состоит из нескольких этапов:
1. *Pretrainig*. В первой стадии, модель обучается инкрементальному предсказанию следующего слова в огромных текстовых корпусах. Модель использует кросс-энтропийный лосс, который называется лосс языкового моделирования и потом корректируется на методе обратного распространения ошибки. В результате получается модель, которая очень хорошо предсказывает слова и может генерировать текст.
2. *Instruction tuning*, также называется обучение файтюнингом или **SFT**: Во втором этапе, модель обучается, снова через кросс энтропийный лосс, чтобы следовать инструкциям. Обучается на специальном корпусе с огромным текстом, содержащем инструкции и корректным ответом к инструкции.
3. *Aligment*, также называется выравниваем предпочтением. На этом заключительном этапе модель обучается, чтобы она стала максимально полезной. Здесь модели предоставляются данные о предпочтениях, состоящие из контекста, за которым следует 2 потенциальных продолжения, одно отмеченное как принятое и как отклоненное. Затем модель обучается с помощью обучения с подкреплением или других алгоритмов с наградой, чтобы создавать принятое продолжение, а не отклоненное.

![[Pasted image 20251116190103.png]]
### Предобучение

Идея предобучения LLM основана на идее самообучении. При самообучении для языкового моделирования мы берём корпус текста в качестве обучающего материала и на каждом временном шаге $t$ просим модель предсказать следующее слово. Сначала она будет предсказывать плохо, но со временем она станет делать это всё лучше и лучше. Мы просто обучаем модель минимизировать ошибку в предсказании следующего слова.

На практике, обучение модели означает настройку параметров базовой архитектуре. Обучаться она будет методом обратного распространения ошибки. Поэтому нам нужны функция потери и обратное распространение ошибки. В качестве лосс функции используем кросс энтропийную функцию потерь.
$$
L_{CE}=-\sum_{w\in V}y_{t}[w]\log \hat{y}_{t}[w]
$$
В этом случае моделирования языка, правильное распределение $y_{t}$ приходит из знаний следующего слова. Это репрезентуется в виде one-hot вектора с 1 на следующем слове и 0 на остальных словах. Таким образом, лосс кросс-энтропии для моделирования языка определяется вероятностью, которую модель присваивает следующему правильному токену.

Без общего лосса можно сказать, что во время $t$ лосс кросс энтропии можно упростить до отрицательной логарифмической вероятности, которую модель присваивает следующему слову в обучающей последовательности, $-\log(w_{t+1})$, или более формально используя $\hat{y}$, что означает вектор  вероятностных оценок токенов из языковой модели:
$$
L_{CE}(\hat{y}_{t},y_{t})=-\log \hat{y}_{t}[w_{t+1}]
$$
Таким образом, в каждой позиции слова $t$ вводных данных, модель принимает на ввод правильную последовательность токенов $w_{1:t}$, и использует их для вычисления вероятностного распределения возможных следующих токенов, чтобы посчитать потерю модели для следующего токена $w_{t+1}$. Затем мы переходим к следующему слову, игнорируем предсказание модели для следующего слова, вместо этого мы используем правильную последовательность слов $w_{1\vartheta+1}$, чтобы модель оценила вероятность токена $w_{t+2}$. Эта идея, что мы всегда даем модели правильную последовательность, чтобы предсказать следующее слово, называемое *teaching force*(принудительным обучением).

![[Pasted image 20251117131526.png]]

На рисунке выше изображен общий подход обучения LLM. На каждом шаге, учитывая все предыдущие токены, модель формирует выходное распределение по всему словарю. Во время обучения вероятность, присвоенная правильному слову, используется для вычисления кросс-энтропийной потери для каждого элемента в последовательности. Потери для каждого набора представляют собой среднюю кросс-энтропийную потерю по всей последовательности отрицательных логарифмических вероятностей, или, более формально
$$
L_{CE}(\text{batch of length T})= \frac{1}{T} \sum_{t=1}^{T}-\log \hat{y}_{t}[w_{t}]
$$
Веса модели в сети затем корректируются, чтобы минимизировать кросс-энтропийную потерю по всему набору с помощью градиентного спуска, используя обратное распространение ошибки на вычислительных графах, чтобы посчитать градиент. Обучение корректирует все веса модели.

### Finetuning

**Определение 7:** *Finetuning* - процесс дополнительного обучения уже предварительно обученной языковой модели на новой выборке данных, используя кросс-энтропийную модель.

**Определение 8:** *Continued Pretraining* - finetuning, при котором мы просто продолжаем обучение, будто новые данные находятся в конце данных предварительного обучения.

![[Pasted image 20251117135629.png]]


### Оценивание LLM
Точность предсказывания слов мы можем оценить при помощи [[Perplexity]]. Но есть и другие точности, которые мы хотим проверить, а именно для различных задач, такие как ответы на вопрос, машинного перевода и размышлений. Рассмотрим одну из метрик: механизм измерения точности ответов на вопросы с множественным выбором ответов. Датасет **MMLU** (Massive Multitask Language Understanding) часто используемый датасет, состоящий из 15908 ответов и вопросов на рассуждения в 57 сферах. Точность ответов на эти вопросы с несколькими вариантами ответов может быть полезным показателем способности модели рассуждать и её фактических знаний.

Но использование MMLU в качестве метрики качества языковой модели имеет проблему, которая характерна для всех оценок, основанных на общедоступных данных. Эта проблема *загрязнение данных*. Оно происходит, когда часть тестовых данных попадает в обучающие данные, из-за чего на вопросах, которые содержатся в обучающем датасете, будут завышать качество модели. Один из способов уменьшить загрязнение данных - предоставить доступ точным обучающим данным, или сообщать о перекрытии обучения с конкретными тестовыми данными.

Также можно оценивать модель по параметрам загрязнения, насколько они огромные и тд
 