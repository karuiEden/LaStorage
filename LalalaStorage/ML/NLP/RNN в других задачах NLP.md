---
title: RNN в других задачах NLP
created: 2025-12-17
tags:
  - nlp
links:
  - "[[RNN]]"
---
### Маркировка последовательностей

В маркировке последовательностей, задача сети состоит в присваивании, выбранной из маленького фиксированного набора меток для каждого элемента последовательности. Одной из классических задач разметки последовательностей является разметка частей речи. В подходе RNN для маркировки последовательностей, вводные данные это эмбеддинги слов и выходные данные - вероятности маркировок, сгенерированные слоем softmax по всему набору меток.

![[Pasted image 20251223170451.png]]

На этом рисунке входными данными являются предварительно обученные эмбеддинги слов, соответствующие входным токенам. Блок RNN представляет собой абстракцию, описывающую развернутую простую рекуррентную сеть , состоящую из входного слоя, скрытого слоя и выходного слоя на каждом шаге, а также общих матриц $U$, $V$ и $W$, составляющие сеть. Выходные данные сети на каждом шаге представляют собой распределение по набору POS-тегов, сгенерированное слоем softmax.

Чтобы сгенерировать последовательность токенов для заданного входного значения мы выполняем прямой инференс по входной последовательностей и выбираем самый вероятный тег из softmax на каждом шаге. Поскольку используется слой softmax для генерации распределения по выходному набору тегов на каждом шаге времени, снова будет использоваться функция потерь кросс-энтропии во время обучения.

### Классификация последовательностей

Другое использование RNN заключается в классификации целой последовательности в 2 или 3 класса. Такой набор задач называется *классификацией текста*.

Для применения RNN в данных задачах, текст, чтобы классифицироваться, пропускается через RNN по слову за один момент времени, генерируя новое скрытое представление на каждом шаге времени. Затем можем взять скрытый слой для последнего токена текста, $h_{n}$, чтобы создать сжатое представление всей последовательности. Можем передать это представление $h_{n}$ в сеть прямого распространения, которая выбирает класс с помощью функции softmax из возможных классов.

![[Pasted image 20251223172620.png]]

Обратите внимание, что в этом подходе нам не нужны промежуточные выходные данные для слоев в последовательности, предшествующей последнему элементу. Следовательно, с этими элементами не связаны никакие функции потерь. Вместо этого функция потерь, используемая для обучения весов в модели, полностью основана на конечной задачи классификации текста. Выходные данные функции softmax из прямого классификатора вместе с функцией потерь кросс-энтропии управляют обучением. Сигнал ошибки классификации распространяется обратно через веса прямого распространения, через его входные данные, а затем через три набора весов в RNN.
Режим обучения, который использует потери из последующего приложения для корректировки весов на всем протяжения сети называется *сквозным обучением (end-to-end training)*.

Другой вариант, вместо использования скрытого состояния последнего токена $h_{n}$ для представления всей последовательности,- это использование какой-либо функции объединения(pooling) всех скрытых состояний $h_{i}$ для каждого слова $i$ в последовательности. Например, мы можем создать представление, которое объединяет все $n$ скрытых состояний, беря их поэлементное среднее:
$$
\mathrm{h}_{mean} = \frac{1}{n} \sum_{i=1}^{n}\mathrm{h}_{i},
$$
Или же можно взять поэлементный максимум; поэлементный максимум множества из $n$-векторов - это новый вектор, $k$-ый элемент которого является максимумом $k$-ых элементов всех $n$ векторов.

### Генерация с помощью RNN моделей

RNN модели также могут быть использованы для генерации текста, т.е. авторегрессивными моделями. На рисунке ниже изображена RNN в виде авторегрессивной модели:

![[Pasted image 20251223174944.png]]

Это простая архитектура лежит в основе современных подходов к таким приложениям, как машинный перевод, суммаризация, ответы на вопросы. Ключ к этим подходам заключается в том, чтобы подготовить компонент генерации к работе с соответствующим контекстом. То есть, вместо простого использования <s> для начала работы, мы можем предоставить более богатый контекст, соответствующий задаче, для перевода контекстом является предложение на языке оригинала, для суммаризации - длинный текст, который мы хотим суммировать.