---
title: RNN как языковые модели
created: 2025-12-17
tags:
  - nlp
  - deep_learning
links:
  - "[[RNN]]"
---
Языковые модели на основе RNN обрабатывают входную последовательность по одному слову за раз, пытаясь предсказать следующее слова на основе текущего слова и предыдущего скрытого состояния. Таким образом, RNN не имеют проблемы ограниченного контекста, которая есть у n-грам моделей, или фиксированного контекста, которая есть у моделей прямого распространения, поскольку скрытое состояние в принципе может представлять информацию обо всех предыдущих словах вплоть до начала последовательности. 
На рисунке ниже показано различие между языковой моделью прямого распространения и языковой моделью RNN, демонстрируя, что языковая модель RNN использует $h_{t-1}$, скрытое состояние с предыдущего шага времени. в качестве представления прошлого контекста.

![[Pasted image 20251217192811.png]]

### Прямой инференс в RNN языковой модели

Прямой инференс в RNN осуществляется также как описано [[RNN#Вывод(Inference) в RNN|здесь]]. Входная последовательность $\mathrm{X}=[\mathrm{x}_{1};\dots;\mathrm{x}_{t};\dots;\mathrm{x}_{N}]$ состоит из ряда слов, каждый представлен как one-hot вектор размером $|V|\times1$, и выходное предсказание, $\hat{y}$, вектор, представляющий вероятностное распределение по всему словарю. На каждом шаге, модель использует матрицу эмбеддингов слов $\mathrm{E}$ для получения эмбеддинга текущего слова, умножает его на матрицу весов $\mathrm{W}$, и затем складывает это со скрытым слоем из предыдущего шага (умноженным на матрицу весов $\mathrm{U}$), чтобы вычислить новый скрытый слой. Скрытый слой затем используется для генерации выходного слоя, который пропущен через слой softmax для генерации вероятностного распределения по всему словарю. То есть, во время $t$:
$$
\begin{array} \\
\mathrm{e}_{t}=  \mathrm{Ex}_{t} \\
\mathrm{h}_{t} = g(\mathrm{Uh}_{t-1}+\mathrm{We}_{t}) \\
\mathrm{\hat{y}}_{t} = \text{softmax}(\mathrm{Vh}_{t})
\end{array}
$$

Когда делается моделирование языка с RNN удобно предположить, что размерность эмбеддинга $d_{e}$ и размерность скрытого слоя $d_{h}$ одинаковы. Поэтому будем называть обе эти величины *размерностью модели* $d$. Таким образом, матрица эмбеддингов $\mathrm{E}$ размером $[d\times |V|]$, и $\mathrm{x}_{t}$ - one hot вектор размера $[|V|\times 1]$. Произведение $\mathrm{e}_{t}$, таким образом, формы $[d\times1]$. $\mathrm{W}$ и $\mathrm{U}$ имеют размеры $[d\times d]$, и $\mathrm{h}_{t}$ тоже формы $[d\times 1]$. $\mathrm{V}$ имеет размерность $[|V|\times d]$, итак результат $\mathrm{Vh}$ - вектор размера $[|V|\times 1]$. Этот вектор можно рассматривать как набор оценок по всему словарю, полученных на основе данных, представленных в $\mathrm{h}$. Пропуская эти оценки через softmax, мы нормализуем их, преобразуя в вероятностное распределение. Вероятность того, что конкретное слово $k$ в словаре является следующим словом, представлена $\mathrm{\hat{y}}_{t}[k]$, $k$-ая компонента $\mathrm{\hat{y}}$:
$$
P(w_{t+1}=k\ | \ w_{1},\dots,w_{t})=\mathrm{\hat{y}}[k]
$$

Вероятность целой последовательности это просто произведение вероятностей каждого элемента последовательности, где будем использовать $\mathrm{\hat{y}}_{i}[w_{i}]$ для обозначения вероятности истинного слова $w_{i}$ на шаге времени $i$.
$$
P(w_{1:n})=\prod_{i=1}^{n}P(w_{i}\ | \ w_{1:i-1}) = \prod_{i=1}^{n}\mathrm{\hat{y}}_{i}[w_{i}]
$$

### Обучение RNN языковой модели

Для обучения RNN как языковой модели, будем использовать алгоритм самообучения . Берем корпус текста обучающего материала и на каждом шаге времени $t$ просим модель предсказать следующее слово. Называем модель самообучающейся, потому что мы не можем добавлять специальные эталонные отметки к данным, естественная последовательность слов сама по себе является самоподготовкой. Мы просто обучаем модель минимизировать ошибку в предсказании истинного следующего слова в обучающей последовательности, используя кросс-энтропию в качестве функции потерь. 
$$
L_{CE}= - \sum_{w\in V}\mathrm{y}_{t}[w]\log \mathrm{\hat{y}}_{t}[w]
$$

В данном случае моделирования языка, правильное распределение $\mathrm{y}_{t}$ приходит из знания следующего слова. Это представляется как one-hot вектор, совпадающий со словарем, где значение для следующего слова равно $1$, а все остальные значения равны $0$. Таким образом, функция потерь кросс-энтропии для языкового моделирования определяется вероятностью, которую модель присваивает правильному следующему слову. Поэтому в момент времени $t$ функция потерь CE представляет собой отрицательный логарифм вероятности, которую модель присваивает следующему слову в обучающей последовательности.
$$
L_{CE}(\mathrm{\hat{y}_{t}}, \mathrm{y}_{t}) = -\log \mathrm{\hat{y}_{t}}[w_{t+1}]
$$

Таким образом, для каждого слова позиции $t$ входных данных, модель берет на вход правильное слово $w_{t}$ вместе с $h_{t-1}$, кодируя информацию из предыдущих $w_{1: t-1}$, и затем использует их, для вычисления распределения вероятностей по всем возможным следующим словам, чтобы вычислить ошибку модели для следующего токена $w_{t+1}$. Затем мы переходим к следующему слову, игнорируем предсказанное моделью значение и вместо этого используем правильное слово $w_{t+1}$ вместе с закодированной предыдущей историей для оценки вероятности токена $w_{t+2}$. То есть используем метод "принуждения учителя".

Веса в сети корректируются для минимизации средней ошибки CE на протяжении обучающей последовательности с помощью градиентного спуска.

![[Pasted image 20251217201544.png]]

### Связывание весов

Столбцы матрицы $\mathrm{E}$ представляют эмбеддинги слов для каждого слова из словаря, изученного в процессе обучения, с целью получения схожих векторных представлений для слов, имеющих схожее значение и функцию. Поскольку при использовании рекуррентных нейронных сетей для моделирования языка предполагаем, что размерность эмбеддинга и размерность скрытого слоя одинаковы, матрица эмбеддингов $\mathrm{E}$ имеет форму $[d\times |V|]$. А матрица последнего слоя $V$ позволяет оценить вероятность каждого слова в словаер, исходя из данных, имеющихся в последнем скрытом слое сети, путем вычисления $\mathrm{Vh}$. Матрица $\mathrm{V}$ имеет форму $[|V|\times d]$. То есть, строки $\mathrm{V}$ имеет форму, как транспонированная матрица $\mathrm{E}$, означает, что $\mathrm{V}$ предоставляет второй набор изученных эмбеддингов слов.

Вместо того, чтобы иметь 2 набора матриц эмбеддингов, языковые модели используют одну матрицу эмбеддингов, которая присутствует как на входном слое, так и на слое softmax. То есть, мы обходимся без $\mathrm{V}$ и используем $\mathrm{E}$ в начале вычислений и $\mathrm{E^{T}}$ в конце вычислений. Используя ту же матрицу в двух местах называется *связыванием весов*. Уравнения со связанными весами для RNN языковой модели принимает следующий вид:
$$
\begin{array} \\
\mathrm{e}_{t} = \mathrm{Ex}_{t} \\
\mathrm{h}_{t} = g(\mathrm{Uh}_{t-1}+ \mathrm{We}_{t})  \\
\mathrm{\hat{y}}_{t}= \text{softmax}(\mathrm{E^{T}h}_{t})
\end{array}
$$

Помимо улучшения показателя перплексии модели, этот подход значительно сокращает количество параметров, необходимых для модели.