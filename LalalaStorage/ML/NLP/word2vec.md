---
title: word2vec
created: 2025-11-14
tags:
  - nlp
  - supervised
links:
  - "[[Эмбеддинги]]"
---
**Определение 1:** *word2vec* - пакет с алгоритмами эмбеддинга. Эмбеддинги word2vec являются статическими.
Идея word2vec состоит в том, что вместо того, чтобы считать как часто каждое слово появляется рядом со необходимым словом, обучают бинарный классификатор, который отвечает на вопрос "Появится ли скорее всего слово $w$ рядом с необходимым словом?" И веса полученного классификатора и будут являться эмбеддингом необходимого слова.

Ключевая идея заключается в том, что для обучающей выборке мы можем использовать бегущий текст, таким образом, модель будет самообучающейся, ибо нам не нужно размечать данные для этого. По сути word2vec является однослойной нейронной сетью(многоклассовая [[Логистическая регрессия|логистическая регрессия]]).

Рассмотрим один из алгоритмов word2vec под названием *skip-gram with negative samplings(SGNS)*.
Идея данного алгоритма заключается в:
1. Представляем целевое слово с рядом стоящими контекстными словами как положительный пример;
2. Случайно выбираем другие слова в лексиконе и пары этих слов с целевым будут отрицательными примерами;
3. Используем логистическую регрессию для обучения классификатора, чтобы различать эти 2 класса;
4. Используем веса модели как эмбеддинг слова.
Тогда вероятность положительного класса равна:
$$
P(+\ | \  w,c_{1:L})=\prod_{i=1}^{L}\sigma(c_{i}\cdot w),
$$
где $w$ - вектор целевого слова, а $c_{i}$ - вектор контекстного слова. Таким образом для каждого слова мы будем иметь 2 вектора, одно как целевое слово, второе как контекстное. Для использования слова в виде вектора могут использовать сумму векторов этого слова или использовать только целевой вектор слова.

![[Pasted image 20251114121528.png]]

Рядом стоящие слова определяются размером контекстного окна, и для при $k$ положительных примеров мы должны брать $2k$ отрицательных примеров. При этом отрицательные слова выбираются при помощи юниграмной взвешенной вероятности $p_{\alpha}(w)$, где $\alpha$ - вес. Если использовалась бы обычная вероятность, то для слова the в качестве шума могло выбраться слово the. 
$$
P_{\alpha}(w)= \frac{\text{count}(w)^{\alpha}}{\sum_{w'}\text{count}(w')}
$$
В практике в качестве веса используется $\alpha=0.75$.
Таким образом цель алгоритма:
- Максимизировать сходства целевого слова и контекстных слов положительных примеров;
- Минимизировать сходства целевого слова и контекстных слов отрицательных примеров.

![[Pasted image 20251114122056.png]]

