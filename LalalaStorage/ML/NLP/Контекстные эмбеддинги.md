---
title: Контекстные эмбеддинги
created: 2025-12-04
tags:
  - nlp
links:
  - "[[Эмбеддинги]]"
  - "[[Двунаправленные энкодер-трансформеры]]"
---
Имея предварительно обученную языковую модель и новое входное предложение, мы можем рассматривать последовательность выходных данных модели как *контекстные эмбеддинги* для каждого токена во входных данных.

**Определение:** *Контекстные эмбеддинги* - векторы, представляющие некоторый аспект значения токена в контексте, и могут быть использованы для любой задачи, требующей значения токенов или слов.

Более формально, имея входную последовательность $x_{1},x_{2},\dots,x_{n}$, мы можем использовать выходной вектор $h^{L}_{i}$ из последнего $L$ слоя модели как представление значения токена $x_{i}$ в контексте последовательности $x_{1},\dots,x_{n}$. Или вместо простого использования вектора $h_{i}^{L}$ из последнего слоя модели, вычислить представление $x_{i}$, усредняя выходные токены $h_{i}$ последних 4 слоёв.

![[Pasted image 20251204013449.png]]

Как используются статические эмбеддинги для представления значения слова, мы можем использовать контекстные эмбеддинги для представления значения слова в контексте для любой задачи, где модели требуется значения слов. В то время как статические эмбеддинги представляют значения типов слов, контекстные эмбеддинги представляют значения экземпляров слов: экземпляра определенного типа слов в определенном контексте.

### Контекстные эмбеддинги и смысл слов

Слова неоднозначны: одно слово может быть использовано в разных вещах. Слова, которые имеют несколько смыслов называются *многозначными*.

**Определение:** *Смысл слова* - дискретное представление одного аспекта значения слова.

Мы можем найти смыслы слов в онлайн-тезаурусах, таких как WordNet, где имеются наборы данных на многих языках, с перечнем значений многих слов.

Факт, что контекст устраняет неоднозначность смысла слов может быть визуализирован.

![[Pasted image 20251204015053.png]]


Таким образом, тезаурусы предлагают дискретные списки смыслов, эмбеддинги предлагают непрерывную многомерную модель значений, хотя и может быть кластеризирована, не делится на полностью дискретные величины.

#### Разрешение неоднозначности смысла слова

Задача по выбору правильного смысла слова называется *разрешение неоднозначности смысла слова* (*word sense disambiguation*) или *WSD*. WSD алгоритмы принимают на вход слово в контексте и набор возможных значений слов и выводит верное значение слова.

![[Pasted image 20251204020357.png]]

WSD алгоритмы могут быть полезным аналитическим инструментом для анализа текста в гуманитарных и социальных науках, и смысл слов могут играть роль в интерпретируемости модели для представлений слов. Смысл слов также имеют интересные свойства распределения. Например, слово часто используется примерно в одном и том же значении в дискурсе, это наблюдение называется правилом *одного значения на дискурс*.

Самый эффективный WSD алгоритм - это просто алгоритм ближайшего соседа, использующий контекстные эмбеддинги. Во время обучения мы пропускаем каждое предложение из некоторого набора данных с разметкой смысла через любой контекстный эмбеддинг, что приводит к контекстному эмбеддингу для каждого помеченного токена. Затем для каждого смысла $s$ из любого слова в корпусе, для каждого из $n$ токенов этого смысла, мы усредняем их $n$ контекстных представлений $v_{i}$, чтобы получить контекстный *эмбеддинг смысла* $v_{s}$ для $s$:
$$
v_{s} = \frac{1}{n}\sum_{i}v_{i}\quad \forall v_{i}\in\text{tokens}(s)
$$

В тестовое время, имея токен целевого слова $t$ в контексте, мы вычисляем контекстный эмбеддинг $t$ и выбираем ближайший соседний смысл из обучающего множества:
$$
\text{sense}(t)= \arg_{s \in \text{senses}(t)} \max \text{cosine}(t,v_{s})
$$
![[Pasted image 20251204022435.png]]

### Контекстные эмбеддинги и схожесть слов

Часто измеряют схожесть двух экземпляров двух слов, используя косинус между их контекстными эмбеддингами.

Обычно перед вычислением косинуса требуется внести некоторые преобразования в эмбеддинги. Это связано с тем, что контекстные эмбеддинги обладают свойством, что векторы для всех слов очень похожи. Если посмотреть на эмбеддинги из последнего слоя BERT или других моделей, эмбеддинги для экземпляров любых двух выбранных случайно слов будут иметь крайне высокие косинусы, которые могут быть довольно близки к 1, что означает, что все векторы слов стремятся указывать в одном направлении. Свойство векторов в системе, которые стремятся указывать в одном направлении, известно как *анизотропия*. Слово "изотропия" означает однородность во всех направлениях, поэтому в изотропной модели набор векторов должен быть ориентирован во всех направлениях, а ожидаемый косинус между парой случайных эмбеддингов равен нулю. Одной из причин анизотропии является то, что косинус определяется небольшим числом измерений контекстного эмбеддинга, значения которых сильно отличаются от других: эти нестандартные измерения имеют очень большие величины и очень высокую дисперсию.

Эмбеддинги можно сделать более изотропичными, стандартизируя вектора.

Одна проблема с косинусом, которая не решается стандартизацией, заключается в том, что косинус имеет тенденцию недооценивать человеческие суждения о схожести значений слов для очень частых слов.

