---
title: Случайное сэмплирование
created: 2025-11-16
tags:
  - nlp
links:
  - "[[LLM]]"
---
**Определение:** Случайная выборка(сэмплирование) - случайный выбор токена в соответствии с  вероятностями модели и её предыдущими выборами. То есть больше генерировать токены, которые имеют большую вероятность модели и меньше генерировать токены, которые имеют меньшую вероятность.

По сути мы будем генерировать токены, пока не выпадет токен EOS.

Но у данного алгоритма есть минус, так как у маловероятных токенов тоже есть шанс быть сгенерированным, поэтому могут получится странные предложения. Поэтому используем для моделей модификации случайной выборки, такие как температурная выборка.

## Температурное сэмплирование

Идея температурной выборки заключается в изменении распределения вероятности таким образом, чтобы увеличить вероятности высоковероятных токенов и уменьшить вероятности низковероятных токенов. В результате, модель меньше генерирует низковероятных токенов и больше высоковероятных токенов.

Мы имплементируем эту идею просто делением логитов на *параметр температуры* $\tau$ перед тем, как передадим это в $\text{softmax}$. В низкотемпературной выборке $\tau \in(0,1]$. В высокотемпературной выборке $\tau >1$.
$$
y=\text{softmax}(u / \tau)
$$
![[Pasted image 20251116182711.png]]

В зависимости от коэффициента выборка могла уходить в жадный декодинг, если коэффициент стремится к нулю, а при увеличении стремится к равновероятному распределению.

![[Pasted image 20251116183331.png]]

### Top-k выборка

*Top-k сэмплирование* - это просто обобщение [[Жадный декодинг|жадного декодирования]]. Вместо выбора одного самого вероятного слова, мы урезаем распределение до $k$ самых вероятных слов, нормализуем снова, чтобы получить правильное распределение, и затем случайно выбираем из этих $k$ слов, соответствующих с ними нормализованными вероятностями.
Более формально:
1. Выбрать зараннее число слов $k$;
2. Для каждого слова в словаре, используем языковую модель для вычисления правдоподобия этого слова, имея контекст $p(w_{t}\ | \ \mathbf{w}_{<t})$
3. Сортируем слова по их правдоподобию, и выбрасываем все слова, которые находятся не в топ $k$ слов по вероятности.
4. Ренормализуем очки $k$ слов, чтобы распределение было корректным;
5. Случайно выбираем слово из оставшихся $k$ самых вероятных слов по его вероятности.
Когда $k=1$, то мы получаем обычное жадное декодирование. Если $k>1$, то иногда мы можем выбрать слово, которое точно не самое вероятное, но всё еще приемлемое, тогда генерация получается более разнообразная, но все еще высокоточная.

### Ядровая или top-p выборка

*Top-p выборка* заключается в сохранении не первых $k$ слов, а первых $p$ процентов вероятностной массы. Цель та же, что и у *top-k*: урезать распределение, чтобы удалить крайне маловероятные слова. Но, измеряя вероятность, а не количество слов, можно надеяться, что измерение будет более надежным в самых разных контекстах, динамических контекстах, динамически увеличивая и уменьшая пул слов-кандидатов.
Имея распределение $P(w_{t}\ | \ \mathbf{w}_{<t})$, мы сортируем распределение с самого вероятностного, и затем *top-p* словарь $V^{(p)}$ - самый маленький набор слов, в котором
$$
\sum_{w\in V^{(p)}}P(w\ | \ \mathbf{w}_{<t})\geq p
$$
