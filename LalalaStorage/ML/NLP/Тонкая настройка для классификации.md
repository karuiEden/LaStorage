---
title: Дообучение для классификации
created: 2025-12-04
tags:
  - nlp
  - deep_learning
links:
  - "[[Двунаправленные энкодер-трансформеры]]"
---
### Классификация последовательностей

Задача классификации последовательностей - классифицировать целую последовательность текста одной меткой. Множество таких задач называется *классификацией текста*, такие как определение тональности текста или определение спама, в которых классифицируем текст по двум или трем классам, а также задачи классификации с большим количеством категорий, например, классификацией по темам на уровне документа.

Для классификации последовательностей входную последовательность текста представляют для классификации единым вектором. Один способ это взять сумму или среднее последних выходных векторов каждого токена в последовательности. Для BERT, мы вместо этого добавляем специальный токен **\[CLS\]**, и присоединяем его к началу каждой входной последовательности, как во время обучения, так и во время кодирования. Выходной вектор в последнем слое модели для входных данных **\[CLS\]** представляет всю последовательность и служит входными данными для головы классификатора, логистической регрессии или нейронной сети классификатора, который принимает соотвествующее решение.

Для примера, вернемся к задаче классификации тональности. Тонкая настройка классификатора для этого приложения включает в себя обучение весов $W_{c}$ для сопоставления выходного вектора $h^{L}_{CLS}$ с набором оценок по возможным классам тональности и размерности модели $d$, $W_{c}\in \mathbb{R}^{d\times3}$. Чтобы классифицировать документ, мы пропускаем входной текст через предварительно обученную языковую модель для генерации $h^{L}_{CLS}$, умножаем его на $W_{c}$ и передаем полученный вектор в softmax.
$$
y = \text{softmax}(h^{L}_{CLS}W_{c})
$$

Для тонкой настройки значений $W_{c}$ требуются данные с ответами, состоящие из входных последовательностей и классов тональности. Обучение проходит как обычно: потеря кросс-энтропии между правильным ответом и ответом, полученным через softmax используется для управлением обучения, которое формирует $W_{c}$.

Эта потеря может быть не только для обучения весов классификатора, но и для обновления весов самой предобученной языковой модели. На практике приемлемая эффективность классификатора обычно достигается лишь минимальными изменениями параметров языковой модели, часто ограничиваясь обновлениями на последних нескольких слоях трансформера.

![[Pasted image 20251204031328.png]]


### Классификация пар последовательностей

Тонкая настройка приложения для задач классификации пар последовательностей происходит так же, как и предварительное обучение, используя цель NSP. Во время тонкой настройки пары меченных предложений из множества с ответами тонкой настройки предоставляются модели, и проходят через все слои модели для получения выходных значений $h$ для каждого входного токена. Как и при задаче классификации последовательности, выходной вектор, связанный с добавленным токеном **\[CLS\]**, представляет собой представление модели о входной паре. И, как и при обучении NSP, два входных предложения разделяются токеном **\[SEP\]**. Для выполнения классификации вектор $h$ на множество обученных весов классификации и пропускается через softmax для генерации предсказания меток, которые затем используются для обновления весов.
Рассмотрим на примере *задачи классификации выводов* с Multi Genre Language Inference (MultiNLI) датасетом. В задаче вывода на естественном языке, также называемой распознавание текстового вывода, модель представлена с двумя предложениями и должна классифицировать отношения между их значениями. Например, в MultiNLI корпусе пары предложений имеют 3 метки: следствие, противоречие или нейтральное. Эти метки описывают отношения между значением первого предложения(предпосылкой) и предложением второго предложения(гипотезы).

Отношение противоречия означает, что предпосылка противоречит гипотезе, следствие означает, что предпосылка соответствует гипотезе, нейтральное означает, что ничего из двух неверно.

Чтобы тонко настроить классификатор для задачи MultiNLI, мы проводим пару предпосылка/гипотеза через двунаправленный энкодер и используем выходной вектор токена **\[CLS\]** как входные данные для головы классификации. Эта голова классификации является такой же, что и для классификации одной последовательности.