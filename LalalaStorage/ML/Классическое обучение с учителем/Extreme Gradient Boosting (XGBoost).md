---
title: Extreme Gradient Boosting (XGBoost)
created: 2025-10-04
tags:
  - ml
  - decision_tree
links:
  - "[[Градиентный бустинг над деревьями]]"
---
Вспомним, что на каждой итерации градиентного бустинга вычисляется вектор сдвигов $s$, который показывает, как нужно скорректировать ответы композиции обучающей выборке, чтобы как можно сильнее уменьшить ошибку:$$s=\left( - \frac{\partial L}{\partial z}\bigg|_{z=a_{N-1}(x_{i})}  \right)_{i=1}^{\ell}=-\nabla_{z}\sum_{i=1}^{\ell}L(y_{i},z_{i})\bigg|_{z_{i}=a_{N-1}(x_{i})}$$
После этого новый базовый алгоритм обучается путем минимизации среднеквадратичного отклонения от вектора сдвигов $s$:
$$b_{N}(x)=\arg_{b\in \mathcal{A}}\min\sum_{i=1}^{\ell}(b(x_{i})-s_{i})^{2}$$
Попробуем обосновать с другой стороны, почему используем среднеквадратичную функцию потерь.

Мы хотим найти алгоритм $b(x)$, решающий следующую задачу:$$\sum_{i=1}^{\ell}L(y_{i},a_{N-1}(x_{i})+b(x_{i}))\to\min_{b}$$
Разложим функцию $L$ в каждом слагаемом в ряд Тейлора до второго члена с центром в ответе композиции $a_{N-1}(x_{i})$:
$$\sum_{i=1}^{\ell}L(y_{i},a_{N-1}(x_{i})+b(x_{i}))\approx \sum_{i=1}^{\ell}\left( L(y_{i},a_{N-1}(x_{i})) - s_{i}b(x_{i})+ \frac{1}{2}h_{i}b^{2}(x_{i}) \right) ,$$
, где через $h_{i}$ обозначены вторые производные по сдвигам:
$$h_{i}= \frac{\partial^{2}}{\partial z^{2}}L(y,z)\bigg|_{a_{N-1}(x_{i})}$$
Первое слагаемое не зависит от нового базового алгоритма, и поэтому его можно выкинуть. Получаем функционал 

$$\sum_{i=1}^{\ell}\left( -s_{i}b(x_{i})+ \frac{1}{2}h_{i}b^{2}(x_{i}) \right) \to\min_{b} \ \ \ \ (1.2)$$ ^961de2

Покажем, что он очень похож на среднеквадратичный. Преобразуем его:$$\sum_{i=1}^{\ell}(b(x_{i})-s_{i})^{2}=\sum_{i=1}^{\ell}(b^{2}(x_{i})-2s_{i}b(x_{i})+s_{i}^{2})=\{ \text{последнее слагаемое не зависит от }b \}=$$
$$=\sum_{i=1}^{\ell}(b^{2}(x_{i})-2s_{i}b(x_{i}))=2\sum_{i=1}^{\ell}\left( -s_{i}b(x_{i})+ \frac{1}{2}b^{2}(x_{i}) \right) \to\min_{b}$$
Видно, что последняя формула совпадает с точностью до константы, если положить $h_{i}=1$. Таким образом, в обычном градиентном бустинге мы используем аппроксимацию второго порядка при обучении очередного базового алгоритма, и при этом отбрасываем информацию о вторых производных.
### Регуляризация

Будем далее работать с функционалом ([[#^961de2|1.2]]). Он измеряет лишь ошибку композиции после добавления нового алгоритма, никак при этом не штрафуя за излишнюю сложность этого алгоритма. Ранее мы решали проблему переобучения путем ограничения глубины деревьев, но можно подойти к вопросу и более гибко. Мы выяснили, что дерево $b(x)$ можно описать формулой$$b(x)=\sum_{j=1}^{J}b_{j}[x \in R_{j}]$$
Его сложность зависит от двух показателей:
1. Число листьев $J$. Чем больше листьев имеет дерево, тем сложнее его разделяющая поверхность, тем больше у него параметров и тем выше риск переобучения.
2. Норма коэффициентов в листьях $\|b\|_{2}^{2}=\sum_{j=1}^{J}b_{j}^{2}$. Чем сильнее коэффициенты отличаются от нуля, тем сильнее данный базовый алгоритм будет влиять на итоговый ответ композиции.

Добавляя регуляризаторы, штрафующие за оба этих вида сложности, получаем следующую задачу:$$\sum_{i=1}^{\ell}\left( -s_{i}b(x_{i})+\frac{1}{2}h_{i}b^{2}(x_{i}) \right) + \gamma J+ \frac{\lambda}{2}\sum_{j=1}^{J}b_{j}^{2}\to\min_{b} $$
Если вспомнить, что дерево $b(x)$ даст одинаковые ответы на объектах, попадающих в один лист, то можно упростить функционал: $$\sum_{j=1}^{J}\left\{ \underbrace{\left( -\sum_{i\in R_{j}}s_{i} \right)}_{=-S_{j}}b_{j} + \frac{1}{2} \left( \lambda+ \underbrace{\sum_{i\in R_{j}}h_{i}}_{=H_{j}} \right)b_{j}^{2}+\gamma  \right\} \to\min_{b}$$
Каждое слагаемое здесь можно минимизировать по $b_{j}$ независимо. Заметим, что отдельное слагаемое представляет собой параболу относительно $b_{j}$, благодаря чему можно аналитически найти оптимальные коэффициенты в листьях: $$b_{j}= \frac{S_{j}}{H_{j}+\lambda}$$
Подставляя данное выражение обратно в функционал, получаем, что ошибка дерева с оптимальными коэффициентами в листьях вычисляется по формуле 

$$H(b)= -\frac{1}{2}\sum_{j=1}^{J} \frac{S_{j}^{2}}{H_{j}+\lambda}+\gamma J \ \ \ (1.3)$$

## Обучение решающего дерева

Мы получили функционал $H(b)$, который для заданной структуры дерева вычисляет минимальное значение ошибки ([[#^961de2|1.2]]), которую можно получить путем подбора коэффициентов в листьях. Заметим, что он прекрасно подходит на роль критерия информативности - с его помощью можно принимать решение, какое разбиение вершины является наилучшим. Значит, с его помощью мы можем строить дерево. Будем выбирать разбиение $[x_{j}<t]$ в вершине $R$ так, чтобы оно решало следующую задачу максимизации: $$Q=H(R)-H(R_{\ell})-H(R_{r})\to\max,$$
где информативность вычисляется по формуле $$H(R)= - \frac{1}{2} \left( \sum_{(h_{i},s_{i})\in R}s_{j} \right) \bigg/ \left( \sum_{(h_{i},s_{i})\in R}h_{j}+\lambda \right)+\gamma.  $$
## Заключение

Итак, градиентный бустинг в $\text{XGBoost}$ имеет ряд важных особенностей.
1. Базовый алгоритм приближает направление, посчитанное с учетом вторых производных функции потерь.
2. Функционал регуляризуется - добавляются штрафы за количество листьев и за норму коэффициентов.
3. При построении дерева используется критерий информативности, зависящий от оптимального вектора сдвига.
4. Критерий останова при обучении дерева также зависит от оптимального сдвига.