---
title: Бэггинг
created: 2025-10-04
tags:
  - ml
  - ensemble
links:
  - "[[Бутстрап]]"
---
Пусть имеется некоторый метод обучения $\mu(X)$. Построим на его основе метод $\tilde{\mu}(X)$, который генерирует случайную подвыборку $\tilde{X}$ с помощью [[Бутстрап|бустрапа]] и подает на её вход метода $\mu:\tilde{\mu}(X)=\mu(\tilde{X})$. Напомним, что бутстрап представляет собой сэмплирование $\ell$ объектов из выборки с возвращением, в результате чего некоторые объекты выбираются несколько раз, а некоторые ни разу. Помещение нескольких копий одного объекта в бутстрапированную выборку соответствует выставлению веса при данном объекте - соответствующее ему слагаемое несколько раз войдёт в функционал, и поэтому штраф за ошибку на нём будет больше.

**Определение:** В *бэггинге* предлагается обучить некоторое число алгоритмов $b_{n}(x)$ с помощью метода $\tilde{\mu}$, и построить итоговую композицию как среднее данных базовых алгоритмов:
$$a_{N}(x)= \frac{1}{N}\sum_{n=1}^{N}b_{n}(x)=\frac{1}{N}\sum_{n=1}^{N}\tilde{\mu}(X)(x).$$
Заметим, что в методе обучения для бэггинна появляется ещё один источник случайности - взятие подвыборки. Чтобы функционал качества $L(\mu)$ был детерминированным, мы будем считать далее, что матожидание $\mathbb{E}_{X}[\cdot]$ берётся не только по всем обучающим выборкам $X$, но ещё и по всем возможным подвыборкам $\tilde{X}$, получаемым с помощью бутстрапа. Это вполне логичное обобщение, поскольку данное матожидание вводится в функционал для учёта случайностей с процедурой обучения модели.

Найдем смещение из разложения ([[Bias-Variance decomposition#^8d4dbd|2.4]]) для бэггинга:
$$\mathbb{E}_{x,y}\bigg[  ( \mathbb{E}_{X}\bigg[  \frac{1}{N}\sum_{n=1}^{N}\tilde{\mu}(X)(x) \bigg]- \mathbb{E}[y\ | \ x] )^{2}\bigg]=$$
$$\mathbb{E}_{x,y}\bigg[  \bigg( \frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{X}[\tilde{\mu}(X)(x)]-\mathbb{E}[y\ | \ x] \bigg )^{2} \bigg ]=$$
$$=\mathbb{E}_{x,y}\bigg[  \bigg( \mathbb{E}_{X}[\tilde{\mu}(X)(x)]-\mathbb{E}[y\ | \ x] \bigg )^{2} \bigg ]$$
Мы получили, что смещение композиции, полученной с помощью бэггинна, совпадает со смещением одного базового алгоритма. Таким образом, бэггинг не ухудшает смещенность модели.
Теперь перейдём к разбросу. Запишем выражение для дисперсии композиции, обученной с помощью бэггинга:
$$\mathbb{E}_{x,y}\left[ \mathbb{E}_{X}\left[ \left(\frac{1}{N}\sum_{n=1}^{N}\tilde{\mu}(X)(x) - \mathbb{E}_{X}\left[ \frac{1}{N}\sum_{n=1}^{N}\tilde{\mu}(X)(x) \right] \right)^{2}\right] \right].$$
Рассмотрим выражение, стоящее под матожиданиями:
$$\mathbb{E}_{x,y}\left[ \mathbb{E}_{X}\left[ \frac{1}{N^{2}}\sum_{n=1}^{N}(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])^{2} +\frac{1}{N^{2}}\sum_{n_{1}\neq n_{2}}(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\mu(X)(x)]) \right] \right]=$$
$$=\frac{1}{N^{2}}\mathbb{E}_{x,y}\left[ \mathbb{E}_{X}\left[ \sum_{n=1}^{N}(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])^{2} \right] \right]+$$
$$+ \frac{1}{N^{2}}\mathbb{E}_{x,y}\left[ \mathbb{E}_{X}\left[ \sum_{n_{1}\neq n_{2}}(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])\times(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)]) \right] \right]=$$
$$= \frac{1}{N}\mathbb{E}_{x,y}[\mathbb{E}_{X}[(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])]]+$$
$$ +\frac{N(N-1)}{N^{2}}\mathbb{E}_{x,y}[\mathbb{E}_{X}[(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])\times(\tilde{\mu}(X)(x)-\mathbb{E}_{X}[\tilde{\mu}(X)(x)])]]$$
Первое слагаемое - дисперсия одного базового алгоритма, деленная на длину композиции $N$. Второе - ковариация между двумя базовыми алгоритмами. Мы видим, что если базовые алгоритмы некоррелированы, то дисперсия композиции в $N$ раз меньше дисперсии отдельных алгоритмов. Если же корреляция имеет место, то уменьшение дисперсии может быть гораздо менее существенным.
Content

> [!example]
> Content

>[!axiom]
>