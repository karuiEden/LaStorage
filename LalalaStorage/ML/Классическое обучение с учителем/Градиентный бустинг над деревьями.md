---
title: Градиентный бустинг над деревьями
created: 2025-10-04
tags:
  - ml
  - decision_tree
links:
  - "[[Градиентный бустинг]]"
  - "[[Решающие деревья]]"
---
Считается, что градиентный бустинг над решающими деревьями - один из самых универсальных и сильных методов машинного обучения, известных на сегодняшний день. 

Вспомним, что решающее дерево разбивает всё пространство на неперессекающиеся области, в каждой из которых его ответ равен константе:
$$b_{n}(x)=\sum_{j=1}^{J_{n}}b_{nj}[x \in R_{j}],$$
где $j=1,\dots,J_{n}$ - индексы листьев, $R_{j}$ - соответствующие области разбиения, $b_{nj}$ - значения в листьях. Значит, на $N$-й итерации бустинга композиция обновляется как $$a_{N}=a_{N-1}(x)+\gamma_{N}\sum_{j=1}^{J_{n}}b_{Nj}[x\in R_{j}]=a_{N-1}(x)+\sum_{j=1}^{J_{n}}\gamma_{N} b_{Nj}[x\in R_{j}].$$
Видно, что добавление в композицию одного дерева с $J_{N}$ листьями равносильно добавлению $J_{N}$ базовых алгоритмов, представляющих собой предикаты вида $[x_{i}\in R_{j}]$. Если бы вместо общего коэффициента $\gamma_{N}$ был свой коэффициент $\gamma_{Nj}$ при каждом предикате, то мы могли бы его подобрать так, чтобы повысить качество композиции. Если подбирать свой коэффициент $\gamma_{Nj}$ при каждом слагаемом, то потребность в $b_{Nj}$ отпадает, его можно просто убрать:
$$\sum_{i=1}^{\ell}\left( y_{i},a_{N-1}(x_{i}) + \sum_{j=1}^{J_{N}}\gamma_{Nj}[x \in R_{j}] \right) \to\min_{\{ \gamma_{Nj} \}_{j=1}^{J_{N}}}.$$
Поскольку области разбиения $R_{j}$ не пересекаются, данная задача распадается на $J_{N}$ независимых подзадач:
$$\gamma_{Nj}=\arg_{\gamma}\min\sum_{x_{i}\in R_{j}}L(y_{i},a_{N-1}(x_{i})+\gamma), \ \ j=1,\dots,J_{N}.$$
В некоторых случаях оптимальные коэффициенты могут быть найдены аналитически - например, для квадратичной и абсолютной ошибки.
Рассмотрим теперь логистическую функцию потерь. В этом случае нужно решить задачу $$F_{j}^{(N)}(\gamma)=\sum_{x_{i}\in R_{j}}\log(1+\exp(-y_{i}(a_{N-1}(x_{i})+\gamma)))\to\min_{\gamma}.$$
Данная задача может быть решена лишь с помощью итерационных методов, аналитической записи для оптимальной $\gamma$ не существует. Однако на практике обычно нет необходимости искать точное решение - оказывается достаточным сделать лишь один шаг метода Ньютона-Рафсона из начального приближения $\gamma_{Nj}=0$. Можно показать, что в этом случае $$\gamma_{Nj}= \frac{\partial F_{j}^{(N)}(0)}{\partial\gamma}\bigg/\frac{\partial^{2}F_{j}^{(N)}(0)}{\partial\gamma^{2}}=-\sum_{x_{i}\in R_{j}}s_{i}^{(N)}\bigg/\sum_{x_{i}\in R_{j}}|s_{i}^{(N)}|(1-|s_{i}^{(N)}|).$$
### Смещение и разброс

В случайных лесах используются глубокие деревья, поскольку от базовых алгоритмов требуется низкое смещение; разброс же устраняется за счёт  усреднения ответов различных деревьев. Бустинг работает несколько иначе - в нём каждый следующий алгоритм целенаправленно понижает ошибку композиции, и даже при использовании простейших базовых моделей композиция может оказаться достаточно сложной. Более того, итоговая композиция вполне может оказаться переобученной при большом количестве базовых моделей. Это означает, что благодаря бустингу может понизить смещение моделей, а разброс останется таким же, либо увеличится. Из-за этого, как правило, в бустинге используются неглубокие решающие деревья (3-6 уровней), которые обладают большим смещением, но не склонны к переобучению.