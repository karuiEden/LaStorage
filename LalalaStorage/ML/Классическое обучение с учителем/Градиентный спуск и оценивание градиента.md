---
title: Градиентный спуск и оценивание градиента
created: 2025-08-05
tags:
  - ml
  - linear_models
  - optimization
links:
  - "[[Инвариантность 1-ого дифференциала. Производная по направлению. Градиент. Уравнения поверхности.]]"
---
**Определение:** *Градиентный спуск* - итеративный метод оптимизации, который заключается в использовании антиградиента.

Пусть $w^{(0)}$ - начальный набор параметров.
Тогда градиентный спуск состоит в повторении следующих шагов до сходимости:
$$w^{(k)}= w^{(k-1)}-\eta_{k}\nabla Q(w^{(k-1)})$$
, где $Q(w^{(k-1)})$ - функционал ошибки для $w$, $\eta_{k}$ - длина шага, которая может быть константой, но и может считаться, например, по формуле:$$\eta_{k} = \frac{1}{k}$$
Останавливать итерационный процесс можно, например, при близости градиента к нулю $\|\nabla Q(w^{(k-1)})\| <\varepsilon$ или при слишком малом изменении вектора весов на последней итерации $\|w^{(k)}-w^{(k-1)}\|<\varepsilon$. Также можно следить за ошибкой модели на отложенной выборке и останавливаться, если эта ошибка перестала убывать.

Существует большое количество условий сходимости градиентного спуска. Одно из них: если функция выпуклая и дифференцируемая, для её первой производной выполнено условие Липшица, длина шага выбрана правильно, то градиентный спуск сойдётся к минимуму функции. Но теоретически обоснованную длину шага использовать сложно, липшицеву константу не всегда легко найти, но и с ней сходимость может быть медленной. Проще выбрать длину шага, исходя из качества получаемой модели на отложенной выборке. Также есть следующая оценка сходимости для градиентного спуска:
$$Q(w^{(k)})-Q(w^{*}) = O\left( \frac{1}{k} \right)$$
Также можно использовать градиентный спуск для невыпуклых функционалов, но он не всегда будет работать, так как можно попасть в плохой локальный минимум или в седловую точку функционала
## Оценивание градиента

Как правило, в задачах машинного обучения функционал $Q(w)$ представлен в виде суммы $\ell$ функций:
$$Q(w) = \frac{1}{\ell}\sum_{i=1}^{\ell}q_{i}(w)$$
Проблема метода градиентного спуска состоит в том, что на каждом шаге необходимо вычислять градиент всей суммы, т.е. полный градиент:
$$\nabla_{w}Q(w)=\frac{1}{\ell}\sum_{i=1}^\ell \nabla_{w}q_{i}(w)$$
Это может быть очень трудоёмко при больших размерах выборки. В то же время точное вычисление градиента не очень сильно нужно, ибо мы делаем не очень большие шаги в сторону антиградиента и наличие в нём неточностей не должно сказаться на общей траектории. Опишем несколько способов оценивания полного градиента.
### Стохастический градиентный спуск

Оценить градиент суммы функций можно градиентом одного случайно взятого слагаемого:
$$\nabla_{w}Q(w)\approx \nabla_{w}q_{i_{k}}(w)$$
, где $i_{k}$ - случайно выбранный номер слагаемого из функционала. Таким образом, мы получим метод **стохастического градиентного спуска**:$$w^{(k)}=w^{(k-1)}-\eta_{k}\nabla _{w}q_{i_{k}}(w^{(k-1)})$$

Главное отличие от обыкновенного градиентного спуска в том, что у обычного градиентного спуска: чем ближе текущая точка к минимуму, тем меньше в ней градиент, за счёт чего сходимость замедляется, но при этом сходится. У стохастического градиентного спуска такого нет, так как полный градиент может сильно отличаться от одного из слагаемых, поэтому точка может отдаляться от минимума.

Для решения этой проблемы, нужно, чтобы длина шага была убывающей. Для этого нужно подбирать формулу, чтобы шаг не стал слишком малым слишком рано или слишком поздно.

В частности, сходимость для выпуклых дифференцируемых функций гарантируется, если функционал удовлетворяет ряду условий и длина шага удовлетворяет условиям Роббинса-Монро:
$$\sum_{k=1}^{\infty}\eta_{k}=\infty;\ \ \ \ \ \sum_{k=1}^{\infty}\eta_{k}^{2}<\infty$$
Таким условиям, например. удовлетворяет $\eta_{k}= \frac{1}{k}$ 

Для выпуклого и гладкого функционала может быть получена следующая оценка:
$$\mathbb{E}[Q(w^{(k)})-Q(w^{*})]=O\left( \frac{1}{\sqrt{ k }} \right)$$
Таким образом, метод стохастического градиента имеет менее трудоёмкие итерации по сравнению с полным градиентом, но скорость сходимости существенно ниже.
Также при помощи метода стохастического градиентного спуска можно экономить память, ибо мы считаем градиент по одному слагаемому.
Можно повысить точность оценки градиента, используя несколько слагаемых вместо одного:
$$\nabla_{w}Q(w)\approx \frac{1}{n}\sum_{j=1}^{n}\nabla_{w}q_{i_{k_{j}}}(w)$$, где $i_{k_{j}}$ - случайно выбранные номера слагаемых из функционала, а $n$ - параметр метода, размер "пачки" объектов для одного градиентного шага. С такой оценкой получаем метод mini-batch gradient descent, который используется для обучения дифференцируемых моделей.

### Метод SAG

Также существует метод *среднего стохастического градиента*, который сочетает низкую сложность итераций стохастического итерационного градиентного спуска и высокую скорость сходимости полного градиентного спуска.
**Принцип работы:** Выбирается первое приближение $w^{0}$, и инициализируются вспомогательные переменные $z_{i}^{0}$, соответствующие градиентам слагаемых функционала:
$$z_{i}^{(0)}=\nabla q_{i}(w^{(0)}), \ i=1,\dots,\ell$$
На $k$-ой итерации выбирается случайное число $i_{k}$ и обновляются вспомогательные переменные:
$$z_{i}^{(k)}=\begin{cases}
\nabla q_{i}(w^{(k-1)}), \ \text{если }i=i_{k}; \\
z_{i}^{(k-1)}, \text{ иначе.}
\end{cases}$$
Оценка градиента вычисляется как среднее вспомогательных переменных - т.е. мы используем все слагаемые, как в полном градиенте, но при этом все слагаемые берутся с предыдущих шагов, а не пересчитываются:
$$\nabla_{w}Q(w)\approx \frac{1}{\ell}\sum_{i=1}^{\ell}z_{i}^{(k)}$$
Градиентный шаг:
$$w^{(k)}=w^{(k-1)}-\eta_{k} \frac{1}{\ell}\sum_{i=1}^{\ell}z_{i}^{(k)}$$
Порядок сходимости для выпуклых и гладких функционалов метод имеет такой же, что и обычный градиентный спуск:
$$\mathbb{E}[Q(w^{(k)})-Q(w^{*})]=O(1/k).$$

Заметим, что для SAG требуется хранение последних вычисленных градиентов для всех объектов выборки. В некоторых случаях это можно избежать. Например, в случае с линейными моделями функционал ошибки можно представить в виде:
$$Q(w)=\frac{1}{\ell}\sum_{i=1}^{\ell}q_{i}(\langle w,x_{i}\rangle).$$
Градиент $i$-ого слагаемого выглядит так:
$$\nabla_{w}q_{i}'(\langle w,x_{i} \rangle)x_{i}.$$
Значит, будет достаточно хранить для каждого объекта лишь число $q_{i}'(\langle w,x_{i}\rangle)$.
Для уменьшения вычисления можно инициализировать $z_{i}^{(0)}$ нулями, а не градиентами отдельных слагаемых из функционала ошибки. В этом случае нужно делить сумму $z_{i}^{(k)}$ на число объектов, чей градиент уже вычислялся хотя бы раз.
### Иные подходы

Существуют много способов получения оценки градиента. Один из них позволяет получить оценку без вычисления градиентов. Для этого необходимо взять случайный вектор $u$ на единичной сфере и домножить его на значение функции в данном направлении:
$$\nabla_{w}Q(w)=Q(w+\delta u)u.$$
Можно показать, что данная оценка является несмещённой для сглаженной версии функционала.

В задаче оценивания градиента можно пойти ещё дальше, если вычислять градиенты $\nabla_{w}q_{i}(w)$ сложно, то можно обучить модель, которая будет выдавать оценку градиента на основе текущих значений параметров. Данный способ был предложен для обучения глубинных нейронных сетей.

## Модификации градиентного спуска

С помощью оценок градиента можно уменьшать сложность одного шага градиентного спуска, но при этом сама идея метода не меняется - мы движемся в сторону наискорейшего убывания функционала. Конечно, такой подход не идеален, и можно по-разному его улучшать, устраняя различные его проблемы. Разберём 2 примера таких модификаций - одна будет направлена на борьбу с осцилляциями, а вторая позволит автоматически подбирать длину шага:
1. [[Momentum(метод инерции)]]
2. [[AdaGrad и RMSprop]]
Если объединить идеи методов выше, то получим метод под названием **Adam**.