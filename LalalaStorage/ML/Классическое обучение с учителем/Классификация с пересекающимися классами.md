---
title: Классификация с пересекающимися классами
created: 2025-09-16
tags:
  - ml
links:
  - "[[Многоклассовая классификация]]"
  - "[[Линейная классификация]]"
---
Будем считать, что в задаче $K$ классов, но теперь они могут *пересекаться* - каждый объект может относиться к нескольким классам. Это означает, что каждому объекту $x$ соответствует вектор $y\in \{ 0,1 \}^{K}$, показывающий к каким классам относится данный объект. Соответственно, обучающей выборке $X$ будет соответствовать матрица $Y\in \{ 0,1 \}^{\ell \times K}$, описывающая метки объектов. Элемент $y_{ik}$ будет показывать, относится ли объект $x_{i}$ к $k$-ому классу. На английском эта классификация называется *multi-label classification*.

## Независимая классификация (binary relevance)

Подход заключается в том, что мы предполагаем, что все классы независимы, и определять принадлежность к каждому классу при помощи отдельных классификаторов. Это означает, что мы обучаем $K$ разных классификаторов $a_{1}(x),\dots,a_{k}(x)$, при чём классификатор $a_{k}(x)$ обучается по выборке $(x_{i},y_{ik})_{i=1}^{\ell}$. Для нового объекта $x$ целевая переменная оценивается как $(a_{1}(x),\dots,a_{K}(x))$. 
Основная проблема данного подхода состоит в том, что никак не учитываются возможные связи между отдельными классами.

## Стекинг классификаторов

Для учёта корреляций между классами можно воспользоваться следующим методом: Разобьем обучающую выборку $X$ на $X_{1}$ и $X_{2}$. На $X_{1}$ обучим $K$ независимых классификаторов $b_{1}(x),\dots ,b_{K}(x)$. Далее сформируем $\forall x_{i}\in X_{2}$ признаковое описание, состоящее из прогнозов классификаторов:
$$x_{ik}'=b_{k}(x), \ \ x_{i}\in X_{2}$$
, получив тем самым выборку $X_{2}'$. Обучим на ней набор классификаторов $a_{1}(x'),\dots,a_{K}(x')$, каждый из которых определяет принадлежность объекта к одному из классов. При этом все новые классификаторы опираются на прогнозы классификаторов из первого этапа $b_{1}(x),\dots,b_{k}(x)$, и поэтому могут обнаружить связи между различными классами. Такой подход называется *стекингом* и достаточно часто используется в машинном обучении для усиления моделей.
Разъясним, почему не стоит обучать оба типа классификаторов на одной и той же выборке. Прогнозы классификаторов $b_{1}(x),\dots,b_{K}(x)$ содержат в себе информацию об обучающей выборке $X_{1}$, получается, что новые признаки $x_{ik}'=b_{k}(x)$, посчитанные на этой выборке, по сути будут "подглядывать" в целевую переменную, и обучение на них приведет к переобучению.
Посмотрим на проблему иначе. Допустим, мы обучили классификаторы $b_{1}(x),\dots,b_{K}(x)$ на выборке $X_{1}$, как и классификатор $a(x))$, использующий в качестве единственного признака результат работы $b(x)$. Если модель $b(x)$ не переобучилась и будет показывать на новых данных такое же качество, то проблем не будет. Но обычно модели немного переобучаются. Пусть на обучении модель $b(x)$ имеет отклонение $5\%$,а на новых данных - $10\%$ из-за переобучения. Тогда модель $a(x)$ будет иметь отклонение $5\%$ на обучении, а на новых данных ситуация изменится, ведь, фактически, изменится и распределение её признака, что приведет к не очень хорошим результатам.

## Трансформация пространства ответов
Существуют подходы, которые пытаются учитывать взаимосвязь между классами в одной модели. Например, данный подход *Tai, Farbound and Lin, Hsuan-Tien "Multilabel Classification with Principal Label Space Transformation"*  предлагает преобразовать пространство ответов так, чтобы классы оказались как можно менее зависимыми. Это можно сделать с помощью [[Сингулярное разложение|сингулярного разложения]] матрицы $Y$:
$$Y=U\Sigma V^{T}$$
Известно, что если в этом разложении занулить все диагональные элементы матрицы $\Sigma$ кроме $m$ наибольших, то мы получим матрицу, наиболее близкую к $Y$ с точки зрения нормы Фробениуса среди всех матриц ранга $m$. 
Обозначим через $V_{M}$ матрицу, состоящих из тех столбцов матрицы $V$, которые соответствуют наибольшим сингулярным числам. Спроецируем её на матрицу $Y$:
$$YV_{M}=Y'\in \mathbb{R}^{\ell \times m}$$
Настроим на новые метки $Y'$ классификаторы $a_{1}(x),\dots ,a_{M}(x)$. Обозначим матрицу прогнозов для нашей выборки через $A'\in \mathbb{R}^{\ell \times m}$. Чтобы получить оценки принадлежности к исходным классам, переведём матрицу $A'$ в исходное пространство:
$$A=A'V^{T}_{M}$$ 
## Метрики качества классификации с пересекающимися классами

Обозначим через $Y_{i}$ множество классов, которым объект $x_{i}$ принадлежит на самом деле, а через $Z_{i}$ - множество классов, к которым объект $x_{i}$ был отнесён алгоритмом $a(x)$.
Вполне логичной мерой ошибки будет хэммингово расстояние между этими множествами - т.е. доля классов, факт принадлежности которых угадан неверно:
$$\text{hamming}(a,X)= \frac{1}{\ell} \frac{|Y_{i}\setminus Z_{i}|+|Z_{i}\setminus Y_{i}|}{K}$$
Данную метрику надо минимизировать.
Стандартные метрики также можно использовать для multi-label классификации, как при помощи микро- и макро-усреднения, так и иным подходом:
$$\text{accuracy}(a,X)= \frac{1}{\ell }\sum_{i=1}^{\ell} \frac{|Y_{i}\cap Z_{i}|}{|Y_{i}\cup Z_{i}|}$$
$$\text{precision}(a,X)=\frac{1}{\ell} \sum_{i=1}^{\ell} \frac{|Y_{i}\cap Z_{i}|}{|Z_{i}|}$$
$$\text{recall}(a,X)= \frac{1}{\ell}\sum_{i=1}^{\ell} \frac{|Y_{i}\cup Z_{i}|}{|Z_{i}|}$$
