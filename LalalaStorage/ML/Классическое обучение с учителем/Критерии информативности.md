---
title: Критерии информативности
created: 2025-09-30
tags:
  - decision_tree
links:
  - "[[Решающее деревья]]"
---
При построении дерева необходимо задать *функционал качества*, на основе которого осуществляется разбиение выборки на каждом шаге. Обозначим через $R_{m}$ множество объектов, попавших в вершину, разбиваемую на данном шаге, а через $R_{\ell}$ и $R_{r}$ - объекты, попадающие в левое и правое поддерево соответственно при заданном предикате. Мы будем использовать предикат следующего вида:
$$Q(R_{m},j,s)=H(R_{m})- \frac{|R_{\ell}|}{|R_{m}|}H(R_{\ell})- \frac{|R_{r}|}{|R_{m}|}H(R_{r})$$
Здесь $H(R)$ - это *критерий информативности* (impurity criterion), который оценивает качество распределения целевой переменной среди объектов множества $R$. Чем меньше разнообразия целевой переменной, тем меньше должно быть значение критерия информативности - и, соответственно, мы будем минимизировать его значение. Функционал качества $Q(R_{m},j,s)$ мы будем максимизировать.

Так как в каждом листе дерево будет выдавать константу, то можно предложить оценивать качество множества $R$ тем, насколько хорошо их целевые переменные предсказываются константой: $$H(R)= \min_{c\in Y} \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}L(y_{i},c),$$
где $L(y,c)$ - некоторая функция потерь.

## Регрессия

Как обычно, в регрессии выберем квадрат отклонения в качестве функции потерь. В этом случае критерий информативности будет выглядеть как $$H(R)=\min_{c\in Y} \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}(y_{i} - c)^{2}$$
Как известно, минимум в этом выражении будет достигаться на среднем значении целевой переменной. Значит критерий можно переписать так $$H(R)= \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}\bigg(    y_{i} - \frac{1}{|R|} \sum_{(x_{j},y_{j})\in R} y_{j} \bigg )^{2}$$
Мы получили, что информативность вершины измеряется её дисперсией - чем она ниже, тем лучше вершина.
