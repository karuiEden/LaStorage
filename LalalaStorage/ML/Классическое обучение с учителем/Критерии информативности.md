---
title: Критерии информативности
created: 2025-09-30
tags:
  - decision_tree
  - ml
links:
  - "[[Решающие деревья]]"
---
При построении дерева необходимо задать *функционал качества*, на основе которого осуществляется разбиение выборки на каждом шаге. Обозначим через $R_{m}$ множество объектов, попавших в вершину, разбиваемую на данном шаге, а через $R_{\ell}$ и $R_{r}$ - объекты, попадающие в левое и правое поддерево соответственно при заданном предикате. Мы будем использовать предикат следующего вида:
$$Q(R_{m},j,s)=H(R_{m})- \frac{|R_{\ell}|}{|R_{m}|}H(R_{\ell})- \frac{|R_{r}|}{|R_{m}|}H(R_{r})$$
Здесь $H(R)$ - это *критерий информативности* (impurity criterion), который оценивает качество распределения целевой переменной среди объектов множества $R$. Чем меньше разнообразия целевой переменной, тем меньше должно быть значение критерия информативности - и, соответственно, мы будем минимизировать его значение. Функционал качества $Q(R_{m},j,s)$ мы будем максимизировать.

Так как в каждом листе дерево будет выдавать константу, то можно предложить оценивать качество множества $R$ тем, насколько хорошо их целевые переменные предсказываются константой: $$H(R)= \min_{c\in Y} \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}L(y_{i},c),$$
где $L(y,c)$ - некоторая функция потерь.

## Регрессия

Как обычно, в регрессии выберем квадрат отклонения в качестве функции потерь. В этом случае критерий информативности будет выглядеть как $$H(R)=\min_{c\in Y} \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}(y_{i} - c)^{2}$$
Как известно, минимум в этом выражении будет достигаться на среднем значении целевой переменной. Значит критерий можно переписать так $$H(R)= \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}\bigg(    y_{i} - \frac{1}{|R|} \sum_{(x_{j},y_{j})\in R} y_{j} \bigg )^{2}$$
Мы получили, что информативность вершины измеряется её дисперсией - чем она ниже, тем лучше вершина.
## Классификация

Обозначим через $p_{k}$ долю объектов класса $k \ (k\in \{ 1,\dots,K \})$, попавших в вершину $R$:
$$p_{k}= \frac{1}{|R|} \sum_{(x_{i},y_{i})\in R} [y_{i}=k]$$
Через $k_{*}$ обозначим класс, чьих представителей оказалось больше всего среди объектов, попавших в данную вершину $k_{*}=\arg_{k}\max p_{k}$
### Ошибка классификации

Рассмотрим индикатор ошибки как функцию потерь:
$$H(R)=\min_{c\in Y} \frac{1}{|R|} \sum_{(x_{i},y_{i})\in R}[y_{i}\neq c]$$
Легко видеть, что оптимальным предсказанием тут будет наиболее популярный класс $k_{*}$ - значит, критерий будет равен следующей доле ошибок: $$H(R)= \frac{1}{|R|} \sum_{(x_{i},y_{i})\in R}[y_{i}\neq k]=1-p_{k}$$
### Критерий Джини

Рассмотрим ситуацию, в которой мы выдаем в вершине не один класс, а распределение на всех классах $c=(c_{1},\dots,c_{K}),\sum_{k=1}^{K}c_{k}=1$. Качество такого распределения можно измерять, например, с помощью критерия Бриера:$$H(R)=\min_{\sum_{k}c_{k}=1} \frac{1}{|R|}\sum_{(x_{i},y_{i})\in R}\sum_{k=1}^{K}(c_{k}-[y_{i}=k])^{2}.$$
Можно показать, что оптимальный вектор вероятностей состоит из долей классов $p_{k}:$ $$c_{*}=(p_{1},\dots,p_{K})$$
Если подставить эти вероятности в исходный критерий информативности и провести ряд преобразований, то мы получим критерий Джини:
$$H(R)= \sum_{k=1}^{K}p_{k}(1-p_{k})$$
### Энтропийный критерий

Мы уже знакомы с более популярным способом оценивания качества вероятностей - логарифмическими потерями, или логарифмом правдободобия:
$$H(R)=\min_{\sum_{k}c_{k}=1}\left( - \frac{1}{|R|} \sum_{(x_{i},y_{i})\in R} \sum_{k=1}^{K} [y_{i}=k]\log c_{k} \right)$$
Для вывода оптимальных значений $c_{k}$ вспомним, что все значения $c_{k}$ должны суммироваться в единицу. Как известно из методов оптимизации, для учёта этого ограничения необходимо искать минимум лагранжиана: $$L(c,\lambda)= - \frac{1}{|R|}  \sum_{(x_{i},y_{i})\in R} \sum_{k=1}^{K}[y_{i}=k]\log c_{k}+\lambda \sum_{k=1}^{K}c_{k}\to \min_{c_{k}}$$
Дифференцируя, получаем:
$$\frac{\partial}{\partial c_{k}}L(c,\lambda)= - \frac{1}{|R|}  \sum_{(x_{i},y_{i})\in R} [y_{i}=k] \frac{1}{c_{k}}+\lambda= - \frac{p_{k}}{c_{k}}+\lambda=0,$$
откуда выражаем $c_{k}= \frac{p_{k}}{\lambda}$. Суммируя эти равенства по $k$, получим $$1=\sum_{k=1}^{K}c_{k}=\frac{1}{\lambda} \sum_{k=1}^{K}p_{k}= \frac{1}{\lambda}$$
откуда $\lambda=1$. Значит минимум достигается при $c_{k}=p_{k}$, как и в предыдущем случае. Подставляя эти выражения в критерий, получим, что он будет представлять собой энтропию распределения классов: $$H(R)= -\sum_{k=1}^{K}p_{k}\log p_{k}$$
Из теории вероятности известно, что энтропия ограничена снизу нулем, причем минимум достигается на вырожденных распределениях $(p_{i}=1,p_{j}=0 \ \ i\neq j)$. Максимум же достигается при равномерном распределении. Отсюда видно, что энтропийный критерий отдаст предпочтение более "вырожденным" распределениям классов в вершинах.