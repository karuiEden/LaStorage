---
title: Линейная классификация
created: 2025-08-08
tags:
  - ml
  - linear_models
links:
---
## Бинарная классификация

Пусть $\mathbb{X}=\mathbb{R}^{d}$, а $\mathbb{Y}=\{ -1,+1 \}$.
$X=\{ (x_{i},y_{i}) \}_{i=1}^{\ell}$ - обучающая выборка.
Класс "$+1$" - положительный, класс "$-1$" - отрицательный.
**Определение:** *Линейная модель бинарной классификации* определяется следующим образом:
$$\DeclareMathOperator{\sgn}{sign} a(x)=\sgn(\langle w,x\rangle +w_{0})=\sgn\left( \sum_{j=1}^{d}w_{j}x_{j}+w_{0} \right),$$
где $w\in \mathbb{R}^{d}$ - вектор весов, $w_{0}\in \mathbb{R}$ - сдвиг(bias).

**Замечание:** $\sgn(z)=0$ при $z=0$, а класс "$0$" отсутствует. Поэтому либо надеемся, что $0$ не выпадет, либо добавить "$0$" к одному из существующих классов.
Если не сказано иначе, мы будем считать, что среди признаков есть константа $x_{d+1}=1$. Тогда можем обойтись без сдвига $w_{0}$, а линейный классификатор будет иметь:$$a(x)=\sgn\langle w,x \rangle.$$Геометрически линейный классификатор соответствует гиперплоскости с вектором нормали $w$. Величина скалярного произведения $\langle w,x \rangle$ пропорциональна расстоянию до точки $x$, а его знак показывает, с какой стороны от гиперплоскости находится данная точка. Таким образом, линейный классификатор разделяет пространство на 2 части с помощью гиперплоскости, и при этом одно полупространство относит к положительному классу, а другое к отрицательному.
## Обучение линейных классификаторов

В отличии от задач регрессии, где возможных ответов бесконечное множество, задачи классификации имеют ограниченные количество вариантов ответа, поэтому количество функционалов ошибки намного меньше в данных задачах.

**Определение:** В задачах классификации соответствующий функционал называется *долей правильных ответов* (accuracy):
$$Q(a,X)= \frac{1}{\ell}\sum_{i=1}^{\ell}[a(x_{i})=y_{i}].$$
Нам будет удобнее решать задачу минимизации, поэтому вместо этого будем использовать долю неправильных ответов:

$$Q(a,X)=\frac{1}{\ell}\sum_{i=1}^{\ell}[a(x_{i})\neq y_{i}]=\frac{1}{\ell}\sum_{i=1}^{\ell}[\sgn\langle w,x_{i} \rangle\neq y_{i}]\to \min_{w}\ \ \ (1.1)$$ ^eef5d2

Данный функционал является дискретным относительно весов, поэтому мы не сможем его обучить с помощью градиентных спусков, а также у такого функционала может быть несколько глобальных минимумов. Поэтому попытаемся свести задачу к минимизации гладкого функционала.
### Отступы

$$Q(a,X)=\frac{1}{\ell}\sum_{i=1}^{\ell}[\underbrace{y_{i}\langle w,x_{i} \rangle}_{M_{i}}<0]\to \min_{w}$$
**Определение:** Отступ - величина $M_{i}=y_{i}\langle w,x_{i} \rangle$. Знак отступа говорит о корректности ответа(положительный отступ соответствует правильному ответу, отрицательный - неправильному). Его абсолютная величина характеризует уверенность классификатора в предсказании.

Так как скалярное произведение $\langle w,x_{i} \rangle$ пропорционально расстоянию объекта до гиперплоскости, то чем меньше отступ к нулю, тем ниже уверенность в его принадлежности.
### Верхние оценки

Функционал ([[#^eef5d2|1.1]]) оценивает ошибку алгоритма на объекте $x$ с помощью пороговой функции потерь $L(M)=[M<0]$, где аргументом функции является отступ $M=y\langle w,x \rangle$. Оценим эту функцию сверху во всех точках $M$ кроме, может быть, небольшой полуокрестности левее нуля:

$$L(M)\leq \tilde{L}(M).$$

![[Pasted image 20250810121543.png|Верхние оценки на пороговую функцию потерь.]] 

После этого можно получить верхнюю оценку на функционал ([[#^eef5d2|1.1]]):
$$Q(a,X)\leq \frac{1}{\ell}\sum_{i=1}^{\ell}\tilde{L}(y_{i}\langle w,x_{i} \rangle)\to \min_{w}$$
Если верхняя оценка $\tilde{L}(M)$ является гладкой, то и данная верхняя оценка является гладкой. Тогда её можно будет минимизировать при помощи градиентного спуска и если верхнюю оценку удастся приблизить к нулю, то и доля неправильных ответов тоже будет близка к нулю.
Примеры верхних оценок:
1. $\tilde{L}(M)=\log(1+e^{ -M })$ - логистическая функция потерь
2. $\tilde{L}(M)=(1-M)_{+}=\max(0,1-M)$ - кусочно-линейная функция потерь(используется в методе опорной векторов)
3. $\tilde{L}(M)=(-M)_{+}=\max(0,-M)$ - кусочно-линейная функция потерь (соответствует персептрону Розенблатта)
4. $\tilde{L}(M)=e^{ -M }$ - экспоненциальная функция потерь
5. $\tilde{L}(M)= \frac{1}{(1+e^{ M })}$ - сигмоидная функция потерь
Все они подойдут для обучения линейных классификаторов.
