| **Линейная модель** - семейство вида линейных функций вида:
$$y=w_{1}x_{1}+w_{2}x_{2}+\dots+w_{D}x_{D}+w_{0}$$
, где $y$ - целевая переменная(**таргет**), $(x_{1},x_{2},\dots,x_{D})$ - вектор, соответствующий объекту выборки(**вектор признаков**), а $w_{1},w_{2},\dots,w_{D},w_{0}$ -  параметры модели. Признаки также называют **фичами**. Вектор $w=(w_{1},w_{2},\dots,w_{D})$ называют вектором параметров, а $w_{0}$ - **сдвигом(bias)**. Вот так линейная модель выглядит в компактном виде:
$$y=\langle x,w\rangle + w_{0}$$
 Задачей при использовании линейной модели состоит в поиске вектора: $(w_{0},w_{1},\dots, w_{D}) \in \mathbb{R}^{D+1}$

Чтобы использовать линейную модель, необходимо, чтобы каждый объект был представлен в виде вектора численных признаков $x_{1},x_{2},\dots x_{D}$. Поэтому модель называют линейной, если она является линейной по этим численным признакам.

![[Pasted image 20250130132939.png]]

Преимущества линейных моделей:
- Простота
- Высокая интерпретируемость(Интерпретируемость - качество моделей, которое показывает степень интерпретации поведения модели)
Недостатки линейных моделей:
- Линейная модель является узким классом функций, поэтому при решении сложных задач стоил прибегнуть к **feature engineering**, чтобы создать дополнительные признаки.
- Если между признаками есть приближённая линейная зависимость, то все коэффициенты могут потерять физический смысл
- Особенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна.
- Значения весов будет меняться от обучающего датасета, при этом при увеличении его размера будет начинаться переобучение.

Темы:
1. [[Линейная регрессия]]