---
tags:
  - ml
  - regression
  - linear_models
---

### Линейные модели
Линейная модель - семейство алгоритмов(моделей) имеющих вид:
$$a(x)=w_{0}+w_{1}x_{1}+\dots w_{d}x_{d}=w_{0}+\sum_{j=1}^{d}w_{j}x_{j}$$
Параметрами модели являются **веса** или **коэффициенты** $w_{j}$. Вес $w_{0}$ также называется свободным коэффициентом или сдвигом(**bias**). В целом сумма является скалярным произведением векторов: вектора признаков $x$ и вектора весов $w$:
$$a(x)=w_{0}+\langle w,x\rangle$$ Также применяют приём, что в каждый вектор признаков добавляют $(d+1)$ - признак, равный единице, а сдвиг добавляют в вектор весов, следовательно, модель будет иметь вид:
$$a(x)=\langle w,x\rangle$$
Преимуществом линейных моделей заключается в том, что они достаточно простые и имеют не так много параметров, что позволяет хорошо интерпретировать результаты модели. Также легко обучать такие модели.

### Области применения
Для линейной модели необходимо, чтобы между признаками и таргетом была линейная зависимость и таргет не зависит от какой-либо комбинации признаков. Также линейная модель принимает лишь численные данные.
#### Категориальные признаки
Категориальные признаки имеют недостатки, так как они не являются счётным, их нельзя сравнивать и складывать. Пусть категориальный признак $f_{j}(x)$ принимает значения из множества $C=\{ c_{1},c_{2},\dots,c_{m} \}$. Тогда можем использовать один из видов кодирования категориальных признаков в бинарные и с помощью него заменим категориальный признак на $m$ бинарных признаков $b_{i}$:
$$b_{i}=[f_{j}(x)=c_{i}]$$
Такой способ называется *one-hot*. Но при таком приёме признаки $b_{1}(x),b_{2}(x),\dots,b_{m}(x)$ являются линейно зависимыми:
$$b_{1}(x)+b_{2}(x)+\dots+b_{m}(x)=1$$
Чтобы избежать этого, можно выбрасывать один из бинарных признаков. Впрочем, такое решение имеет и недостатки — например, если на тестовой выборке появится новая категория, то её как раз можно закодировать с помощью нулевых бинарных признаков; при удалении одного из них это потеряет смысл.
После *one-hot* кодирования модель будет выглядеть:
$$a(x) = w_{1}[f(x)=c_{1}]+w_{2}[f(x)=c_{2}]+\dots+w_{m}[f(x)=c_{m}]+\{ \text{взаимодействие с другими признаками} \}$$
#### Работа с текстами

При работе с текстами также есть метод кодирования *мешок слов(bag of words)*. Найдём все слова в текстах и пронумеруем их: $\{ c_{1},c_{2},\dots c_{m} \}$. Будем кодировать текст $m$ признаками$b_{1}(x),b_{2}(x),\dots b_{m}(x)$ равен количеству вхождений слова $c_{j}$ в текст. Тогда модель будет выглядеть так:
$$a(x)=w_{1}b_{1}(x)+w_{2}b_{2}(x)+\dots+w_{m}b_{m}(x)+\dots$$
#### Бинаризация численных признаков
Может произойти такая ситуация, что один из численных признаков будет иметь не линейную зависимость с таргетом, необходимо устранить эту проблему, один из способов это перекодировать численные признаки в бинарные. Для этого выберем некоторую сетку $\{ t_{1},t_{2},\dots t_{m} \}$ это может быть равномерная сетка между минимумом и максимумом признака, или, например, сетка из эмпирических квантилей.
Добавим сюда два значения: $t_{0}=-\infty$ и $t_{m+1}=+\infty$. Новые признаки обозначим:
$$b_{i}(x)=[t_{i-1}<x_{j}\leq t_{i}], \ i,=1,\dots,m,m+1$$
Тогда линейная модель будет выглядеть так:
$$a(x)=w_{1}[t_{0}<x_{j}\leq t_{1}] + \dots+w_{m}[t_{m}<x_{j}\leq t_{m+1}]+\dots$$
### Измерение ошибки в задачах регрессии
См. [[Метрики регрессии]] 

### Обучение модели
Часто линейная регрессия обучается с использованием *средне квадратичной* ошибки. Тогда наша задачи будет выглядеть так(сдвиг находится в векторе весов):
$$\frac{1}{\mathcal{\ell}}\sum_{i=1}^{n}(\langle w,x_{i}\rangle-y_{i})^{2}\to \min_{w}$$
Также эту задачу можно выразить в матричном виде:$$\frac{1}{\ell}\|Xw-y\|^{2}\to\min_{w}$$
, где $\|Xw-y\|^{2}$ является $L^{2}$-нормой.
После преобразований получим значение весов:
$$w=(X^{T}X)^{-1}X^{T}y$$
//TODO: Написать вывод после изучения матричной дифференцируемости
*Вывод:*

Хотя данный способ является точным, но он имеет несколько минусов:
- Обращение матрицы - сложная операция с кубической сложностью от количества признаков. Если признаков очень много, то вычисления могут стать слишком трудными. Поэтому используют численные методы оптимизации.
- Матрица $X^{T}X$ может быть вырожденной или плохо обусловленной. В этом случае либо обращение невозможно, либо его результат будет неустойчивым. Решается при помощи регуляризации.
И, в целом,  аналитические формулы являются редкостью для решения задач машинного обучения.
