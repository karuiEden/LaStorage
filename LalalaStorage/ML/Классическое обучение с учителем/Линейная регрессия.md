### Линейная регрессия и метод наименьших квадратов (МНК)
Простейшим примером постановки задачи линейной регрессии является **метод наименьших квадратов**.

Пусть у нас есть датасет $(X,y)$, где $y=(y_{i})_{i=1}^{N}\in \mathbb{R}^{N}$  - вектор значений целевой переменной, а $X=(x_{i})_{i=1}^{N}\in \mathbb{R}^{N\times D}, x_{i}\in \mathbb{R}^{D}$ - матрица объектов-признаков, в которая $i$-ая строка - это вектор признаков $i$-того объекта выборки. Мы хотим моделировать зависимость $y_{i}$ от $x_{i}$ как линейную функцию со свободным членом. Тогда линейная функций из $\mathbb{R}^{D}$ в $\mathbb{R}$ выглядит так:
$$f_{w}(x_{i}) = \langle x_{i},w\rangle + w_{0}$$
Но свободный член часто опускают, потому что такого же результата можно достичь добавив один признак к объекту тождественно равный 1, а свободный член добавить в вектор весов:
$$\begin{pmatrix}  
x_{i1} & \dots & x_{iD}
\end{pmatrix} \begin{pmatrix}
w_{1}  \\
\vdots \\
w_{D}
\end{pmatrix} \ \ +w_{0} = \begin{pmatrix}
1 & x_{i1} & \dots & x_{iD}
\end{pmatrix} \begin{pmatrix}
w_{0} \\
w_{1} \\
\vdots \\
w_{D}
\end{pmatrix}$$
Поэтому легче обозначать линейную функцию, как $f_{w}(x_{i}) = \langle w,x_{i}\rangle$

#### Сведение к оптимизации
Мы хотим, чтобы на датасете $(X,y)$ наша функция $f_{w}(x_{i})$ как можно лучше приближала зависимость.
![[Pasted image 20250130133344.png]]

То есть мы должны научиться измерять качество модели и улучшать её изменяя параметры(веса).
|  Функция, которая оценивает как часто модель ошибается называется **функцией потерь(loss-функцией)**.

Функции потерь бывают разные, и поэтому нужно выбрать ту, которая поможет нам подобрать наилучшие веса для функции. В качестве примера возьмём квадрат $L^{2}$-нормы вектора разницы предсказаний модели и $y$:
- $L^{2}$ - норма разницы - это евклидово расстояние $||y-f_{w}(x_{x})||_{2}$ между вектором таргетов и вектором предсказаний модели, то есть мы приближаем их в смысле обычного расстояния.
Наша функция потерь выглядит так:
$$L(f,X,y) = ||y-f_{w}(X)||_{2}^{2}=||y-wX||_{2}^{2}=\sum_{i=1}^{N}(y_{i} - \langle x_{i},w\rangle)^{2}$$
Такая функция не очень хороша, так как её значение нельзя интерпретировать для датасетов разных размеров. Поэтому можно использовать **среднеквадратичное отклонение** или же **Mean Square Error(MSE):
$$MSE(f,X,y) = \frac{1}{N}||y-wX||_{2}^{2}$$
| **Функционал** - отображение, которое принимает функцию и возвращает число.

По сути, функции потери являются функционалом, так как для каждой функции возвращают значение качества функции, поэтому, чтобы получить лучшую модель, надо минимизировать функционал по $w$:
$$||y-f(X)||_{2}^{2} \longrightarrow \min_{w}$$
Перейдём теперь к решению задач.
#### МНК: точный аналитический подход
Точку минимума можно находить по разному. Рассмотрим геометрический подход.
Пусть $x^{(1)},\dots,x^{(D)}$ - столбцы матрицы $X$, то есть столбцы признаков. Тогда:
$$Xw=w_{1}x^{(1)}+\dots+w_{D}x^{(D)}$$
и задачу регрессии можно сформулировать как поиск линейной комбинации столбцов матрицы $X$, которая наилучшим образом приближает столбец $y$ по евклидовой норме, то есть найти проекцию $y$ на подпространство, образованное $x^{(1)},\dots,x^{(D)}$ 

Разложим $y=y_{\|} + y_{\perp}$, где $y_{\|}=Xw$, а $y_{\perp}$ - ортогональная составляющая, то есть $y_{\perp}=y-Xw \perp x^{(1)},\dots,x^{(D)}$ В матричном виде это будет выглядеть так:
$$X^{T}(y-Xw)=0$$
$$w=(X^{T}X)^{-1}X^{T}y$$
Вычислительная сложность данного решения - $O(D^{2}N+D^{3})$, где $N$ - длина выборки, $D$ - количество признаков у одного объекта. Тогда $D^{2}N$ отвечает за перемножение матриц $X^{T}$ и $X$, а $D^{3}$ отвечает за обращение матрицы $X^{T}X$. Перемножать матрицы $(X^{T}X)^{-1}X^{T}$ не стоит, лучше перемножить $y$ и $X^{T}$, а затем уже перемножить вектор на $(X^{T}X)^{-1}$ .

##### Проблемы такого метода
1. Высокие затраты на вычисление, так как необходимо перемножать матрицы, что уже является затратным процессом.
2. Матрица $X^{T}X$ почти всегда обратима, но часто плохо обусловлена. Особенно если признаков много, то может появиться приближённая линейная засимость между ними. Поэтому в подобных случаях погрешность нахождения $w$ будет зависеть от квадрата числа обусловленности матрицы $X$, что очень плохо, так как малые возмущения $y$. Приводят к катастрофическим изменениям $w$.

#### МНК: приближенный численный метод
Минимизируемый функционал является выпуклым и гладким(то есть непрерывным и дифференцируемой), следовательно, можно искать точку минимума с помощью *итеративных градиентовых методов*.

Имея хоть какое-то приближение оптимального значения параметра $w$, мы можем его улучшить, посчитав градиент функции потерь и немного сдвинув вектор весов в направлении антиградиента:
$$w_{j} \mapsto w_{j}-\frac{\alpha (d)}{dw_{j}}L(f_{w},X,y)$$
где $\alpha$ -  параметр алгоритма (**темп обучения**), который контролирует величину шага в направлении антиградиента. Такой алгоритм называется **градиентный спуском**.
Градиентный спуск для среднеквадратичного отклонения будет выглядеть так:
$$\nabla_{w}L=\frac{2}{N}X^{T}(Xw-y)$$
##### Алгоритм градиентного спуска
```
w = random_normal()
repeat S times: //  можно заменить на abs(err) > tolerance
	f = X.dot(w)
	err = f - y
	grad = 2 * X.T.dot(err) / N
	w -= alpha * grad
```
То есть, мы находим предсказания, затем считаем ошибку, находим градиент и улучшаем веса.
Замечания:
- Поскольку задача выпуклая, то выбор начальной точки влияет на скорость сходимости, но не настолько, чтобы нельзя было брать 0 или другое
- Число обусловленности матрицы $X$ существенно влияет на скорость сходимости градиентного спуска.
- Темп обучения $\alpha$ влияет на обучение и в целом является гиперпараметром, как и `S` или `tolerance`.
Вычислительная сложность градиентного спуска равна $O(NDS)$
Сложность по памяти $O(ND)$
#### Стохастический градиентный спуск

На каждом шаге градиентного спуска нам требуется выполнять довольно дорогую операцию - посчитать градиент на всей выборке($O(ND)$), поэтому предлагается вместо градиента использовать оценку градиента на какой-то подвыборке, которая называется **батчем**.
А именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет:
$$L(f,X,y) = \frac{1}{N} \sum_{i=1}^{N}L(f,x_{i},y_{i})$$
То, градиент:
$$\nabla_{w}L(f,X,y)= \frac{1}{N} \sum_{i=1}^{N}\nabla _{w}L(f,x_{i},y_{i})$$
, то предлагается брать оценку на подвыборку:
$$\nabla_{w}L(f,X,y) \approx \frac{1}{B}\sum_{i=1}^{B}\nabla_{w}L(f,x_{i},y_{i})$$
для некоторого подмножества этих пар $(x_{i},y_{i})_{i=1}^{B}$. Необходимо обратить внимание на множители, которые стоят перед суммой, так как мы считаем, что полный градиент $L(f,X,y)$, равен среднему градиентов каждого объекта, следовательно, как оценку матожидания $\mathbb{E}\nabla_{w}L(f,x,y)$, тогда оценка матожидания на меньшей подвыборке будет также иметь вид среднего градиентов по объектам это подвыборки.

Выборку делим на батчи с помощью линейного прохода(перед которым можно перемешать всю выборку) по всей выборке для этого введём новый гиперпараметр $B$, который будет означать размер батча, также у нас теперь вместо $S$ попыток будет гиперпараметр $E$, который означает количество эпох. Одна эпоха это один полный проход сэмплера по нашей выборке. 
**Алгоритм:**
```
w = normal(0,1)
repeat E times:
	for i = B; i <= n, i += B:
		x_batch = X[i-B:i]
		y_batch = y[i-B:i]
		f = x_batch.dot(w)
		err = f - y_batch
		grad = 2 * x_batch.T.dot(err) / B
		w -= alpha * grad
```
Сложность по времени - $O(NDE)$.
Отличается от обычного градиентного спуска тем, что за одну эпоху мы сделали в $\frac{N}{B}$ действий больше.
Сложность по памяти - $O(BD)$

Визуальная разница между обычным градиентным спуском и стохастическим:
![[Pasted image 20250130175457.png]]

Хотя шаги стохастического градиентного спуска и более шумные, но при этом считаются они быстрее, но в конечном итоге веса приходят в оптимальное значение.

Замечание: помимо градиентных решений существуют и другие виды приближённых решений. Например, Stepwise regression, Orthogonal matching pursuit или LARS.

## Регуляризация

| **Мультиколлинеарность** - наличие линейной (приближённо линейной) зависимости между признаками.

| **Регуляризация** - метод добавления некоторых дополнительных ограничений с целью решить некорректно поставленную задачу или предотвратить переобучение.

Поэтому вместо исходной задачи, необходимо решить такую:
$$\min_{w}(L(f,X,y))=\min_{w}(\|Xw-y\|_{2}^{2}+\lambda\|w\|_{k}^{k})$$
где $\lambda$ - гиперпараметр, а $\|w\|_{k}^{k}$ равен либо $\|w\|_{1}$, либо $\|w\|^{2}$

| $\lambda\|w\|_{k}^{k}$ -  **регуляризационный член** (**регуляризатор**), а $\lambda$ - коэффициент регуляризации.

$\lambda$ достаточно сильно влияет на модель. Его подбирают по логарифмической шкале, а также для сравнения моделей с разным $\lambda$ используют валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение.

**Важно**: если $w_{0}$ находится в векторе весов, то регулязировать его не надо, поэтому:
$$\|w\|_{1} = \sum_{j=1}^{N}|w_{j}|$$
$$\|w\|_{2}^{2}=\sum_{j=1}^{N}w_{j}^{2}$$
В случае $L^{2}$-регуляризации решение задачи сильно не поменяется. Вот так будет выглядеть "точное" решение:
$$w=(X^{T}X+\lambda I)X^{T}y$$
Чем ближе $X^{T}X$ ближе к вырожденной матрице, тем тяжелее её обратить, поэтому лучше немного её исказить, сделав решение менее точным, но при этом уменьшить проблемы с вычислением.
А в приближённом решении:
$$L(f,X,y) = \|Xw-y\|_{2}^{2}+\lambda\|w\|_{2}^{2}$$
градиент по $w$ будет выглядеть так:
$$\nabla_{w} L(f,X,y)=2X^{T}(Xw-y)+2\lambda w$$
В целом, $L^{2}$ и $L^{1}$ регуляризацию можно применять к любой loss функции, и не только для задач регрессии. Поэтому loss функция после регуляризации будет иметь вид:
$$\tilde{L}(f,X,y)=L(f,X,y)+\lambda\|w\|_{1}$$
или
$$
\tilde{L}(f,X,y) = L(f,X,y) + \lambda\|w\|_{2}^{2}
$$
### Разреживание весов в $L^{1}$-регуляции

У $L^{1}$ регуляции есть одна особенность: после регуляризации ею некоторые веса становятся равны нулю, что позволяют удалить ненужные нам признаки, а также позволяет автоматически убирать (приближённо) линейно зависимые признаки, что решает проблему с мультиколлинеарностью.

#### Объяснение


