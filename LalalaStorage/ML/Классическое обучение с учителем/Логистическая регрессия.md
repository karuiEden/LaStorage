---
title: Логистическая регрессия
created: 2025-08-11
tags:
  - ml
  - linear_models
links:
  - "[[Линейная классификация]]"
---
**Определение:** Логистическая регрессия - линейная модель классификации, полученная путём логистического функционала:
$$L(y,z)=\log(1+\exp(-yz))$$
**Вид модели:**
$$a(w,x)=\begin{cases}
0, \sigma(\langle w,x \rangle) \leq 0.5, \\
1, \ \sigma(\langle w,x \rangle) > 0.5
\end{cases}$$
Помимо $0.5$ может быть другой порог.

Пусть $\forall x\in X$ задана вероятность $p(y= +1\ | \ x)$ того, что объект $x$ будет принадлежать классу $+1$. Это означает, что мы допускаем наличие объектов, которые имеют разные значения целевой переменной, но при этом имеют одинаковые признаковые описания; причем если устремить количество объекта $x$ до бесконечности, то доля положительных объектов среди них будет равна $p(y= +1\ | \ x)$.

Итак, рассмотрим точку $x$ пространства объектов. Как мы договорились, в ней имеется распределение на ответах $p(y=+1\ |\ x)$. Допустим, что алгоритм $b(x)$ возвращает числа из отрезка $[0,1]$. Наша задача подобрать для него такую процедуру обучения, что в точке $x$ ему будет оптимально выдавать число $p(y=+1\ |\ x)$. Если объект $x$ встречается несколько раз в выборке $n$ раз с ответами $\{ y_{1},\dots,y_{n} \}$, то получим требование:$$\arg \min_{b\in \mathbb{R}} \frac{1}{n}\sum_{i=1}^{n} L(y_{i},b)\approx p(y=+1\ |\ x)$$
При $n \to \infty$ получим, что функционал сводится к матожиданию ошибки:
$$\arg \min_{b\in \mathbb{R}}\mathbb{E}[L(y_{i},b)\ |\ x]=p(y=+1\ | \ x)$$
Например, квадратичная функция потерь обладает этим свойством, если использовать классы $0,1$, а функция потерь $L(y,x)=|y-z|$ не обладает этим свойством. Но для задач классификации квадратичная функция потерь плохо подходит, ведь штрафовать она будет очень слабо. 
Поэтому попробуем сконструировать функцию потерь из других соображений. Если алгоритм $b(x)\in[0,1]$ действительно выдаёт вероятности, то они должны согласовываться с выборкой. С точки зрения алгоритма вероятность того, что в выборке встретится объект $x_{i}$ с классом $y_{i}$ равна $b(x_{i})^{[y_{i}=+1]}(1-b(x_{i}))^{[y_{i}=-1]}$. Исходя из этого, можно записать правдоподобие выборки:
$$Q(a,X)=\prod_{i=1}^{\ell}b(x_{i})^{[y_{i}=+1]}(1-b(x_{i}))^{[y_{i}=-1]}$$ 
Данное правдоподобие можно использовать как функционал для обучения алгоритма, но для начала прологарифмировав его с минусом:
$$-\sum_{i=1}^{\ell}([y_{i}=+1]\ln b(x_{i}))+[y_{i}=-1]\ln(1-b(x_{i}))\to \min$$
Данная функция потерь называется **логарифмической** (log-loss). Покажем, что она также способна корректно предсказывать вероятности. Запишем матожидание функции потерь в точке $x$:
$$\mathbb{E}[L(y,b)\ |\ x]=\mathbb{E}[-[y=+1]\ln b-[y=-1]\ln(1- b)\ |\ x]=$$
$$=-p(y=+1\ |\ x)\ln b- (1 - p(y=+1 \ | \ x))\ln(1-b)$$
Продифференцируем по $b$:
$$\frac{\partial}{\partial b}\mathbb{E}[L(y,b)\ | \ x]= \frac{p(y=+1\ | \ x)}{b} + \frac{1-p(y=+1\ |\ x)}{1-b} =0$$
Легко заметить, что оптимальный ответ алгоритма равен вероятности положительного класса:
$$b^{*}=p(y=+1\ |\ x)$$
Выше мы требовали, что $b(x)$ возвращал числа из отрезка $[0,1]$. Это легко достичь, если положить $b(x)=\sigma(\langle w,x \rangle)$, где в качестве $\sigma$ может выступать любая монотонно неубывающая функция с областью значений $[0,1]$. Будем использовать сигмоидную функцию: $\sigma(z)=\frac{1}{1+\exp(-z)}$. Таким образом, чем выше скалярное произведение $\langle w,x \rangle$, тем больше будет предсказанная вероятность. Как при этом можно интерпретировать это скалярное произведение? Для этого преобразуем уравнение:
$$p(y=1\ |\ x)= \frac{1}{1+\exp(-\langle w,x \rangle)}$$
Выразим отсюда скалярное произведение:
$$\langle w,x \rangle= \ln \frac{p(y=+1\ |\ x)}{p(y=-1\ |\ x)}$$
В итоге, получим, что скалярное произведение  равно логарифму отношения вероятностей классов (log-odds).

По сравнению с квадратичной функцией потерь, которая слабо штрафует за ошибки, логарифмическая функция потерь будет намного больше штрафовать за ошибки из-за чего модель будет лучше предсказывать вероятности.

Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:
$$-\sum_{i=1}^{\ell}\bigg(   [y_{i}=+1] \ln \frac{1}{1+\exp(-\langle w,x_{i} \rangle)} + [y_{i}=-1] \ln \frac{\exp(-\langle w,x_{i} \rangle)}{1+\exp(-\langle w,x_{i} \rangle)} \bigg)=$$
$$=-\sum_{i=1}^{\ell}\bigg(   [y_{i}=+1] \ln \frac{1}{1 + \exp(-\langle w,x_{i} \rangle)} +[y_{i}=-1] \ln \frac{1}{1+\exp(\langle w,x_{i} \rangle)} \bigg)$$
$$\sum_{i=1}^{\ell}\ln(1+\exp(-y_{i}\langle w,x_{i} \rangle))$$
Полученная функция представляет собой логистические потери. Линейная модель классификации, полученная путем данного функционала называется **логистической регрессией**.
