---
title: Метод опорных векторов
created: 2025-09-15
tags:
  - ml
  - supervised
links:
  - "[[Линейная классификация]]"
---
Рассмотрим подход к построению функцию потерь, основанный на максимизации зазора между классами. Будем рассматривать линейные классификаторы:
$$a(x)=\sgn(\langle w,x \rangle + b), \ \ w\in \mathbb{R}^{d},b\in \mathbb{R}$$ 
## Разделимый случай

Будем считать, что $\exists w_{*}, b_{*}$, что соответствующий им классификатора $a(x)$ не допускает ни одной ошибки на обучающей выборке. В этом случае говорят, что выборка **линейно разделима**.

Пусть задан некоторый классификатор $a(x)=\sgn(\langle w,x \rangle + b)$. Заметим, что если одновременно умножить параметры $w$ и $b$ на одну и ту же положительную константу, то классификатор не изменится. Используем это и отнормируем параметры так, что $$\min_{x\in X}|\langle w,x \rangle + b| =1$$
Можно показать, что расстояние от произвольной точки $x_{0}\in \mathbb{R}^{d}$ до гиперплоскости, определяем данным классификатором, равно $$\rho(x_{0},a)= \frac{|\langle w,x \rangle + b|}{\|w\|}$$
Тогда расстояние от гиперплоскости до ближайшего объекта обучающей выборки равно $$\min_{x\in X} \frac{|\langle w,x \rangle + b|}{\|w\|}= \frac{1}{\|w\|}\min_{x\in X} |\langle w,x \rangle+b| = \frac{1}{\|w\|}$$
**Определение:** *Отступ (margin)* - расстояние от гиперплоскости до ближайшего объекта обучающей выборки, равное $$\frac{1}{\|w\|}$$
Таким образом, если классификатор без ошибок разделяет обучающую выборку, то ширина его разделяющей полосы равна $\frac{2}{\|w\|}$. Известно, что максимизация ширины разделяющей полосы приводит к повышению обобщающей способности классификатора. Вспомним, что на повышение обобщающей способности направлена регуляризация, которая штрафует большую норму весов - а чем больше норма весов, тем меньше ширина разделяющей полосы.
Итого, необходимо построить классификатор, идеально разделяющий обучающую выборку, и при этом имеющий максимальный отступ. Запишем соответствующую оптимизационную задачу, которая и будет определять *метод опорных векторов для линейно разделимой выборки (hard margin support vector machine)*:
$$
\begin{cases}
\frac{1}{2}\|w\|^{2}\to \min_{w,b} \\
y_{i}(\langle w,x_{i} \rangle + b) \geq 1, \ \ i =1,\dots,\ell
\end{cases} \ \ \ (2.2)$$

^6fd572

Здесь мы пользовались тем, что линейный классификатор даст на объекте $x_{i}$ правильный ответ, только и только тогда, когда $y_{i}(\langle w,x_{i} \rangle+b)\geq 0$. Хотя из условия нормировки следует, что $y_{i}(\langle w,x_{i} \rangle+b)\geq 1$. 
В данной задаче функционал является строго выпуклым, а ограничения линейными, поэтому и сама задача является выпуклой и имеет единственное решение. Более того, задача является эффективной, поэтому может быть решена крайне эффективно.

## Неразделимый случай

Теперь рассмотрим общий случай, когда выборку невозможно идеально разделить гиперплоскостью. Это означает, что какие бы $w$ и $b$ мы не взяли, хотя бы одно из ограничений в задаче ([[#^6fd572|2.2]]) будет нарушено:
$$\exists x_{i}\in X:y_{i}(\langle w,x_{i} \rangle+b) < 1$$
Сделаем эти ограничения "мягкими", введя штраф $\xi_{i}\geq 0$ за их нарушение:
$$y_{i}(\langle w,x_{i} \rangle + b) < 1-\xi_{i}, \ \ i=1,\dots,\ell$$
Отметим, что если отступ объекта лежит между нулём и единицей, то объект верно классифицируется, но имеет ненулевой штраф $\xi>0$. Таким образом, мы штрафуем объекты за попадание внутрь разделяющей гиперплоскости.
В данном случае величина $\frac{1}{\|w\|}$ называется *мягким отступом (soft margin)*. С одной стороны, мы хотим максимизировать отступ, с другой - минимизировать штраф за неидеальное разделение выборки $\sum_{i=1}^{\ell}\xi_{i}$ . Эти задачи противоречат друг другу: так как излишняя подгонка под выборку приводит к маленькому отступу, а максимизация отступов - к большей ошибке на выборке. В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин. Приходим к оптимизационной задаче, соответствующей *методу опорных векторов для линейной не разделимой выборке (soft margin support vector machine)*.
$$\begin{cases}
\frac{1}{2}\|w\|^{2}+C\sum_{i=1}^{\ell}\xi_{i}\to \min_{w,b,\xi} \\
y_{i}(\langle w,x_{i} \rangle+b)\geq1-\xi_{i} \ \ \ \ i=1,\dots ,\ell \\
\xi_{i}\geq 0, \ \ \ i=1,\dots,\ell. 
\end{cases} \ \ \ (2.3)$$

^a42788

Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку.
Данная задача также является выпуклой и имеет единственное решение.

## Сведение к безусловной задаче

Попробуем свести задачу ([[#^a42788|2.3]]) к задаче безусловной оптимизации.
Перепишем условия задачи:
$$\begin{cases}
\xi_{i}\geq 1-y_{i}(\langle w,x_{i} \rangle + b) \\
\xi_{i}\geq 0
\end{cases}$$
Поскольку при этом в функционале требуется, чтобы штрафы $\xi_{i}$ были как можно меньше, то можно получить следующую явную формулу для них:$$\xi_{i}=\max(0,1-y_{i}(\langle w,x_{i} \rangle+b))$$
Данное выражение для $\xi_{i}$ учитывает все ограничения задачи ([[#^a42788|2.3]]). Значит, если подставим формулу в функционал, то получим безусловную задачу оптимизации:
$$\frac{1}{2}\|w\|^{2}+C\sum_{i=1}^{\ell}\max(0,1-y_{i}(\langle w,x_{i} \rangle + b)) \to \min_{w,b}$$
Данная задача не является гладкой, поэтому решать её может быть тяжело, но она показывает, что метод опорных векторов, по сути, строит верхнюю оценку вида $L(y,z)=\max(0,1-yz)$ на долю ошибок, и добавляет к ней стандартную квадратичную регуляризацию.