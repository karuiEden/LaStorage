---
title: Многоклассовая классификация
created: 2025-09-15
tags:
  - linear_models
  - ml
links:
  - "[[Логистическая регрессия]]"
  - "[[Метод опорных векторов]]"
  - "[[Линейная классификация]]"
---
Будем считать, что каждый объект относится к одному из $K$ классов $Y=\{ 1,\dots,K \}$
## Сведение к серии бинарных задач

Рассмотрим два самых популярных способа сведения многоклассовой задачи к набору бинарных.

### Один против всех (one-versus-all)

Обучим $K$ линейных классификаторов $b_{1}(x),\dots ,b_{k}(x)$, выдающих оценки принадлежности классам $1,\dots,K$ соответственно. Например, в случае с линейными моделями эти модели будут иметь вид:
$$b_{k}(x)=\langle w_{k},x \rangle+w_{0k}$$ Классификатор с номером $k$ будем обучать по выборке $(x_{i}, 2[y_{i}=k]-1)_{i=1}^{\ell}$ иными словами, мы учим классификатор отличать $k$-ый класс от всех остальных.
Итоговый классификатор будет выдавать класс, соответствующий самому уверенному из бинарных алгоритмов:
$$a(x)=\arg\max_{k\in \{ 1,\dots K \}}b_{k}(x)$$ Проблема данного подхода в том, что каждый из классификаторов $b_{1}(x),\dots,b_{k}(x)$ обучается на своей выборке из-за чего выходы этих классификаторов могут иметь разные масштабы и их неправильно будет сравнивать. Но при этом нормировать веса модели не всегда может быть хорошим решением. Например, для SVM это потеряет весь смысл, ибо нормирование весов изменит и их норму.
### Всех против всех (all-versus-all)

Обучим $C^{2}_{K}$ классификаторов $a_{ij}(x), \ i,j=1,\dots, K, \ i\neq j$. Например, в случае с линейными моделями эти модели будут иметь вид:
$$b_{k}(x)=\sgn(\langle w_{k},x \rangle+w_{0k})$$
Классификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij}\subset X$, содержащей только классы $i$ и $j$:
$$X_{ij}=\{ (x_{n},y_{n}) \in X\ | \ [y_{n}=i]=1\  \vee \ [y_{n}=j]=1   \}.$$
Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$.
Чтобы классифицировать новый объект, подадим его на вход каждого из построенных классификаторов. Каждый из них проголосует за свой класс. Тогда в качестве ответа выберем тот класс, за который наберётся больше всего голосов:
$$a(x)=\arg\max_{k\in \{ 1,\dots,K \}} \sum_{i=1}^{K}\sum_{j\neq i}[a_{ij}(x)=k]$$
## Многоклассовая логистическая регрессия

В логистической регрессии для двух классов мы строили линейную модель типа $b(x)=\langle w,x \rangle + w_{0}$, а затем переводили её прогноз в вероятность через сигмоидную функцию $\sigma(z)= \frac{1}{1+\exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей $b_{k}(x)=\langle w_{k},x \rangle+w_{0k}$, каждая из которых даёт оценку принадлежности объекта одному из классов, как преобразовать вектор оценок $(b_{1}(x),\dots,b_{K}(x))$ в вероятности? Для этого можем воспользоваться оператором $\text{SoftMax}(z_{1},\dots,z_{K})$, который производит нормировку вектора:
$$\text{SoftMax}(z_{1},\dots,z_{K})=\bigg(   \frac{\exp(z_{1})}{\sum_{k=1}^{K}\exp(z_{k})},\dots, \frac{\exp(z_{K})}{\sum_{k=1}^{K}\exp(z_{k})} \bigg) $$
В этом случае вероятность $k$-ого класса будет выражаться как:
$$P(y=k\ | \ x,w) = \frac{\exp(\langle w_{k},x \rangle + w_{0k})}{\sum_{j=1}^{K}\exp(\langle w_{j},x \rangle + w_{0j})} $$ Обучать эти веса предлагается также через метод максимального правдоподобия, как и обычную логистическую регрессию:
$$\sum_{i=1}^{\ell}\ln P(y=y_{i}\ |\ x_{i}, w)\to\max_{w_{1},\dots,w_{K}}$$
## Многоклассовый метод опорных векторов

В алгоритме "один против всех" мы *независимо* строили свой классификатор за каждый класс. Попробуем теперь строить эти классификаторы одновременно, в рамках одной оптимизационной задачи. Есть много подходов обобщения метода опорных векторов, разберём метод описанный в работе [Koby Crammer и Yoram Singer "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"](https://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf) 
Для простоты будем считать, что в выборке имеется константный признак, и не будет явно указывать сдвиг $b$. Будем настраивать $K$ наборов параметров $w_{1},\dots,w_{K}$, и итоговый алгоритм определим как $$a(x)=\arg\max_{k\in \{ 1,\dots ,K \}}\langle w_{k},x \rangle$$
Рассмотрим следующую функцию потерь:
$$\max_{k}\bigg\{ \langle w_{k},x \rangle +1 -[k=y(x) ] \bigg\} - \langle w_{y(x)},x \rangle$$
Разберёмся сначала с выражением, по которому берём максимум. Если $k=y(x)$, то оно равно $\langle w_{k},x \rangle$, в ином случае $\langle w_{k},x \rangle + 1$. Если оценка за верный класс больше оценок за остальные классы, хотя бы на единицу, то максимум будет достигаться на $k=y(x)$, в этом случае потеря будет равна нулю. Иначе потеря будет равна больше нуля. Здесь можно увидеть аналогию с бинарным *SVM*, так как штрафуется не только неправильный ответ, но и неуверенная классификация.
Рассмотрим сначала линейно разделимую выборку, т.е. такую, что существуют веса $w_{1*},\dots,w_{K*}$, при которых функция потери будет равна нулю. В бинарном SVM мы строили классификатор с максимальным отступом. Известно, что аналогом отступа для многоклассового случая является [[Норма Фробениуса|норма Фробениуса]] матрицы $W$, $k$-ая строка которой совпадает с $w_{k}$:
$$\rho=\frac{1}{\|W\|^{2}}= \frac{1}{\sum_{k=1}^{K}\sum_{j=1}^{d}w_{kj}^{2}}$$
Получаем следующую задачу:
$$\begin{cases}
\frac{1}{2}\|W\|^{2}\to\min_{W} \\
\langle w_{y_{i}},x_{i} \rangle + [y_{i}=k] -  \langle w_{k},x_{i} \rangle\geq1, \ \ i=1,\dots,\ell;k=1,\dots,K \\
\end{cases}$$
Перейдем теперь к общему случаю. Как и в бинарном методе опорных векторов, перейдём к мягкой функции потерь, введя штрафы за неверную или неуверенную классификацию. Получим задачу:$$\begin{cases}
\frac{1}{2}\|W\|^{2}\to \min_{W} \\
\langle w_{y_{i}},x_{i} \rangle + [y_{i}=k] -  \langle w_{k},x_{i} \rangle\geq1 - \xi_{i}, \ \ i=1,\dots,\ell;k=1,\dots,K \\
\xi_{i}\geq 0, \ \ i=1,\dots ,\ell
\end{cases}$$
Решать эту задачу можно, например, через пакет $\text{SVM}^\text{multiclass}$.
Такой подход решает проблему с несоизмеримостью величин, выдаваемых отдельными классификаторами, так как классификаторы настраиваются одновременно и выдаваемые ими оценки должны правильно соотноситься друг с другом, чтобы удовлетворять ограничениям.

## Метрики качества многоклассовой классификации

В многоклассовых задачах, как правило, стараются свести подсчёт качества к вычислению классических метрик классификации. Выделяют два подхода к этому: микро- и макро-усреднение.
Пусть выборка состоит из $K$ классов. Рассмотрим $K$ двухклассовых задач, каждая из которых заключается в отделении своего класса от остальных, то есть целевые значения для $k$-ой задачи вычисляются как $y_{i}^{k}=[y_{i}=k]$. Для каждой из них можно вычислить различные характеристики (TP, FP) алгоритма $a^{k}(x)=[a(x)=k]$; будем обозначать эти величины как $\text{TP}_{k},\text{ FP}_{k},\text{ FN}_{k},\text{ TN}_{k}$. Заметим, что в двуклассовом случае все метрики качества, которые мы изучали, выражались через эти элементы матрицы ошибок.
При микро-усреднении сначала эти характеристики усредняются по всем классам, а затем вычисляется итоговая двуклассовая метрика - например, точность, $F$-мера и полнота:
$$\text{precision}(a,X)= \frac{\overline{\text{TP}}}{\overline{\text{TP}}+\overline{\text{FP}}}$$
, где, например, $\overline{\text{TP}}$ вычисляется по формуле:
$$\overline{\text{TP}}=\frac{1}{K}\sum_{k=1}^{K}\text{TP}_{k}$$
При макро-усреднении сначала вычисляется итоговая метрика для каждого класса, а затем результаты усредняются по всем классам. Например, точность будет вычислена как$$\text{precision}(a,X)=\frac{1}{K}\sum_{k=1}^{K}\text{precision}_{k}(a,X) \ \ \ \ \text{precision}_{k}(a,X)= \frac{\text{TP}_{k}}{\text{TP}_{k} + \text{FP}_{k}}$$
В случае микро-усреднения, если какой-то класс имеет очень маленькую мощность, он практически не будет влиять на результат, поскольку его вклад в усреднение будет минимален. В случае же макро-усреднения проводится для величин, которые уже не чувствительны к соотношению размеров классов, и поэтому каждый класс внесёт равный вклад в итоговую метрику.