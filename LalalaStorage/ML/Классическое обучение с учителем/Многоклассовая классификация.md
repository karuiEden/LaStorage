---
title: Многоклассовая классификация
created: 2025-09-15
tags:
  - linear_models
  - ml
links:
  - "[[Логистическая регрессия]]"
  - "[[Метод опорных векторов]]"
  - "[[Линейная классификация]]"
---
Будем считать, что каждый объект относится к одному из $K$ классов $Y=\{ 1,\dots,K \}$
## Сведение к серии бинарных задач

Рассмотрим два самых популярных способа сведения многоклассовой задачи к набору бинарных.

### Один против всех (one-versus-all)

Обучим $K$ линейных классификаторов $b_{1}(x),\dots ,b_{k}(x)$, выдающих оценки принадлежности классам $1,\dots,K$ соответственно. Например, в случае с линейными моделями эти модели будут иметь вид:
$$b_{k}(x)=\langle w_{k},x \rangle+w_{0k}$$ Классификатор с номером $k$ будем обучать по выборке $(x_{i}, 2[y_{i}=k]-1)_{i=1}^{\ell}$ иными словами, мы учим классификатор отличать $k$-ый класс от всех остальных.
Итоговый классификатор будет выдавать класс, соответствующий самому уверенному из бинарных алгоритмов:
$$a(x)=\arg\max_{k\in \{ 1,\dots K \}}b_{k}(x)$$ Проблема данного подхода в том, что каждый из классификаторов $b_{1}(x),\dots,b_{k}(x)$ обучается на своей выборке из-за чего выходы этих классификаторов могут иметь разные масштабы и их неправильно будет сравнивать. Но при этом нормировать веса модели не всегда может быть хорошим решением. Например, для SVM это потеряет весь смысл, ибо нормирование весов изменит и их норму.
### Всех против всех (all-versus-all)

Обучим $C^{2}_{K}$ классификаторов $a_{ij}(x), \ i,j=1,\dots, K, \ i\neq j$. Например, в случае с линейными моделями эти модели будут иметь вид:
$$b_{k}(x)=\sgn(\langle w_{k},x \rangle+w_{0k})$$
Классификатор $a_{ij}(x)$ будем настраивать по подвыборке $X_{ij}\subset X$, содержащей только классы $i$ и $j$:
$$X_{ij}=\{ (x_{n},y_{n}) \in X\ | \ [y_{n}=i]=1\  \vee \ [y_{n}=j]=1   \}.$$
Соответственно, классификатор $a_{ij}(x)$ будет выдавать для любого объекта либо класс $i$, либо класс $j$.
Чтобы классифицировать новый объект, подадим его на вход каждого из построенных классификаторов. Каждый из них проголосует за свой класс. Тогда в качестве ответа выберем тот класс, за который наберётся больше всего голосов:
$$a(x)=\arg\max_{k\in \{ 1,\dots,K \}} \sum_{i=1}^{K}\sum_{j\neq i}[a_{ij}(x)=k]$$
## Многоклассовая логистическая регрессия

В логистической регрессии для двух классов мы строили линейную модель типа $b(x)=\langle w,x \rangle + w_{0}$, а затем переводили её прогноз в вероятность через сигмоидную функцию $\sigma(z)= \frac{1}{1+\exp(-z)}$. Допустим, что мы теперь решаем многоклассовую задачу и построили $K$ линейных моделей $b_{k}(x)=\langle w_{k},x \rangle+w_{0k}$, каждая из которых даёт оценку принадлежности объекта одному из классов, как преобразовать вектор оценок $(b_{1}(x),\dots,b_{K}(x))$ в вероятности? Для этого можем воспользоваться оператором $\text{SoftMax}(z_{1},\dots,z_{K})$, который производит нормировку вектора:
$$\text{SoftMax}(z_{1},\dots,z_{K})=\bigg(   \frac{\exp(z_{1})}{\sum_{k=1}^{K}\exp(z_{k})},\dots, \frac{\exp(z_{K})}{\sum_{k=1}^{K}\exp(z_{k})} \bigg) $$
В этом случае вероятность $k$-ого класса будет выражаться как:
$$P(y=k\ | \ x,w) = \frac{\exp(\langle w_{k},x \rangle + w_{0k})}{\sum_{j=1}^{K}\exp(\langle w_{j},x \rangle + w_{0j})} $$ Обучать эти веса предлагается также через метод максимального правдоподобия, как и обычную логистическую регрессию:
$$\sum_{i=1}^{\ell}\ln P(y=y_{i}\ |\ x_{i}, w)\to\max_{w_{1},\dots,w_{K}}$$
## Многоклассовый метод опорных векторов
