---
title: Преобразования признаков
created: 2025-10-05
tags:
  - ml
links:
  - "[[LalalaStorage/ML/Основы|Основы]]"
---
## Нелинейные признаки

С помощью линейной регрессии можно восстанавливать нелинейные зависимости, если провести преобразование признакового пространства:
$$x=(x_{1},\dots,x_{d})\to \phi(x)=(\phi_{1}(x),\dots,\phi_{m}(x))$$
Например, можно перейти к квадратичным признакам:
$$\phi(x)=(x_{1},\dots,x_{d},x_{1}^{2},\dots,x_{d}^{2},x_{1}x_{2},\dots,x_{d-1}x_{d}).$$
Линейная модель над новыми признаками уже сможет приближать любые квадратичные закономерности. Аналогично можно работать и с полиномиальными признаками более высоких порядков.

Возможны и другие преобразования:
- $\log x_{j}$ - для признаков с тяжелыми хвостами
- $\exp\left( \frac{\|x-\mu\|^{2}}{\sigma} \right)$ - для измерения близости до некоторой точки
- $\sin\left( \frac{x_{j}}{T} \right)$ - для задач с периодическими зависимостями

## Масштабирование

При обучении линейных моделей полезно масштабировать признаки, то есть  приводить их к единой шкале. Разберёмся, зачем это нужно.
Рассмотрим функцию $f_{1}(x)= \frac{1}{2}x_{1}^{2}+\frac{1}{2}x_{2}^{2}$, выберем начальное приближение $x^{(0)}=(1,1)$ и запустим из него градиентный спуск с параметром $\eta=1$. Окажется, что за один шаг мы сможем сразу попасть в точку минимума.

Теперь "растянем" функцию вдоль одной из осей: $f_{2}(x)= 50x_{1}^{2}+\frac{1}{2}x_{2}^{2}$. При таком же начальном приближении $x^{(0)}$  антиградиент на первой итерации будет равен $(-100,-1)$, и попасть по нему в минимум уже невозможно - более того, при неаккуратном выборе длины шага можно очень далеко уйти от минимума.
Пример траектории градиентного спуска при такой форме функции можно увидеть ниже.
![[Pasted image 20251005125546.png]]

Аналогичная проблема возникает с функционалом ошибки в линейном регресии, если один из признаков существенно отличается по масштабу от остальных.

Чтобы избежать этого, признаки следует масштабировать:

>[!definition] Стандартизация
>$$x_{ij}:= \frac{x_{ij}-\mu_{j}}{\sigma_{j}},$$
>где $\mu_{j}= \frac{1}{\ell}\sum_{i=1}^{\ell}x_{ij}, \sigma_{j}= \frac{1}{\ell}\sum_{i=1}^{\ell}(x_{ij}-\mu_{j})^{2}.$

>[!definition] MinMax
>$$x_{ij}:= \frac{x_{ij}=\min_{i}x_{ij}}{\max_{i}x_{ij}-\min_{i}x_{ij}}$$

