---
title: Разреженные модели
created: 2025-10-05
tags:
  - ml
links:
  - "[[LalalaStorage/ML/Основы|Основы]]"
  - "[[Регуляризация]]"
---
>[!definition] Разреженные модели
>Модели, в которых некоторые веса равны нулю

Несколько примеров:
1. Известно, что не все признаки являются релевантными. Поэтому нужно проводить отбор признаков.
2. Ограничение по скорости построения предсказания.
3. В обучающей выборке объектов намного меньше, чем признаков.

Изучим подробнее $L_{1}$-регуляризатор.
### Угловые точки

Можно показать, что если функционал $Q(w)$ является выпуклым, то задача безусловной минимизации функции $Q(w)+\alpha\|w\|_{1}$ эквивалентна задаче условной оптимизации
$$
\begin{cases}
Q(w)\to\min_{w} \\
\|w\|_{1}\leq C
\end{cases}
$$
для некоторого $C$. На рисунке ниже изображены линии уровня функционала $Q(w)$, а также множество, определяемое ограничением $\|w\|_{1}\leq C$.

![[1_9_c157936d62_f29bca1e43.webp]]

Решение определяется точкой пересечения допустимого множества с линией уровня ближайшей к безусловному минимуму.

### Штрафы при малых весах

Предположим, что текущий вектор весов состоит из двух элементов $w=(1,\varepsilon)$, где $\varepsilon$ близко к нулю, и мы хотим немного изменить данный вектор по одной из координат. Найдем изменение $L_{2}$ и $L_{1}-$норм вектора при уменьшении первой компоненты на некоторое положительное число $\delta<\varepsilon$:
$$\begin{flalign}
&\|w-(\delta ,0)\|^{2}_{2}=1-2\delta+\delta^{2}+\varepsilon^{2} \\
&\|w-(\delta ,0)\|_{1}=1-\delta+\varepsilon
\end{flalign}$$

Вычислим то же самое для изменения второй компоненты:
$$\begin{flalign}
&\|w-(0,\delta)\|_{2}^{2}=1-2\varepsilon\delta+\delta^{2}+\varepsilon^{2} \\
&\|w-(0,\delta)\|_{1}=1-\delta+\varepsilon
\end{flalign}$$
Видно, что с точки зрения $L_{2}$-нормы выгоднее уменьшать первую компоненту, а для $L_{1}$-нормы оба изменения равноценны. Таким образом, при выборе $L_{2}$-регуляризации гораздо меньше шансов, что маленькие веса будут окончательно обнулены.

### Проксимальный шаг

>[!definition] Проксимальные методы
>Класс методов оптимизации, которые хорошо подходят для функционалов с негладкими слагаемыми.

Приведем формулу для шага проксимального метода в применении к линейной регрессии с квадратичным функционалом и $L_{1}$-регуляризатором:
$$w^{(k)}=S_{\eta\alpha}(w^{(k-1)}-\eta \nabla_{w}F(w^{(k-1)})),$$
где $F(w)=\|Xw-y\|^{2}$ - функционал ошибки без регуляризатора, $\eta$ - длина шага, $\alpha$ - коэффициент регуляризации, а функция $S_{\eta\alpha}(w)$ применяется к вектору весов покомпонентно, и для одного элемента выглядит как $$S_{\eta\alpha}(w_{i})=\begin{cases}
w_{i}-\eta\alpha, & w_{i}>\eta\alpha \\
0, & |w_{i}|<\eta\alpha \\
w_{i}+\eta\alpha, & w_{i} < -\eta\alpha
\end{cases}$$
Из формулы видно, что если на данном шаге значение некоторого веса не очень большое, то на следующем шаге этот вес будет обнулён, причём больше коэффициент регуляризации, тем больше весов будут обнуляться.
