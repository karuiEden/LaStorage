---
title: Регуляризация
created: 2025-10-05
tags:
  - ml
  - linear_models
links:
  - "[[Линейная регрессия]]"
  - "[[Линейная классификация]]"
  - "[[LalalaStorage/ML/Основы|Основы]]"
---
Если матрица $X^{T}X$ не является обратимой, то оптимизацией среднеквадратичной ошибки могут возникнуть некоторые трудности. Действительно, в некоторых случаях оптимизационная задача $Q(w)\to\min$ может иметь бесконечное число решений, большинство которых являются переобученными и плохо работают на тестовых данных.

Пусть в выборке есть линейно зависимые признаки. Это по определению означает, что $\exists v:\forall x\in X:\langle v,x \rangle=0.$ Допустим, мы нашли оптимальный вектор весов $w$ для линейного классификатора. Но тогда классификаторы с векторами $w+\alpha v$ будут давать *точно такие же* ответы на всех объектах, так как $$\langle w+\alpha v ,x \rangle=\langle w,x \rangle+\alpha \underbrace{\langle v,x \rangle}_{=0}=\langle w,x \rangle.    $$
Это значит, что метод оптимизации может найти решение со сколько угодно большими весами. Такие решения не очень хороши, поскольку классификатор будет чувствителен к крайне малым изменениям в признаках объекта, а значит, переобучен.

Мы уже знаем, что переобучение нередко приводит к большим значениям коэффициентов. Чтобы решить эту проблему, добавим к функционалу *регуляризатор*, который штрафует за слишком большую норму вектора весов:
>[!definition] Регуляризатор
>Функция $R(w)$, штрафующая функционал для решения проблемы переобучения

Функционал с регуляризатором:
$$Q_{\alpha}(w)=Q(w)+\alpha R(w)$$
>[!definition] $L_{2}$-регуляризатор
>$$R(w)=\|w\|_{2}=\sum_{i=1}^{d}w_{i}^{2}$$

>[!definition] $L_{1}$-регуляризатор
>$$R(w)=\|w\|_{1}=\sum_{i=1}^{d}|w_{i}|$$

>[!definition] Параметр регуляризации
>Коэффициент $\alpha$, контролирующий баланс между обучением и штрафом за излишнюю сложность.

При этом свободный коэффициент $w_{0}$ нет смысла регуляризовывать - если мы будет штрафовать его за величину, то получится, что мы учитываем некоторые априорные представления о близости целевой переменной к нулю и отсутствии необходимости в учёте её смещения. Такое предположение является странным. Особенно если в выборке есть константный признак и коэффициент $w_{0}$ обучается наряду с остальными весами, то в случае с регуляризацией, его необходимо исключить.

Квадратичный регуляризатор достаточно прост в использовании в отличие от $L_{1}$-регуляризатора, у которого нет производной в нуле. При этом $L_{1}$-регуляризатор имеет интересную особенность: его использование приводит к занулению части весов.

Обратим внимание на вид решения при использовании $L_{2}$-регуляризатора вместе с среднеквадратичной ошибкой. В этом случае формулу для оптимального вектора весов можно записать в явном виде:$$w=\left( X^{T}X+\alpha I \right)^{-1}X^{T}y. $$
Благодаря добавлению диагональной матрицы к $X^{T}X$ данная матрица оказывается положительно определённой, и поэтому её можно обратить. Таким образом, при использовании $L_{2}$ регуляризации решение всегда будет единственным.