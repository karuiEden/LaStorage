---
title: Решающее деревья
created: 2025-09-22
tags:
  - ml
  - decision_tree
links:
---
**Определение:** *Бинарное решающее дерево* - это алгоритм $a(x)$, действующий в бинарном дереве, в котором:
- каждой внутренней вершине $v$ приписана функция (или предикат) $\beta_{v}:\mathbb{X}\to \{ 0,1 \}$;
- Каждой листовой вершине приписан прогноз $c_{v}\in Y$ (в случае с классификацией листу может быть приписан вектор вероятностей).
стартующий из корня $v_{0}$ и вычисляет значение $\beta_{v_{0}}$. Если значение равно нулю, то алгоритм переходит в левое поддерево. Иначе, в правое. Всё это происходит, пока не достигается листовая вершина, алгоритм возвращает тот класс, который приписан этой вершине.

В большинстве случаев используются предикаты $\beta_{v}$, которые сравнивают значение одного из признаков с порогом:
$$\beta_{v}(x,j,t)=[x_{j}<t]$$
Существуют также и многомерные предикаты:
- линейные $\beta_{v}=[\langle w,x \rangle < t]$
- метрические $\beta_{v}=[\rho(x,x_{0})<t]$, где $x_{0}$ является одним из объектов выборки любой точкой признакового пространства.
 Многомерные предикаты позволяют строить ещё более сложные разделяющие поверхности, но редко используются на практике, так как усиливают переобучение.
 
 Главный недостаток решающего дерева - это высокая переобучаемость, при этом в композиции они работают очень эффективно.
### Построение деревьев

Так как решающее дерево легко переобучается, то построить идеальное решающее дерево на обучающей выборке не составит труда. Поэтому поставим задачу поиска дерева, которое является минимальным по количеству листьев, среди всех деревьев, не допускающих ошибок на обучающей выборке - в этом случае можно понадеяться на наличие у дерева обобщающей способности. Но данная задача является $\text{NP}$-полной, поэтому приходится ограничиваться жадными алгоритмами построения дерева.

**Жадный алгоритм построения решающего дерева:**
1. Поиск лучшего разбиения на две части $R_{1}(j,t)=\{ x \ | \ x_{j}<t\}$ и $R_{2}(j,t)=\{ x \ | \ x_{j} \geq t \}$ с точки зрения заранее заданного функционала качества;
2. Поиск лучших значений $j$ и $t$. Создание предиката $[x_{j} < t]$;
3. Рекурсивное повторение шагов 1 и 2 до выполнения условия останова.
4. Объявление текущей вершины листовой и ставим в соответствие ответ:
	1.  Если задача классификацией, то ответом может быть класс, к которому относится больше всего объектов в листе, или вектор вероятностей (скажем, вероятность класса может быть равна доле его объектов в листе)
	2. Если задача регрессии, то это может быть среднее значение, медиана или другая функция от целевых переменных объектов в листе.

Решающие деревья могут обрабатывать пропущенные значения - ситуации, в которых для некоторых объектов неизвестны значения одного  или нескольких признаков. Для этого необходимо модифицировать процедуру разбиения выборки в вершине.

После того, как дерево построено, можно провести его *стрижку* ($\text{pruning}$) - удаление некоторых вершин с целью понижения сложности и повышения обобщающей способности.

Таким образом, конкретный метод построения решающего дерева определяется:
1. Видом предикатов в вершинах;
2. Функционалом качества $Q(X,j,t)$;
3. Критерием останова;
4. Методом обработки пропущенных значений;
5. Методом стрижки.

Также могут иметь место некоторые расширения, связанные с учётом весов объектов, работой с категориальными признаками и т.д.
## Обработка пропущенных значений

Одним из основных преимуществ решающих деревьев является рассмотреть возможность работы с пропущенными значениями. Рассмотрим некоторые варианты.
Пусть нам нужно вычислить функционал качества для предиката $\beta(x)=[x_{j}<t]$, но в выборке $R$ для некоторых объектов неизвестно значение признака $j$ - обозначим их через $V_{j}$. В таком случае при вычислении функционала можно просто проигнорировать эти объекты, сделав поправку на потерю информации от этого:
$$Q(R,j,s)\approx \frac{|R \setminus V_{j}|}{|R|} Q(R\setminus V_{j},j,s).$$
Затем, если данный предикат окажется лучшим, поместим объекты из $V_{j}$ как в левое, так и в правое поддерево. Также можно присвоить им при этом веса $|R_{\ell}| / |R|$ в левом поддереве и $|R_{r}| / |R|$ в правом. В дальнейшем веса можно учитывать, добавляя их как коэффициенты перед индикаторами $[y_{i}=k]$ во всех формулах.
На этапе применения дерева это также необходимо выполнять. Если объект попал в вершину, предикат которой не может быть вычислен из-за пропуска значения, то прогнозы для него вычисляются в обоих поддеревьях, и затем усредняются с весами, пропорциональными числу обучающих объектов в этих поддеревьях. Иными словами, если прогноз вероятности для класса $k$ в поддереве $R_{m}$ обозначается через $a_{mk}(x)$, то получаем такую формулу:
$$a_{mk}(x)=\begin{cases}
a_{\ell k}(x), \ \ \beta_{m}(x)=0; \\
a_{rk}(x),\ \ \beta_{m}(x)=1; \\
\frac{|R_{\ell}|}{|R_{m}|}a_{\ell k}(x)+ \frac{|R_{r}}{|R_{m}|}a_{rk}(x), \ \ \beta_{m}(x)\text{ нельзя вычислить.}
\end{cases}$$
Другой подход заключается в построении *суррогатных предикатов* в каждой вершине. Так называется предикат, который использует другой признак, но при этом дает разбиение, максимально близкое к данному.

Однако, нередко схожее качество показывают и более простые способы обработки пропусков - например, заменить пустые значения на нули. Для деревьев также разумно будет заменить пропуски в признаке на числа, которые превосходят любое значение данного признака. В этом случае в дереве можно будет выбрать такое разбиение по этому признаку, что все объекты с известными значениями пойдут в левое поддерево, а все объекты с пропусками - в правое.

## Учет категориальных признаков

Самый очевидный способ обработки категориальных признаков - разбивать вершину на столько поддеревьев, сколько значений имеет данный категориальный признак($\text{multi-way splits}$). Такой подход может показывать хорошие результаты, но при этом есть риск получения дерева с крайне большим числом листьев.

Рассмотрим подробнее другой подход. Пусть категориальный признак $x_{j}$ имеет множество значений $Q=\{ u_{1},\dots ,u_{q} \}$, $|Q|=q$. Разобьем множество значений на два неперескающихся подмножества $Q=Q_{1}\cup Q_{2}$, и определим предикат как индикатор попадания в первое подмножество: $\beta(x)=[x_{j}\in Q_{1}]$. Таким образом, объект будет попадать в левое поддерево, если признак $x_{j}$ попадает в подмножество $Q_{1}$, и правое поддерево в противном случае. Основная проблема, что для построения оптимального предиката необходимо перебрать $2^{q-1}-1$ вариантов разбиения, что может быть не вполне возможным.

Оказывается, можно обойтись без полного перебора в случаях с бинарной классификацией и регрессией. Обозначим через $R_{m}(u)$ множество объектов, которые попали в вершину $m$ и у которых $j-$й признак имеет значение $u$; через $N_{m}(u)$ обозначим количество таких объектов.
В случае с бинарной классификацией упорядочим все значения категориального признака на основе того, какая доля объектов с таким значением имеет класс $+1$:
$$\frac{1}{N_{m}(u_{(1)})}\sum_{x_{i}\in R_{m}(u_{(1)})}[y_{i}=+1]\leq \dots \leq \frac{1}{N_{m}(u_{(q)})}\sum_{x_{i}\in R_{m}(u_{(q)})}[y_{i}=+1]$$
после чего заменим категорию $u_{(i)}$ на число $i$, и будем искать разбиение как для вещественного признака. Можно показать, что если искать оптимальное разбиение по критерию Джини или энтропийному критерию, то мы получим такое же разбиение, как и при переборе $2^{q-1}-1$ вариантам.
Для задачи регрессии с $\text{MSE}$-функционалом это тоже будет верно, если упорядочивать значения признака по среднему ответу объектов с таким значением:
$$\frac{1}{N_{m}(u_{(1)})}\sum_{x_{i}\in R_{m}(u_{(1)})}y_{i}\leq \dots \leq \frac{1}{N_{m}(u_{(q)})}\sum_{x_{i}\in R_{m}(u_{(q)})} y_{i}$$
Именно такой подход используется в библиотеке Spark MLlib.

## Методы построения деревьев

Существует несколько популярных методов построения деревьев:
- $\text{ID3}$: использует энтропийный критерий. Строит дерево до тех пор, пока в каждом листе не окажутся объекты одного класса, либо пока разбиение вершины даст уменьшение энтропийного критерия.
- $\text{C4.5}:$ использует критерий $\text{Gain Ratio}$ (нормированный энтропийный критерий). Критерий останова - ограничение на число объектов в листе. Стрижка производится с помощью метода $\text{Error-Based Pruning}$, который использует оценки обобщающей способности для принятия решения об удалении вершины. Обработка пропущенных значений осуществляется с помощью метода, который игнорирует их при вычисления критерия, а потом распределяет их в оба поддерева с определёнными весами.
- $\text{CART}$: использует критерий Джини. Стрижка осуществляется с помощью $\text{Cost Complexity Pruming}$. Для обработки пропусков используется метод суррогатных предикатов.
## Связь с линейными моделями

По определению, решающее дерево $a(x)$ разбивает всё признаковое пространство на некоторое количество непересекающихся подмножеств $\{ J_{1},\dots,J_{n} \}$, и в каждом подмножестве $J_{j}$ выдает константный прогноз $w_{j}$. Значит, соответствующий алгоритм можно записать аналитически: $$a(x)=\sum_{j=1}^{n}w_{j} [x\in J_{j}].$$

Обратим внимание, что это линейная модель над признаками $([x\in J_{j}])_{j=1}^{n}$ - а ведь мы хотели избавиться от линейности! Получается, что решающее дерево с помощью жадного алгоритма подбирает преобразование признаков для данной задачи, а затем просто строит линейную модель над этими признаками. 