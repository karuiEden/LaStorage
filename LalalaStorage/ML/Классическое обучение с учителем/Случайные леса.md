---
title: Случайные леса
created: 2025-10-04
tags:
  - ml
  - decision_tree
  - ensemble
links:
  - "[[Бэггинг]]"
  - "[[Решающие деревья]]"
---
Как мы выяснили, бэггинг позволяет объединить несмещённые, но чувствительные к обучающей выборке алгоритмы в несмещённую композицию с низкой дисперсией. Хорошим семейством базовых алгоритмов здесь являются решающие деревья - они достаточно сложны и могут достигать нулевой ошибки на любой выборке (следовательно, имеют низкое смещение), но в то же время легко переобучаются.
**Определение:** Метод *случайных лесов* основан на бэггинге над решающими деревьями, см. алгоритм [[#^b3c443|3.1]]. Выше мы отметили, что бэггинг сильнее уменьшает дисперсию базовых алгоритмов, если они слабо коррелированы. В случайных лесах корреляция между деревьями понижается путём рандомизации по двум направлениям:  по объектам и по признакам.

```pseudo
\begin{algorithm}
\caption{Random Forest (3.1)}
\begin{algorithmic}
\For{ $n = 1, \dots, N$}
  \State Generate bootstrap sample $\tilde{X}_n$
  \State Build tree $b_n(x)$ using sample $\tilde{X}_n$
\EndFor
\State Return ensemble $a_N(x) = \frac{1}{N} \sum_{n=1}^N b_n(x)$
\end{algorithmic}
\end{algorithm}
```

^b3c443

Во-первых, каждое дерево обучается по бустрированной подвыборке. Во-вторых, в каждой вершине разбиение ищется по подмножеству признаков. Вспомним, что при построении дерева последовательно происходит разделения вершин до тех пор, пока не будет достигнуто идеальное качество на обучении. Каждая вершина разбиваеи выборку по одному из признаков относительно некоторого порога. В случайных лесах признак, по которому производится разбиение, выбирается не из всех возможных признаков, а лишь из их случайного подмножества размера $m$.

Рекомендуется в задачах классификации брать $m=\sqrt{ d }$, а в задачах регрессии - $m=\frac{d}{3}$, где $d$ - число признаков. Также рекомендуется в задачах классификации строить каждое дерево до тех пор, пока в каждом листе не окажется по одному объекту, а в задачах регрессии - пока в каждом листе не окажется по пять объектов.

Случайные леса - один из самых сильных методов построения композиций. На практике он может работать немного хуже градиентного бустинга, но при этом он гораздо более прост в реализации.

### Out-of-Bag

Каждое дерево в случайном лесе обучается по подмножеству объектов. Это значит, что те объекты, которые не вошли в бутстрапированную выборку $X_{n}$ дерева $b_{n}$, по сути являются контрольными для данного дерева. Значит, мы можем для каждого объекта $x_{i}$ найти деревья, которые были обучены без него, и вычислить по их ответам $\text{out-of-bag-ошибку}$: $$\text{OOB}=\sum_{i=1}^{\ell}L\left( y_{i}, \frac{1}{\sum_{n=1}^{N}[x_{i}\not\in X_{n}]} \sum_{n=1}^{N}[x_{i}\not\in X_{n}]b_{n}(x_{i}) \right),$$
где $L(y,z)$ - функция потерь. Можно показать, что по мере увеличения числа деревьев $N$ данная оценка стремится к $\text{leave-one-out-оценке}$, но при этом существенно проще для вычисления.

## Связь с метрическими методами

Случайные леса, по сути, осуществляют предсказание для объекта на основе меток похожих объектов из обучения. Схожесть объектов при этом тем выше, чем чаще эти объекты оказываются в одном и том же листе дерева. Покажем это формально.

Рассмотрим задачу регрессии с квадратичной функцией потерь. Пусть $T_{n}(x)$ - номер листа $n$-ого дерева из случайного леса, в который попадает объект $x$. Ответ дерева на объекте $x$ равен среднему ответу по всем обучающим объектам, которые попали в лист $T_{n}(x)$. Это можно записать как $$b_{n}(x)=\sum_{i=1}^{\ell}w_{n}(x,x_{i})y_{i},$$
где
$$w_{n}(x,x_{i})= \frac{[T_{n}(x)=T_{n}(x_{i})]}{\sum_{j=1}^{\ell}[T_{n}(x)=T_{n}(x_{j})]}.$$
Тогда ответ композиции равен
$$a_{N}(x)=\frac{1}{N}\sum_{n=1}^{N}\sum_{i=1}^{\ell}w_{n}(x,x_{i})y_{i}=\sum_{i=1}^{\ell}\left( \frac{1}{N}\sum_{n=1}^{N}w_{n}(x,x_{i}) \right)y_{i}.$$
Видно, что ответ случайного лема представляет собой сумму ответов всех объектов обучения с некоторыми весами, причём данные веса измеряют сходство объектов $x$ и $x_{i}$, на основе того, сколько раз они оказались в одном и том же листе. Таким образом, случайный лес позволяет ввести некоторую функцию расстояния на объектах. Как мы узнаем позже, на этом принципе основан целый класс *метрических* методов, наиболее популярным представителем которых является метод $k$ ближайших соседей.

Отметим, что номер листа $T_{n}(x)$, в который попал объект, сам по себе является ценным признаком. Достаточно неплохо работает подход, в котором по выборке обучается композиция из небольшого числа деревьев с помощью случайного леса или градиентного бустинга, а затем к ней добавляются категориальные признаки $T_{1}(x),T_{2}(x),\dots,T_{N}(x)$. Новые признаки являются результатом нелинейного разбиения пространства и несут в себе информацию о сходстве объектов.
