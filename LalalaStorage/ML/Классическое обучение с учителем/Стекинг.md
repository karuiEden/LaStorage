---
title: Стекинг
created: 2025-10-04
tags:
  - ml
  - ensemble
links:
---
>[!definition] Стекинг
>Способ построения композиции, в котором прогнозы алгоритмов объявляются новыми признаками, а поверх них обучается ещё один алгоритм(мета-алгоритм).

Допустим, мы независимо обучили $N$ базовых алгоритмов $b_{1}(x),\dots,b_{N}(x)$ на выборке $X$, и теперь хотим обучить на их прогнозах мета-алгоритм $a(x)$. Самым простым вариантом будет обучить его на этой же выборке:$$\sum_{i=1}^{\ell}L(y_{i},a(b_{1}(x_{i}),\dots,b_{N}(x_{i})))\to\min_{a}$$
При таком подходе $a(x)$ будет отдавать предпочтение тем базовым алгоритмам, которые сильнее всех подогнались под целевую переменную на обучении. Если среди базовых алгоритмов будет идеально переобученный, то мета-алгоритму будет выгодно использовать только прогнозы данного переобученного базового алгоритма, поскольку это позволит добиться лучших результатов с точки зрения записанного функционала. При этом такой алгоритм будет показывать очень низкое качество на новых данных.

Чтобы избежать таких проблем, следует обучать базовые алгоритмы и мета-алгоритм на разных выборках. Разобьем нашу обучающую выборку на $K$ блоков $X_{1},\dots,X_{K}$, и обозначим через $b_{j}^{-k}(x)$ базовый алгоритм $b_{j}(x)$, обученный по всем блокам, кроме $k$-го. Тогда функционал для обучения мета-алгоритма можно записать как $$\sum_{k=1}^{K}\sum_{(x_{i},y_{i})\in X_{k}}L(y_{i},a(b_{1}^{-k}(x_{i}),\dots,b_{N}^{-k}(x_{i})))\to\min_{a}$$
В данном случае при вычислении ошибки мета-алгоритма на объекте $x_{i}$ используются базовые алгоритмы, которые не видели этот объект при обучении, и поэтому мета-алгоритм не может переобучиться на их прогнозах.
### Блендинг

Частным случаем стекинга является блендинг, в котором мета-алгоритм является линейным:$$a(x)=\sum_{n=1}^{N}w_{n}b_{n}(x).$$
Это самый простой способ объединить несколько алгоритмов в композицию. Иногда даже блендинг без обучения весов позволяет улучшить качество по сравнению с отдельными базовыми алгоритмами.

### Категориальные и текстовые признаки

Категориальные и текстовые признаки могут быть серьёзной помехой для использования композиций над деревьями. Стандартным способом кодирования является бинаризация для категориальных признаков и $\text{TF-IDF}$ для текстовых, что приводит к очень большой размерности признакового пространства. Случайный лес на таком наборе признаков будет обучаться долго из-за большой глубины деревьев, а градиентный бустинг может показать слишком плохие результаты из-за небольшой глубины базовых деревьев.

Одним из решений этой проблемы может стать стекинг, в котором градиентным бустингом обучается мета-алгоритм $a(x)$, а каждый категориальный и текствоый признак схлопывается в одно число соответствующим базовым алгоритмом. Для категориальных признаков базовый алгоритм, например, может вычислять счётчики - при этом обратим внимание, что мы уже отмечали важность разделения обучающих выборок для счётчиков и настраиваемых поверх них моделей. Также популярным выбором для базовых алгоритмов являются линейные модели.
