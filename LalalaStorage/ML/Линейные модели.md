*Тэги:* ML

### Почему они до сих пор используются?

- Не переобучаются
- Легко применяется(но требует большой обработки данных)
- Они простые,  также надёжные

## Линейная регрессия
### Определение
Линейная регрессия выглядит так:
$$\hat{f}(x_1,... ,x_n)=w_0 + x_1w_1 + ... + x_nw_n$$
Это можно записать удобнее, представив признаки в виде вектора **$X=(1, x_1, ..., x_n)$** :
$$\hat{f}(X)=\sum_{i=0}^nw_ix_i = X·W$$

### Линейность по параметрам

Какие могут быть происхождения признаков $x_i$?
- Численный признак
- Преобразование численных признаков
- Степени численных признаков
- Значение с [[Анализ данных#One-hot Encoding|OHE]] 
- Взаимодействия между численными признаками

Поэтому линейная регрессия линейна по параметрам, а не по признакам

### Пример

![](https://lh7-rt.googleusercontent.com/slidesz/AGV_vUdor_aTt9kVLacfZnzNNkGntVr2CsLLjZ4Tou58AWrDUyiwnq2caYUnMgIVuBD1RLR4Ys5n76mzMgFw1ZzWkgrCwSBWsFRQ4bolyH_A_DMaIApqQOuijNIyXrlYXkyFnoNdHO7nkQQFlRr5gYlpcmKJtFe0AhS9AJE_v62Luh_eNk2w1CE=s2048?key=UkJVMefAAMIVX1xgakt8xg)

Задача получить такую же модель

Решение:

$$Xw = y$$

Здесь может быть 2 случая:
 Когда количество наблюдений равно количеству признаков($l = n$)
	Решение:
$$w = X^{-1}y$$ Решение существует, если существует обратная матрица($|X| != 0$)

Но обычно получается так, что наблюдений гораздо больше признаков($l >> n$). Тогда СЛАУ будет называться **переопределённой**. Для неё мы можем получить лишь приближенное решение:
$$w = X^+y=(X^TX)^{-1}X^Ty$$ 
**$X^+$ - псевдообратная матрица. **
Такое решение будет иметь наименьшую квадратичную ошибку.

### Получение решения через производную

Подставим веса в функцию потерь и запишем в векторном виде:
$$L(w)=\sum_{i=1}^l{(x^T_iw - y_i)^2} = (Xw - y)^T(Xw-y)$$
Возьмём производную:
$$\frac{\partial L}{\partial w} = 2X^T(Xw-y)$$

Если у $X$ ЛНЗ столбцы, то производную можем прировнять к $0$:
$$ 2X^T(Xw-y) = 0 $$
$$ X^TXw = X^Ty$$
$$w=(X^TX)^{-1}X^Ty$$
