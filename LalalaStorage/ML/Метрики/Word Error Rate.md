---
title: Word Error Rate
created: 2025-12-28
tags:
  - ml
  - nlp
links:
---
Стандартной метрикой оценки систем распознавания речи является частота ошибок распознавания слов. Частота ошибок распознавания слов основана на том, насколько строка слов, возвращаемая распознавателем, отличается от эталонной транскрипции. Первый шаг в вычислении ошибок распознавания слов - это вычисление минимального расстояния редактирования в словах между предполагаемой и правильной строками, что дает нам минимальное количество замен слов, вставок и удалений слов, необходимых для сопоставления между правильной и предполагаемой строками. Затем частота ошибок распознавания слов WER определяется следующим образом:
$$
\text{Word Error Rate} = 100 \cdot \frac{\text{Insertions + Substitutions + Deletions}}{\text{Total Words in Correct Transcript}}
$$
Ниже пример выравнивания эталонного и гипотетического высказывания из корпуса CallHome, показывающий значения, использованные для вычисления частоты ошибок:

![[Pasted image 20251228174001.png]]

В этом высказывании шесть замен, три вставки и одно удаление:
$$
\text{Word Error Rate} = 100 \frac{6 + 3+ 1}{13} =76.9\%
$$
Стандартный метод вычисления частоты ошибок в произношении слов - это бесплатный скрипт *sclite*, доступный в Национальном институте стандартов и технологий (NIST). Sclite получает на вход серию эталонных предложений и соответствующий набор гипотетических предложений. Помимо выравнивания, и вычисления частоты ошибок в произношении слов, sclite выполняет ряд других полезных задач. Например, для анализа ошибок он предоставляет полезную информацию, такую как матрицы ошибок, показывающие, какие слова часто ошибочно вставляются или удаляются. sclite также предоставляет частоту ошибок по дикторам, а также полезную статистику, такую как частота ошибок в предложениях, процент предложений, содержащих хотя бы одну ошибку в произношении слова.

### Нормализация текста перед оценкой

Для систем нормализация текста перед вычислением частоты ошибок является самым обычным делом. Существует множество пакетов для реализации правил нормализации английского языка включают:
1. Удаление метаязыка \[неязыковых элементов, примечаний, комментариев к транскрипции], которые встречаются между совпадающими скобками (\[,]);
2. Удаление или стандартизация междометий или заполненных пауз;
3. Стандартизация сокращенных и несокращенных форм английского языка.
4. Нормализация нестандартных слов(числа, даты, валюты, время);
5. Унификация орфографических правил США и Великобритании.
### Статистическая значимость для ASR: MAPSSWE или MacNemar

Как и в случае с другими алгоритмами обработки языка, нам необходимо знать, является ли конкретное улучшение частоты ошибок в словах значимых или нет. Стандартным статистическим тестом для определения различия в частоте ошибок в словах между двумя системами является тест *MAPSSWE* - это параметрический тест, который рассматривает разницу между количеством ошибок в словах, которые производят две системы, усредненным по ряду сегментов. Сегменты могут быть довольно короткими или такими же длинными, как целое высказывание; в общем случае, мы хотим иметь наибольшее количество сегментов, чтобы обосновать предположение о нормальном распределении и максимизировать мощность. Тест требует, чтобы ошибки в одном сегменте были статистически независимы от ошибок в другом сегменте. Поскольку системы ASR обычно используют языки программирования на основе триграмм, мы можем аппроксимировать это требование, определив сегмент как область, ограниченную с обеих сторон словами, которые оба распознавателя распознают правильно. Вот пример из NIST с четырьмя областями:

![[Pasted image 20251228180749.png]]

В области I система *A* имеет две ошибки, а система *B* - ноль; в области III система *A* имеет одну ошибку, а система *B* - две. Определим последовательность переменных $Z$, представляющих разницу между ошибками в двух системах, следующим образом:

![[Pasted image 20251228181629.png]]

В приведенном выше примере последовательность значений $Z$ - $\{ 2, -1,-1,1 \}$. Интуитивно, если две системы идентичны, мы ожидаем, что средняя разность, то есть среднее значение $Z$, будет равно нулю. Если мы обозначим истинное среднее значение разностей как $\mu_{z}$, то нам нужно знать, равно ли $\mu_{z}=0$. Следуя первоначальному предложению и обозначениям Гиллика и Кокса, мы можем оценить истинное среднее значение на основе нашей ограниченной выборки как $\hat{\mu}_{z}=\sum_{i=1}^{n} \frac{Z_{i}}{n}$. Оценка дисперсии $Z_{i}$ равна:
$$
\sigma_{z}^{2}= \frac{1}{n-1} \sum_{i=1}^{n}(Z_{i}-\mu_{z})^{2}
$$

Пусть
$$
W= \frac{\hat{\mu}_{z}}{\sigma_{z}/\sqrt{ n }}
$$

При достаточно большом ($n>50$) $W$ будет приблизительно иметь нормальное распределение с единичной дисперсией. Нулевая гипотеза $H_{0}: \mu_{z}=0$, и, следовательно, она может быть отклонена, если $2\cdot P(Z\geq |w|)\leq0.05$ (двусторонний тест) или $P(Z\geq |w|)\leq0.05$ (односторонний тест), где $Z$ - стандартное нормальное распределение, а $w$ - реализованное значение $W$; эти вероятности можно найти в стандартных таблицах нормального распределения.

В более ранних работах иногда использовался *тест Макнемара* для проверки значимости, но тест Макнемара применим только тогда, когда ошибки, совершаемые системы, независимы, что неверно в непрерывном распознавании речи, где ошибки, совершаемые над словом, чрезвычайно зависимы от ошибок, совершаемых над соседними словами.

Можно ли улучшить показатель частоты ошибок в словах как метрику? Было бы неплохо, например, иметь что-то, что не придавало бы каждому слову одинаковый вес, возможно, ценя содержательные слова, такие как "Tuesday", больше, чем целом согласны с тем, что это была бы хорошая идея, оказалось сложно договориться о метрике, которая бы работала бы во всех приложениях автоматического распознавания речи.