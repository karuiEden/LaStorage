---
tags:
  - ml
  - regression
  - metric
---


Чтобы обучить регрессионые модели, необходимо определиться, как оценивать их качество. Пусть $y$ - значение целевой переменной, $a$ - прогноз модели.

### $MSE$ и $R^{2}$:
Основной способ посчитать отклонение -  посчитать квадрат разности:
$$L(y,a) = (a-y)^{2}$$
Благодаря своей дифференцируемости эта функция чаще всего используется в задачах регрессии. Основанный на ней функционал называется среднеквадратичным отклонением:
$$MSE(a, X) = \frac{1}{\ell}\sum_{i=1}^{\ell}(a(x_{i})-y_{i})^{2}$$
Но у $MSE$ есть одна проблема, которая заключается в её плохой интерпретированности, так как она не сохраняет единицы измерения, чтобы избежать этой проблемы, используют корень из среднеквадратичного отклонения(RMSE - Root Mean Square Error):
$$RMSE(a,X)=\sqrt{ \frac{1}{\ell}\sum_{i=1}^{\ell}(a(x_{i})-y_{i})^{2} }$$
Хотя и $MSE$ хорошо подходит для сравнения двух моделей, но оценить насколько правильно работает модель нельзя, так как значение отклонения, равное 10, может значить, о том, что модель хорошо работает если таргет большой (примерно в интервале от 10000 до 100000), а если целевая переменная лежит в интервале от 0 до 1, то модель работает плохо. Поэтому используют **коэффициент детерминации**($R^{2}$):
$$R^{2}(a,X)=1- \frac{ \sum_{i=1}^{\ell}(a(x_{i})-y_{i})^{2} }{\sum_{i=1}^{\ell}(y_{i}-\bar{y})^{2}}$$, где $\bar{y}$ - среднее значение целевой переменной.
По сути коэффициент детерминации измеряет долю дисперсии, объяснённую моделью, в дисперсии целевой переменной. Фактически является нормированной квадратичной ошибкой. Если $R^{2}\longrightarrow 1$, то модель хорошо работает, если $R^{2}\longrightarrow 0$, то прогнозы модели сопоставимы с константным предсказанием.
### $MAE$

Если функцию квадратичное отклонение заменить на модуль: $$L(y,a)=|a-y|$$
То такой функционал функционал будет называться средней абсолютной ошибкой($MAE(\text{Mean Absolute Error})$:$$MAE(X,a)=\frac{1}{\ell}\sum_{i=1}^{\ell}|a(x_{i})-y_{i}|$$
Хотя модуль отклонения не является дифференцируемым, но при этом менее чувствителен к выбросам, чем квадратичное отклонение.

Рассмотрим две модели, один из объектов будет выбросом. Модель $a_{1}(x)$ почти не ошибается на "нормальных" объектах, но сильно ошибается на выбросе. Модель $a_{2}(x)$ искажает значения на "нормальных" признаках, но зато ошибка на выбросе не такая большая.
![[Pasted image 20250202131546.png]]
Как можно заметить, первая модель лучше предсказывает, если смотреть $MAE$, но по $MSE$ более хорошей моделью оказывается вторая модель. У квадратичной функции потерь штраф за ошибку растёт нелинейно с ростом отклонения прогноза от ответа, а для абсолютной функции потерь равносильно снижение отклонения на одну и ту же величину для нормального объекта и для выброса. Но эта особенность работает, когда выбросов немного.

Ещё показать более хорошую устойчивость модуля отклонения можно на примере. В обучающей выборке признаковое описание у всех одинаковое, но при этом значения целевых переменных у всех отличается. Тогда для $MSE$ задача будет выглядеть так:$$MSE(X,a)\frac{1}{\ell}\sum_{i=1}^{\ell}(a(x_{i})-y_{i})^{2}\longrightarrow \min_{a}$$

 Тогда минимуму будет достигаться при среднем значении всех ответов:
 $$a^{*}_{MSE}=\frac{1}{\ell}\sum_{i=1}^{\ell}y_{i}$$
 Если взять абсолютную функцию потерь, то задача будет выглядеть так:$$MAE(X,a)=\frac{1}{\ell}\sum_{i=1}^{\ell}|a(x_{i})-y_{i}|\longrightarrow \min_{a}$$
 Теперь решением будет:
 $$a^{*}_{MAE}=median\{ y_{i} \}_{i=1}^{\ell}$$
Небольшое количество выбросов никак не повлияет на медиану - она существенно устойчива к величинам, которые выбиваются из общего распределения.

Но также у абсолютной функции потерь есть и проблема. Рассмотрим её производную, а также производную квадратичной функции потери:
$$\frac{\partial}{\partial a}|a-y|=sign(a-y) \ \ a\neq y$$
$$\frac{\partial}{\partial a}(a-y)^{2}=2(a-y)$$
Можно заметить, что производная абсолютной функции потерь никак не зависит от близости прогноза к правильному ответу, по её значению невозможно понять насколько мы близки к правильному прогнозу.

### Huber loss

Квадратичная функция лучше с точки зрения оптимизации, а абсолютная функция более устойчива к выбросам. Если объединить две хорошие части этих функций, чтобы на плохих предсказаниях была абсолютная величина, а на близких к верным квадратичная. Одним из вариантов такого объединения называется функцией потери Хубера:$$L_{\delta}(y,a)=\begin{cases}
\frac{1}{2}(y-a)^{2}, |y-a|<\delta \\
\delta\left( |y-a|- \frac{1}{2}\delta \right), |y-a|\geq\delta \\
\end{cases}$$
У этой функции есть параметр $\delta$, который регулирует, что мы считаем за выбросы.
При $\delta \longrightarrow 0$ функция потерь Хубера вырождается в абсолютную функцию потери.
При $\delta \longrightarrow \infty$ - в квадратичную.
У функции потери Хубера есть один недостаток, у её второй производной есть разрывы.
### Log-Cosh
У функции потерь $\log-\cosh$ такого нет:
$$L(y,a)=\log \cosh(a-y)$$
Как и в случае с функцией потерь Хубера, для маленьких отклонений здесь имеет место квадратичное поведение, а для больших — линейное.
![[Pasted image 20250202145544.png]]

### MSLE
Перейдём к логарифмам ответов и предсказаний:
$$L(y,a)=(\log(a+1)-\log(y+1))^{2}$$
Соответствующий функционал называется **среднеквадратичной логарифмичной ошибкой(Mean square logarithmic error, MSLE)**. Данная метрика подходит для моделей, где целевая переменная является неотрицательной и неотрицательными прогнозами модели. Так как мы всё логарифмируем, то мы больше штрафуем за отклонения порядка величин, чем за отклонения значения. Также функция логарифма не симметричная, поэтому штрафует сильнее заниженные отклонения, а завышенные - слабее.

### MAPE и SMAPE
В задачах прогнозирования нередко оценивают относительную ошибку.
1. Хорошо для интерпретации
2. Позволяет работать с разными масштабами.
Для этого используют функции потерь, которые не связаны с масштабом. Пример:
$$L(y,a)= \bigg|\frac{y-a}{y}\bigg |$$
Соответствующий функционал называется **средней абсолютной процентной ошибкой(Mean Absolute Percentage error, MAPE)** 
У MAPE есть проблемы с симметричностью, в случае с заниженными прогнозами, значения будут ограничены значениями правильного ответа, а в случае завышенных прогнозов ограничений сверху нет. Это исправляется в симметричной модификации (SMAPE, Symmetric Mean Absolute Percentage error):
$$L(y,a)= \frac{|y-a|}{(|y|+|a|)/ \frac{1}{2}}$$
