---
title: AdaGrad и RMSprop
created: 2025-08-08
tags:
  - ml
  - optimization
links:
  - "[[Градиентный спуск и оценивание градиента]]"
---
**Проблема, решаемая методом:** Чувствительность к выбору длины шага градиентного спуска.
**Решение:**  В методе AdaGrad предлагается сделать свою длину шага для каждой компоненты вектора параметров. При этом шаг будет тем меньше, чем более длинные шаги мы делали на предыдущих итерациях:
$$G_{kj}=G_{k-1,j}+(\nabla_{w}Q(w^{(k-1)}))^{2}_{j};$$
$$w_{j}^{(k)}=w_{j}^{(k-1)}- \frac{\eta_{k}}{\sqrt{ G_{kj} +\varepsilon }}(\nabla_{w}Q(w^{(k-1)}))_{j}.$$
, где $\varepsilon$ - небольшая константа, которая предотвращает деление на 0. В данном методе можно зафиксировать длину шага $\eta_{k}$ и не подбирать её в процессе обучения. 
Данный метод подходит для разреженных задач.

У AdaGrad есть большой недостаток: $G_{kj}$ монотонно растёт, поэтому шаги могут остановиться ещё до нахождения минимума. Это решается в методе RMSprop, где используется экспоненциальное затухание градиентов:
$$G_{kj}=\alpha G_{k-1,j}+(1-\alpha)(\nabla_{w}Q(w^{(k-1)}))^{2}_{j}.$$
В этом случае размер шага по координате зависит в основном от того, насколько быстро мы двигались по ней на последних итерациях.