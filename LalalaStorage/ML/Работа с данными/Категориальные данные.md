---
title: Категориальные данные
created: 2025-09-16
tags:
  - ml
links:
---
Допустим, в выборке имеется категориальный признак, значение которого на объекте $x$ обозначим через $f(x)$. Будем считать, что она будет принимать значения из множества $U=\{ u_{1},\dots,u_{n} \}$. Чтобы использовать категориальные признаки в линейных моделях, необходимо закодировать их.

## Бинарное кодирование (one-hot encoding)

Простейший способ кодирования - создать $n$ индикаторов, каждый из них которых будет отвечать за одно из значений категориального признака. Иначе, мы создаём $n$ бинарных признаков $g_{1}(x),\dots,g_{n}(x)$, которые определяются как:
$$g_{j}(x)=[f(x)=u_{j}]$$
Проблема этого подхода заключается в том, что если категориальный признак состоит из огромного множества значений, то после кодирования появится огромное количество новых признаков. Линейные модели хорошо работают с этим, так как имеют небольшое количество параметров и за счёт лёгкого обучения.
## Бинарное кодирование с хэшированием

Есть модификация бинарного кодирования, которая ускоряет процесс вычисления признаков. Выберем хэш-функцию $h:U\to \{ 1,2,\dots B ,\}$, которая переводит значения признака в числа от $1$ до $B$. $$g_{j}(x)=[h(f(x))=j], \ j=1,\dots,B$$
Основное преимущество данной модификации, что при ней отпадает необходимость в хранении соответствий между значениями категориального признака и индексами бинарных признаков .Также она позволяет сократить количество признаков, если $|U|>B$.

## Счётчики

Заметим, что значения категориального признака нужны лишь для предсказания класса. Соответственно, если два возможных значения $u_{i}$ и $u_{j}$ характерны для одного и того же класса, то можно их не различать.
Определим наш способ кодирования. Вычислим для каждого значения $u$ категориального признака $(K+1)$ величин:
$$\text{counts}(u,X)=\sum_{(x,y)\in X}[f(x)=u],$$
$$\text{successes}_{k}(u,X)=\sum_{(x,y)\in X}[f(x)=y][y=k], \ \ k=1,\dots,K$$
По сути, мы посчитали количество объектов с данным значением признака, а также количество объектов различных классов среди них.
После того, как данные величины посчитаны, заменим наш категориальный признак $f(x)$ на $K$ вещественных:
$$g_{k}(x,K)=\frac{\text{successes}_{k}(f(x),X)+c_{k}}{\text{counts}(f(x),X)+\sum_{m=1}^{K}c_{m}}, \ \ k=1,\dots ,K$$
Здесь признак $g_{k}(x,K)$ оценивает вероятность $p(y=k\ | \ f(x))$. Величины $c_{k}$ являются своего рода регуляризаторами, предотвращая деление на ноль, если в выборке $X$ нет ни одного объекта с таким значением признака. Для простоты можно полагать их все равными $1:c_{1}=\dots=c_{K}=1$. У признаков $g_{k}(x,K)$ есть несколько названий: счётчики, правдоподобия и т.д.
По сути $g_{k}(x,K)$ является простейшим классификатором, поэтому при обучении полноценного классификатора мы рискуем получить переобучение. Чтобы решить эту проблему, используем подход аналогичный кросс валидации. Выборка разбивается на $m$ частей: $X_{1},X_{2},\dots,X_{m}$, и для подвыборки $X_{i}$ значения признаков вычисляются на основе статистик, подсчитанных по всем остальным частям:
$$x\in X_{i}\implies g_{k}(x)=g_{k}(x,X\setminus X_{i})$$ Для тестовой выборки значения целевой переменной неизвестно, поэтому на таких объектах счётчики будут считаться на основе статистик $\text{counts и successes}$, посчитанных по всей обучающей выборке.
Также можно использовать некоторые приёмы:
1. К признакам можно добавлять не только $g_{k}(x)$, но и значения $\text{counts}(f(x),X)$ и $\text{successes}_{k}(f(x),X)$
2. Можно сгенерировать парные признаки, т.е $f_{ij}(x)=(f_{i}(x),f_{j}(x))$ и для них вычислить счётчики. Хотя это увеличит количество признаков, но увеличит качество модели.
3. Если у категориального признака много значений, то для экономии можно хранить хэши этих значений.
4. Можно вычислять разные счётчики с разными значениями параметров $c_{1},\dots,c_{K}$.
5. Редкие значения признаков можно объединить в один, так как для редких признаков тяжело будет посчитать статистики $\text{counts}$ и $\text{successes}$.
